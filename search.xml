<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Transformer（中）架构解析</title>
      <link href="2021/04/27/transformer-jia-gou-jie-xi/"/>
      <url>2021/04/27/transformer-jia-gou-jie-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer（中）架构解析"><a href="#Transformer（中）架构解析" class="headerlink" title="Transformer（中）架构解析"></a>Transformer（中）架构解析</h1><h2 id="2-1-认识Transformer架构"><a href="#2-1-认识Transformer架构" class="headerlink" title="2.1 认识Transformer架构"></a>2.1 认识Transformer架构</h2><hr><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解Transformer模型的作用.</li><li>了解Transformer总体架构图中各个组成部分的名称.</li></ul><hr><h3 id="Transformer模型的作用"><a href="#Transformer模型的作用" class="headerlink" title="Transformer模型的作用"></a>Transformer模型的作用</h3><ul><li>基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.</li></ul><hr><ul><li>声明:<ul><li>在接下来的架构分析中, 我们将假设使用Transformer模型架构处理从一种语言文本到另一种语言文本的翻译工作, 因此很多命名方式遵循NLP中的规则. 比如: Embeddding层将称作文本嵌入层, Embedding层产生的张量称为词嵌入张量, 它的最后一维将称作词向量等.</li></ul></li></ul><hr><h3 id="Transformer总体架构图"><a href="#Transformer总体架构图" class="headerlink" title="Transformer总体架构图"></a>Transformer总体架构图</h3><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/4.png" alt="avatar"></p><hr><ul><li>Transformer总体架构可分为四个部分:<ul><li>输入部分</li><li>输出部分</li><li>编码器部分</li><li>解码器部分</li></ul></li></ul><hr><ul><li>输入部分包含:<ul><li>源文本嵌入层及其位置编码器</li><li>目标文本嵌入层及其位置编码器</li></ul></li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/5.png" alt="avatar"></p><hr><ul><li>输出部分包含:<ul><li>线性层</li><li>softmax层</li></ul></li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/6.png" alt="avatar"></p><hr><ul><li>编码器部分:<ul><li>由N个编码器层堆叠而成</li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/7.png" alt="avatar"></p><hr><ul><li>解码器部分:<ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/8.png" alt="avatar"></p><hr><h3 id="小节总结"><a href="#小节总结" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了Transformer模型的作用:</p><ul><li>基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.</li></ul><hr></li><li><p>Transformer总体架构可分为四个部分:</p><ul><li>输入部分</li><li>输出部分</li><li>编码器部分</li><li>解码器部分</li></ul><hr></li><li><p>输入部分包含:</p><ul><li>源文本嵌入层及其位置编码器</li><li>目标文本嵌入层及其位置编码器</li></ul><hr></li><li><p>输出部分包含:</p><ul><li>线性层</li><li>softmax处理器</li></ul><hr></li><li><p>编码器部分:</p><ul><li>由N个编码器层堆叠而成</li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><hr></li><li><p>解码器部分:</p><ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><hr><hr><hr><h2 id="2-2-输入部分实现"><a href="#2-2-输入部分实现" class="headerlink" title="2.2 输入部分实现"></a>2.2 输入部分实现</h2><hr><h3 id="学习目标-1"><a href="#学习目标-1" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解文本嵌入层和位置编码的作用.</li><li>掌握文本嵌入层和位置编码的实现过程.</li></ul><hr><ul><li>输入部分包含:<ul><li>源文本嵌入层及其位置编码器</li><li>目标文本嵌入层及其位置编码器</li></ul></li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/5.png" alt="avatar"></p><hr><h3 id="文本嵌入层的作用"><a href="#文本嵌入层的作用" class="headerlink" title="文本嵌入层的作用"></a>文本嵌入层的作用</h3><ul><li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li></ul><hr><ul><li>pytorch 0.3.0及其必备工具包的安装:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 使用pip安装的工具包包括pytorch-0.3.0, numpy, matplotlib, seabornpip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib seaborn# MAC系统安装, python版本&lt;=3.6pip install torch==0.3.0.post4 numpy matplotlib seaborn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>文本嵌入层的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 导入必备的工具包import torch# 预定义的网络层torch.nn, 工具开发者已经帮助我们开发好的一些常用层, # 比如，卷积层, lstm层, embedding层等, 不需要我们再重新造轮子.import torch.nn as nn# 数学计算工具包import math# torch中变量封装函数Variable.from torch.autograd import Variable# 定义Embeddings类来实现文本嵌入层，这里s说明代表两个一模一样的嵌入层, 他们共享参数.# 该类继承nn.Module, 这样就有标准层的一些功能, 这里我们也可以理解为一种模式, 我们自己实现的所有层都会这样去写.class Embeddings(nn.Module):    def __init__(self, d_model, vocab):        """类的初始化函数, 有两个参数, d_model: 指词嵌入的维度, vocab: 指词表的大小."""        # 接着就是使用super的方式指明继承nn.Module的初始化函数, 我们自己实现的所有层都会这样去写.        super(Embeddings, self).__init__()        # 之后就是调用nn中的预定义层Embedding, 获得一个词嵌入对象self.lut        self.lut = nn.Embedding(vocab, d_model)        # 最后就是将d_model传入类中        self.d_model = d_model    def forward(self, x):        """可以将其理解为该层的前向传播逻辑，所有层中都会有此函数           当传给该类的实例化对象参数时, 自动调用该类函数           参数x: 因为Embedding层是首层, 所以代表输入给模型的文本通过词汇映射后的张量"""        # 将x传给self.lut并与根号下self.d_model相乘作为结果返回        return self.lut(x) * math.sqrt(self.d_model)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>nn.Embedding演示:</li></ul><pre class="line-numbers language-none"><code class="language-none">&gt;&gt;&gt; embedding = nn.Embedding(10, 3)&gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])&gt;&gt;&gt; embedding(input)tensor([[[-0.0251, -1.6902,  0.7172],         [-0.6431,  0.0748,  0.6969],         [ 1.4970,  1.3448, -0.9685],         [-0.3677, -2.7265, -0.1685]],        [[ 1.4970,  1.3448, -0.9685],         [ 0.4362, -0.4004,  0.9400],         [-0.6431,  0.0748,  0.6969],         [ 0.9124, -2.3616,  1.1151]]])&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)&gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])&gt;&gt;&gt; embedding(input)tensor([[[ 0.0000,  0.0000,  0.0000],         [ 0.1535, -2.0309,  0.9315],         [ 0.0000,  0.0000,  0.0000],         [-0.1655,  0.9897,  0.0635]]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 词嵌入维度是512维d_model = 512# 词表大小是1000vocab = 1000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 输入x是一个使用Variable封装的长整型张量, 形状是2 x 4x = Variable(torch.LongTensor([[100,2,421,508],[491,998,1,221]]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">emb = Embeddings(d_model, vocab)embr = emb(x)print("embr:", embr)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">embr: Variable containing:( 0 ,.,.) =   35.9321   3.2582 -17.7301  ...    3.4109  13.8832  39.0272   8.5410  -3.5790 -12.0460  ...   40.1880  36.6009  34.7141 -17.0650  -1.8705 -20.1807  ...  -12.5556 -34.0739  35.6536  20.6105   4.4314  14.9912  ...   -0.1342  -9.9270  28.6771( 1 ,.,.) =   27.7016  16.7183  46.6900  ...   17.9840  17.2525  -3.9709   3.0645  -5.5105  10.8802  ...  -13.0069  30.8834 -38.3209  33.1378 -32.1435  -3.9369  ...   15.6094 -29.7063  40.1361 -31.5056   3.3648   1.4726  ...    2.8047  -9.6514 -23.4909[torch.FloatTensor of size 2x4x512]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="位置编码器的作用"><a href="#位置编码器的作用" class="headerlink" title="位置编码器的作用"></a>位置编码器的作用</h3><ul><li>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.</li></ul><hr><ul><li>位置编码器的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 定义位置编码器类, 我们同样把它看做一个层, 因此会继承nn.Module    class PositionalEncoding(nn.Module):    def __init__(self, d_model, dropout, max_len=5000):        """位置编码器类的初始化函数, 共有三个参数, 分别是d_model: 词嵌入维度,            dropout: 置0比率, max_len: 每个句子的最大长度"""        super(PositionalEncoding, self).__init__()        # 实例化nn中预定义的Dropout层, 并将dropout传入其中, 获得对象self.dropout        self.dropout = nn.Dropout(p=dropout)        # 初始化一个位置编码矩阵, 它是一个0阵，矩阵的大小是max_len x d_model.        pe = torch.zeros(max_len, d_model)        # 初始化一个绝对位置矩阵, 在我们这里，词汇的绝对位置就是用它的索引去表示.         # 所以我们首先使用arange方法获得一个连续自然数向量，然后再使用unsqueeze方法拓展向量维度使其成为矩阵，         # 又因为参数传的是1，代表矩阵拓展的位置，会使向量变成一个max_len x 1 的矩阵，         position = torch.arange(0, max_len).unsqueeze(1)        # 绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中，        # 最简单思路就是先将max_len x 1的绝对位置矩阵， 变换成max_len x d_model形状，然后覆盖原来的初始位置编码矩阵即可，         # 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要求除了形状外，        # 还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛.  这样我们就可以开始初始化这个变换矩阵了.        # 首先使用arange获得一个自然数矩阵， 但是细心的同学们会发现， 我们这里并没有按照预计的一样初始化一个1xd_model的矩阵，         # 而是有了一个跳跃，只初始化了一半即1xd_model/2 的矩阵。 为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵，        # 我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正弦波上， 第二次初始化的变换矩阵分布在余弦波上，         # 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵.        div_term = torch.exp(torch.arange(0, d_model, 2) *                             -(math.log(10000.0) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        # 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，        # 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.        pe = pe.unsqueeze(0)        # 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，        # 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤进行更新的增益对象.         # 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.        self.register_buffer('pe', pe)    def forward(self, x):        """forward函数的参数是x, 表示文本序列的词嵌入表示"""        # 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切片到与输入的x的第二维相同即x.size(1)，        # 因为我们默认max_len为5000一般来讲实在太大了，很难有一条句子包含5000个词汇，所以要进行与输入张量的适配.         # 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requires_grad设置成false.        x = x + Variable(self.pe[:, :x.size(1)],                          requires_grad=False)        # 最后使用self.dropout对象进行'丢弃'操作, 并返回结果.        return self.dropout(x)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>nn.Dropout演示:</li></ul><pre class="line-numbers language-none"><code class="language-none">&gt;&gt;&gt; m = nn.Dropout(p=0.2)&gt;&gt;&gt; input = torch.randn(4, 5)&gt;&gt;&gt; output = m(input)&gt;&gt;&gt; outputVariable containing: 0.0000 -0.5856 -1.4094  0.0000 -1.0290 2.0591 -1.3400 -1.7247 -0.9885  0.1286 0.5099  1.3715  0.0000  2.2079 -0.5497-0.0000 -0.7839 -1.2434 -0.1222  1.2815[torch.FloatTensor of size 4x5]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>torch.unsqueeze演示:</li></ul><pre class="line-numbers language-none"><code class="language-none">&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])&gt;&gt;&gt; torch.unsqueeze(x, 0)tensor([[ 1,  2,  3,  4]])&gt;&gt;&gt; torch.unsqueeze(x, 1)tensor([[ 1],        [ 2],        [ 3],        [ 4]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 词嵌入维度是512维d_model = 512# 置0比率为0.1dropout = 0.1# 句子最大长度max_len=60<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 输入x是Embedding层的输出的张量, 形状是2 x 4 x 512x = embrVariable containing:( 0 ,.,.) =   35.9321   3.2582 -17.7301  ...    3.4109  13.8832  39.0272   8.5410  -3.5790 -12.0460  ...   40.1880  36.6009  34.7141 -17.0650  -1.8705 -20.1807  ...  -12.5556 -34.0739  35.6536  20.6105   4.4314  14.9912  ...   -0.1342  -9.9270  28.6771( 1 ,.,.) =   27.7016  16.7183  46.6900  ...   17.9840  17.2525  -3.9709   3.0645  -5.5105  10.8802  ...  -13.0069  30.8834 -38.3209  33.1378 -32.1435  -3.9369  ...   15.6094 -29.7063  40.1361 -31.5056   3.3648   1.4726  ...    2.8047  -9.6514 -23.4909[torch.FloatTensor of size 2x4x512]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">pe = PositionalEncoding(d_model, dropout, max_len)pe_result = pe(x)print("pe_result:", pe_result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">pe_result: Variable containing:( 0 ,.,.) =  -19.7050   0.0000   0.0000  ...  -11.7557  -0.0000  23.4553  -1.4668 -62.2510  -2.4012  ...   66.5860 -24.4578 -37.7469   9.8642 -41.6497 -11.4968  ...  -21.1293 -42.0945  50.7943   0.0000  34.1785 -33.0712  ...   48.5520   3.2540  54.1348( 1 ,.,.) =    7.7598 -21.0359  15.0595  ...  -35.6061  -0.0000   4.1772 -38.7230   8.6578  34.2935  ...  -43.3556  26.6052   4.3084  24.6962  37.3626 -26.9271  ...   49.8989   0.0000  44.9158 -28.8435 -48.5963  -0.9892  ...  -52.5447  -4.1475  -3.0450[torch.FloatTensor of size 2x4x512]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>绘制词汇向量中特征的分布曲线:</li></ul><pre class="line-numbers language-none"><code class="language-none">import matplotlib.pyplot as plt# 创建一张15 x 5大小的画布plt.figure(figsize=(15, 5))# 实例化PositionalEncoding类得到pe对象, 输入参数是20和0pe = PositionalEncoding(20, 0)# 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数, # 且这个tensor里的数值都是0, 被处理后相当于位置编码张量y = pe(Variable(torch.zeros(1, 100, 20)))# 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值# 因为总共有20维之多, 我们这里只查看4，5，6，7维的值.plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())# 在画布上填写维度提示信息plt.legend(["dim %d"%p for p in [4,5,6,7]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>输出效果:</li></ul></blockquote><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/11.png" alt="avatar"></p><blockquote><ul><li>效果分析:<ul><li>每条颜色的曲线代表某一个词汇中的特征在不同位置的含义.</li><li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</li><li>正弦波和余弦波的值域范围都是1到-1这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.</li></ul></li></ul></blockquote><hr><h3 id="小节总结-1"><a href="#小节总结-1" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了文本嵌入层的作用:</p><ul><li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li></ul><hr></li><li><p>学习并实现了文本嵌入层的类: Embeddings</p><ul><li>初始化函数以d_model, 词嵌入维度, 和vocab, 词汇总数为参数, 内部主要使用了nn中的预定层Embedding进行词嵌入.</li><li>在forward函数中, 将输入x传入到Embedding的实例化对象中, 然后乘以一个根号下d_model进行缩放, 控制数值大小.</li><li>它的输出是文本嵌入后的结果.</li></ul><hr></li><li><p>学习了位置编码器的作用:</p><ul><li>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.</li></ul><hr></li><li><p>学习并实现了位置编码器的类: PositionalEncoding</p><ul><li>初始化函数以d_model, dropout, max_len为参数, 分别代表d_model: 词嵌入维度, dropout: 置0比率, max_len: 每个句子的最大长度.</li><li>forward函数中的输入参数为x, 是Embedding层的输出.</li><li>最终输出一个加入了位置编码信息的词嵌入张量.</li></ul><hr></li><li><p>实现了绘制词汇向量中特征的分布曲线:</p><ul><li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</li><li>正弦波和余弦波的值域范围都是1到-1, 这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.</li></ul></li></ul><hr><hr><hr><h2 id="2-3-编码器部分实现"><a href="#2-3-编码器部分实现" class="headerlink" title="2.3 编码器部分实现"></a>2.3 编码器部分实现</h2><hr><h3 id="学习目标-2"><a href="#学习目标-2" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解编码器中各个组成部分的作用.</li><li>掌握编码器中各个组成部分的实现过程.</li></ul><hr><ul><li>编码器部分:<ul><li>由N个编码器层堆叠而成</li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/7.png" alt="avatar"></p><hr><h3 id="2-3-1-掩码张量"><a href="#2-3-1-掩码张量" class="headerlink" title="2.3.1 掩码张量"></a>2.3.1 掩码张量</h3><hr><ul><li>学习目标:<ul><li>了解什么是掩码张量以及它的作用.</li><li>掌握生成掩码张量的实现过程.</li></ul></li></ul><hr><hr><ul><li>什么是掩码张量:<ul><li>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩，也可以说被替换, 它的表现形式是一个张量.</li></ul></li></ul><hr><ul><li>掩码张量的作用:<ul><li>在transformer中, 掩码张量的主要作用在应用attention(将在下一小节讲解)时，有一些生成的attention张量中的值计算有可能已知了未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li></ul></li></ul><hr><ul><li>生成掩码张量的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none">def subsequent_mask(size):    """生成向后遮掩的掩码张量, 参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵"""    # 在函数中, 首先定义掩码张量的形状    attn_shape = (1, size, size)    # 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间,     # 再使其中的数据类型变为无符号8位整形unit8     subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')    # 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作,     # 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减,     # 如果是0, subsequent_mask中的该位置由0变成1    # 如果是1, subsequent_mask中的该位置由1变成0     return torch.from_numpy(1 - subsequent_mask)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>np.triu演示:</li></ul><pre class="line-numbers language-none"><code class="language-none">&gt;&gt;&gt; np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], k=-1)array([[ 1,  2,  3],       [ 4,  5,  6],       [ 0,  8,  9],       [ 0,  0, 12]])&gt;&gt;&gt; np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], k=0)array([[ 1,  2,  3],       [ 0,  5,  6],       [ 0,  0,  9],       [ 0,  0, 0]])&gt;&gt;&gt; np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], k=1)array([[ 0,  2,  3],       [ 0,  0,  6],       [ 0,  0,  0],       [ 0,  0, 0]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入实例:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 生成的掩码张量的最后两维的大小size = 5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">sm = subsequent_mask(size)print("sm:", sm)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 最后两维形成一个下三角阵sm: (0 ,.,.) =   1  0  0  0  0  1  1  0  0  0  1  1  1  0  0  1  1  1  1  0  1  1  1  1  1[torch.ByteTensor of size 1x5x5]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>掩码张量的可视化:</li></ul><pre class="line-numbers language-none"><code class="language-none">plt.figure(figsize=(5,5))plt.imshow(subsequent_mask(20)[0])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/12.png" alt="avatar"></p><hr><blockquote><ul><li>效果分析:<ul><li>通过观察可视化方阵, 黄色是1的部分, 这里代表被遮掩, 紫色代表没有被遮掩的信息, 横坐标代表目标词汇的位置, 纵坐标代表可查看的位置;</li><li>我们看到, 在0的位置我们一看望过去都是黄色的, 都被遮住了，1的位置一眼望过去还是黄色, 说明第一次词还没有产生, 从第二个位置看过去, 就能看到位置1的词, 其他位置看不到, 以此类推.</li></ul></li></ul></blockquote><hr><ul><li><p>2.3.1 掩码张量总结:</p><ul><li>学习了什么是掩码张量:<ul><li>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩, 也可以说被替换, 它的表现形式是一个张量.</li></ul></li></ul><hr><ul><li>学习了掩码张量的作用:<ul><li>在transformer中, 掩码张量的主要作用在应用attention(将在下一小节讲解)时，有一些生成的attetion张量中的值计算有可能已知量未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li></ul></li></ul><hr><ul><li>学习并实现了生成向后遮掩的掩码张量函数: subsequent_mask<ul><li>它的输入是size, 代表掩码张量的大小.</li><li>它的输出是一个最后两维形成1方阵的下三角阵.</li><li>最后对生成的掩码张量进行了可视化分析, 更深一步理解了它的用途.</li></ul></li></ul></li></ul><hr><hr><h3 id="2-3-2-注意力机制"><a href="#2-3-2-注意力机制" class="headerlink" title="2.3.2 注意力机制"></a>2.3.2 注意力机制</h3><hr><ul><li>学习目标:<ul><li>了解什么是注意力计算规则和注意力机制.</li><li>掌握注意力计算规则的实现过程.</li></ul></li></ul><hr><ul><li>什么是注意力:<ul><li>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</li></ul></li></ul><hr><ul><li>什么是注意力计算规则:<ul><li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li></ul></li></ul><hr><ul><li>我们这里使用的注意力的计算规则:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/9.png" alt="avatar"></p><hr><ul><li>Q, K, V的比喻解释:</li></ul><pre class="line-numbers language-none"><code class="language-none">假如我们有一个问题: 给出一段文本，使用一些关键词对它进行描述!为了方便统一正确答案，这道题可能预先已经给大家写出了一些关键词作为提示.其中这些给出的提示就可以看作是key， 而整个的文本信息就相当于是query，value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的答案信息，这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息，因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多，并且能够开始对我们query也就是这段文本，提取关键信息进行表示.  这就是注意力作用的过程， 通过这个过程，我们最终脑子里的value发生了变化，根据提示key生成了query的关键词表示方法，也就是另外一种特征表示方法.刚刚我们说到key和value一般情况下默认是相同，与query是不同的，这种是我们一般的注意力输入形式，但有一种特殊情况，就是我们query与key和value相同，这种情况我们称为自注意力机制，就如同我们的刚刚的例子， 使用一般注意力机制，是使用不同于给定文本的关键词表示它. 而自注意力机制,需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它, 相当于对文本自身的一次特征提取.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>什么是注意力机制:<ul><li>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.</li></ul></li></ul><hr><ul><li>注意力机制在网络中实现的图形表示:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/10.png" alt="avatar"></p><hr><ul><li>注意力计算规则的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none">def attention(query, key, value, mask=None, dropout=None):    """注意力机制的实现, 输入分别是query, key, value, mask: 掩码张量,        dropout是nn.Dropout层的实例化对象, 默认为None"""    # 在函数中, 首先取query的最后一维的大小, 一般情况下就等同于我们的词嵌入维度, 命名为d_k    d_k = query.size(-1)    # 按照注意力公式, 将query与key的转置相乘, 这里面key是将最后两个维度进行转置, 再除以缩放系数根号下d_k, 这种计算方法也称为缩放点积注意力计算.    # 得到注意力得分张量scores    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)    # 接着判断是否使用掩码张量    if mask is not None:        # 使用tensor的masked_fill方法, 将掩码张量和scores张量每个位置一一比较, 如果掩码张量处为0        # 则对应的scores张量用-1e9这个值来替换, 如下演示        scores = scores.masked_fill(mask == 0, -1e9)    # 对scores的最后一维进行softmax操作, 使用F.softmax方法, 第一个参数是softmax对象, 第二个是目标维度.    # 这样获得最终的注意力张量    p_attn = F.softmax(scores, dim = -1)    # 之后判断是否使用dropout进行随机置0    if dropout is not None:        # 将p_attn传入dropout对象中进行'丢弃'处理        p_attn = dropout(p_attn)    # 最后, 根据公式将p_attn与value张量相乘获得最终的query注意力表示, 同时返回注意力张量    return torch.matmul(p_attn, value), p_attn<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>tensor.masked_fill演示:</li></ul><pre class="line-numbers language-none"><code class="language-none">&gt;&gt;&gt; input = Variable(torch.randn(5, 5))&gt;&gt;&gt; input Variable containing: 2.0344 -0.5450  0.3365 -0.1888 -2.1803 1.5221 -0.3823  0.8414  0.7836 -0.8481-0.0345 -0.8643  0.6476 -0.2713  1.5645 0.8788 -2.2142  0.4022  0.1997  0.1474 2.9109  0.6006 -0.6745 -1.7262  0.6977[torch.FloatTensor of size 5x5]&gt;&gt;&gt; mask = Variable(torch.zeros(5, 5))&gt;&gt;&gt; maskVariable containing: 0  0  0  0  0 0  0  0  0  0 0  0  0  0  0 0  0  0  0  0 0  0  0  0  0[torch.FloatTensor of size 5x5]&gt;&gt;&gt; input.masked_fill(mask == 0, -1e9)Variable containing:-1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09-1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09-1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09-1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09-1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09 -1.0000e+09[torch.FloatTensor of size 5x5]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 我们令输入的query, key, value都相同, 位置编码的输出query = key = value = pe_resultVariable containing:( 0 ,.,.) =   46.5196  16.2057 -41.5581  ...  -16.0242 -17.8929 -43.0405 -32.6040  16.1096 -29.5228  ...    4.2721  20.6034  -1.2747 -18.6235  14.5076  -2.0105  ...   15.6462 -24.6081 -30.3391   0.0000 -66.1486 -11.5123  ...   20.1519  -4.6823   0.4916( 1 ,.,.) =  -24.8681   7.5495  -5.0765  ...   -7.5992 -26.6630  40.9517  13.1581  -3.1918 -30.9001  ...   25.1187 -26.4621   2.9542 -49.7690 -42.5019   8.0198  ...   -5.4809  25.9403 -27.4931 -52.2775  10.4006   0.0000  ...   -1.9985   7.0106  -0.5189[torch.FloatTensor of size 2x4x512]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>调用:</li></ul><pre class="line-numbers language-none"><code class="language-none">attn, p_attn = attention(query, key, value)print("attn:", attn)print("p_attn:", p_attn)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 将得到两个结果# query的注意力表示:attn: Variable containing:( 0 ,.,.) =    12.8269    7.7403   41.2225  ...     1.4603   27.8559  -12.2600   12.4904    0.0000   24.1575  ...     0.0000    2.5838   18.0647  -32.5959   -4.6252  -29.1050  ...     0.0000  -22.6409  -11.8341    8.9921  -33.0114   -0.7393  ...     4.7871   -5.7735    8.3374( 1 ,.,.) =   -25.6705   -4.0860  -36.8226  ...    37.2346  -27.3576    2.5497  -16.6674   73.9788  -33.3296  ...    28.5028   -5.5488  -13.7564    0.0000  -29.9039   -3.0405  ...     0.0000   14.4408   14.8579   30.7819    0.0000   21.3908  ...   -29.0746    0.0000   -5.8475[torch.FloatTensor of size 2x4x512]# 注意力张量:p_attn: Variable containing:(0 ,.,.) =   1  0  0  0  0  1  0  0  0  0  1  0  0  0  0  1(1 ,.,.) =   1  0  0  0  0  1  0  0  0  0  1  0  0  0  0  1[torch.FloatTensor of size 2x4x4]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>带有mask的输入参数：</li></ul><pre class="line-numbers language-none"><code class="language-none">query = key = value = pe_result# 令mask为一个2x4x4的零张量mask = Variable(torch.zeros(2, 4, 4))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">attn, p_attn = attention(query, key, value, mask=mask)print("attn:", attn)print("p_attn:", p_attn)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>带有mask的输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># query的注意力表示:attn: Variable containing:( 0 ,.,.) =    0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770( 1 ,.,.) =   -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491[torch.FloatTensor of size 2x4x512]# 注意力张量:p_attn: Variable containing:(0 ,.,.) =   0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500(1 ,.,.) =   0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500  0.2500[torch.FloatTensor of size 2x4x4]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.3.2 注意力机制总结:</p><ul><li>学习了什么是注意力:<ul><li>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</li></ul></li></ul><hr><ul><li>什么是注意力计算规则:<ul><li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li></ul></li></ul><hr><ul><li>学习了Q, K, V的比喻解释:<ul><li>Q是一段准备被概括的文本; K是给出的提示; V是大脑中的对提示K的延伸.</li><li>当Q=K=V时, 称作自注意力机制.</li></ul></li></ul><hr><ul><li>什么是注意力机制:<ul><li>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.</li></ul></li></ul><hr><ul><li>学习并实现了注意力计算规则的函数: attention<ul><li>它的输入就是Q，K，V以及mask和dropout, mask用于掩码, dropout用于随机置0.</li><li>它的输出有两个, query的注意力表示以及注意力张量.</li></ul></li></ul></li></ul><hr><hr><h3 id="2-3-3-多头注意力机制"><a href="#2-3-3-多头注意力机制" class="headerlink" title="2.3.3 多头注意力机制"></a>2.3.3 多头注意力机制</h3><hr><ul><li>学习目标:<ul><li>了解多头注意力机制的作用.</li><li>掌握多头注意力机制的实现过程.</li></ul></li></ul><hr><ul><li>什么是多头注意力机制:<ul><li>从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是，我只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，这些变换不会改变原有张量的尺寸，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</li></ul></li></ul><hr><ul><li>多头注意力机制结构图:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/13.png" alt="avatar"></p><hr><ul><li>多头注意力机制的作用:<ul><li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li></ul></li></ul><hr><ul><li>多头注意力机制的代码实现:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 用于深度拷贝的copy工具包import copy# 首先需要定义克隆函数, 因为在多头注意力机制的实现中, 用到多个结构相同的线性层.# 我们将使用clone函数将他们一同初始化在一个网络层列表对象中. 之后的结构中也会用到该函数.def clones(module, N):    """用于生成相同网络层的克隆函数, 它的参数module表示要克隆的目标网络层, N代表需要克隆的数量"""    # 在函数中, 我们通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层,    # 然后将其放在nn.ModuleList类型的列表中存放.    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])# 我们使用一个类来实现多头注意力机制的处理class MultiHeadedAttention(nn.Module):    def __init__(self, head, embedding_dim, dropout=0.1):        """在类的初始化时, 会传入三个参数，head代表头数，embedding_dim代表词嵌入的维度，            dropout代表进行dropout操作时置0比率，默认是0.1."""        super(MultiHeadedAttention, self).__init__()        # 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，        # 这是因为我们之后要给每个头分配等量的词特征.也就是embedding_dim/head个.        assert embedding_dim % head == 0        # 得到每个头获得的分割词向量维度d_k        self.d_k = embedding_dim // head        # 传入头数h        self.head = head        # 然后获得线性层对象，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用clones函数克隆四个，        # 为什么是四个呢，这是因为在多头注意力中，Q，K，V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个.        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)        # self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None.        self.attn = None        # 最后就是一个self.dropout对象，它通过nn中的Dropout实例化而来，置0比率为传进来的参数dropout.        self.dropout = nn.Dropout(p=dropout)    def forward(self, query, key, value, mask=None):        """前向逻辑函数, 它的输入参数有四个，前三个就是注意力机制需要的Q, K, V，           最后一个是注意力机制中可能需要的mask掩码张量，默认是None. """        # 如果存在掩码张量mask        if mask is not None:            # 使用unsqueeze拓展维度            mask = mask.unsqueeze(0)        # 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本.        batch_size = query.size(0)        # 之后就进入多头处理环节        # 首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，将输入QKV分别传到线性层中，        # 做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加了一个维度h，代表头数，        # 这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，        # 计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，        # 为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，        # 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.        query, key, value = \           [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1, 2)            for model, x in zip(self.linears, (query, key, value))]        # 得到每个头的输入后，接下来就是将他们传入到attention中，        # 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中.        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)        # 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，        # 因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法，        # 这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，        # 所以，下一步就是使用view重塑形状，变成和输入形状相同.        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head * self.d_k)        # 最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.        return self.linears[-1](x)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>tensor.view演示:</li></ul><pre class="line-numbers language-none"><code class="language-none">&gt;&gt;&gt; x = torch.randn(4, 4)&gt;&gt;&gt; x.size()torch.Size([4, 4])&gt;&gt;&gt; y = x.view(16)&gt;&gt;&gt; y.size()torch.Size([16])&gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions&gt;&gt;&gt; z.size()torch.Size([2, 8])&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4)&gt;&gt;&gt; a.size()torch.Size([1, 2, 3, 4])&gt;&gt;&gt; b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension&gt;&gt;&gt; b.size()torch.Size([1, 3, 2, 4])&gt;&gt;&gt; c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory&gt;&gt;&gt; c.size()torch.Size([1, 3, 2, 4])&gt;&gt;&gt; torch.equal(b, c)False<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>torch.transpose演示:</li></ul><pre class="line-numbers language-none"><code class="language-none">&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 1.0028, -0.9893,  0.5809],        [-0.1669,  0.7299,  0.4942]])&gt;&gt;&gt; torch.transpose(x, 0, 1)tensor([[ 1.0028, -0.1669],        [-0.9893,  0.7299],        [ 0.5809,  0.4942]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 头数headhead = 8# 词嵌入维度embedding_dimembedding_dim = 512# 置零比率dropoutdropout = 0.2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 假设输入的Q，K，V仍然相等query = value = key = pe_result# 输入的掩码张量maskmask = Variable(torch.zeros(8, 4, 4))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">mha = MultiHeadedAttention(head, embedding_dim, dropout)mha_result = mha(query, key, value, mask)print(mha_result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[-0.3075,  1.5687, -2.5693,  ..., -1.1098,  0.0878, -3.3609],         [ 3.8065, -2.4538, -0.3708,  ..., -1.5205, -1.1488, -1.3984],         [ 2.4190,  0.5376, -2.8475,  ...,  1.4218, -0.4488, -0.2984],         [ 2.9356,  0.3620, -3.8722,  ..., -0.7996,  0.1468,  1.0345]],        [[ 1.1423,  0.6038,  0.0954,  ...,  2.2679, -5.7749,  1.4132],         [ 2.4066, -0.2777,  2.8102,  ...,  0.1137, -3.9517, -2.9246],         [ 5.8201,  1.1534, -1.9191,  ...,  0.1410, -7.6110,  1.0046],         [ 3.1209,  1.0008, -0.5317,  ...,  2.8619, -6.3204, -1.3435]]],       grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.3.3 多头注意力机制总结:</p><ul><li>学习了什么是多头注意力机制:<ul><li>每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头.将每个头的获得的输入送到注意力机制中, 就形成了多头注意力机制.</li></ul></li></ul><hr><ul><li>学习了多头注意力机制的作用:<ul><li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li></ul></li></ul><hr><ul><li>学习并实现了多头注意力机制的类: MultiHeadedAttention<ul><li>因为多头注意力机制中需要使用多个相同的线性层, 首先实现了克隆函数clones.</li><li>clones函数的输入是module，N，分别代表克隆的目标层，和克隆个数.</li><li>clones函数的输出是装有N个克隆层的Module列表.</li><li>接着实现MultiHeadedAttention类, 它的初始化函数输入是h, d_model, dropout分别代表头数，词嵌入维度和置零比率.</li><li>它的实例化对象输入是Q, K, V以及掩码张量mask.</li><li>它的实例化对象输出是通过多头注意力机制处理的Q的注意力表示.</li></ul></li></ul></li></ul><hr><hr><h3 id="2-3-4-前馈全连接层"><a href="#2-3-4-前馈全连接层" class="headerlink" title="2.3.4 前馈全连接层"></a>2.3.4 前馈全连接层</h3><hr><ul><li>学习目标:<ul><li>了解什么是前馈全连接层及其它的作用.</li><li>掌握前馈全连接层的实现过程.</li></ul></li></ul><hr><ul><li>什么是前馈全连接层:<ul><li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li></ul></li></ul><hr><ul><li>前馈全连接层的作用:<ul><li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</li></ul></li></ul><hr><ul><li>前馈全连接层的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 通过类PositionwiseFeedForward来实现前馈全连接层class PositionwiseFeedForward(nn.Module):    def __init__(self, d_model, d_ff, dropout=0.1):        """初始化函数有三个输入参数分别是d_model, d_ff,和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，           因为我们希望输入通过前馈全连接层后输入和输出的维度不变. 第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出维度.            最后一个是dropout置0比率."""        super(PositionwiseFeedForward, self).__init__()        # 首先按照我们预期使用nn实例化了两个线性层对象，self.w1和self.w2        # 它们的参数分别是d_model, d_ff和d_ff, d_model        self.w1 = nn.Linear(d_model, d_ff)        self.w2 = nn.Linear(d_ff, d_model)        # 然后使用nn的Dropout实例化了对象self.dropout        self.dropout = nn.Dropout(dropout)    def forward(self, x):        """输入参数为x，代表来自上一层的输出"""        # 首先经过第一个线性层，然后使用Funtional中relu函数进行激活,        # 之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果.        return self.w2(self.dropout(F.relu(self.w1(x))))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>ReLU函数公式: ReLU(x)=max(0, x)</li></ul><hr><ul><li>ReLU函数图像:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/ReLU.png" alt="avatar"></p><hr><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">d_model = 512# 线性变化的维度d_ff = 64dropout = 0.2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 输入参数x可以是多头注意力机制的输出x = mha_resulttensor([[[-0.3075,  1.5687, -2.5693,  ..., -1.1098,  0.0878, -3.3609],         [ 3.8065, -2.4538, -0.3708,  ..., -1.5205, -1.1488, -1.3984],         [ 2.4190,  0.5376, -2.8475,  ...,  1.4218, -0.4488, -0.2984],         [ 2.9356,  0.3620, -3.8722,  ..., -0.7996,  0.1468,  1.0345]],        [[ 1.1423,  0.6038,  0.0954,  ...,  2.2679, -5.7749,  1.4132],         [ 2.4066, -0.2777,  2.8102,  ...,  0.1137, -3.9517, -2.9246],         [ 5.8201,  1.1534, -1.9191,  ...,  0.1410, -7.6110,  1.0046],         [ 3.1209,  1.0008, -0.5317,  ...,  2.8619, -6.3204, -1.3435]]],       grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">ff = PositionwiseFeedForward(d_model, d_ff, dropout)ff_result = ff(x)print(ff_result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[-1.9488e+00, -3.4060e-01, -1.1216e+00,  ...,  1.8203e-01,          -2.6336e+00,  2.0917e-03],         [-2.5875e-02,  1.1523e-01, -9.5437e-01,  ..., -2.6257e-01,          -5.7620e-01, -1.9225e-01],         [-8.7508e-01,  1.0092e+00, -1.6515e+00,  ...,  3.4446e-02,          -1.5933e+00, -3.1760e-01],         [-2.7507e-01,  4.7225e-01, -2.0318e-01,  ...,  1.0530e+00,          -3.7910e-01, -9.7730e-01]],        [[-2.2575e+00, -2.0904e+00,  2.9427e+00,  ...,  9.6574e-01,          -1.9754e+00,  1.2797e+00],         [-1.5114e+00, -4.7963e-01,  1.2881e+00,  ..., -2.4882e-02,          -1.5896e+00, -1.0350e+00],         [ 1.7416e-01, -4.0688e-01,  1.9289e+00,  ..., -4.9754e-01,          -1.6320e+00, -1.5217e+00],         [-1.0874e-01, -3.3842e-01,  2.9379e-01,  ..., -5.1276e-01,          -1.6150e+00, -1.1295e+00]]], grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.3.4 前馈全连接层总结:</p><ul><li>学习了什么是前馈全连接层:<ul><li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li></ul></li></ul><hr><ul><li>学习了前馈全连接层的作用:<ul><li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</li></ul></li></ul><hr><ul><li>学习并实现了前馈全连接层的类: PositionwiseFeedForward<ul><li>它的实例化参数为d_model, d_ff, dropout, 分别代表词嵌入维度, 线性变换维度, 和置零比率.</li><li>它的输入参数x, 表示上层的输出.</li><li>它的输出是经过2层线性网络变换的特征表示.</li></ul></li></ul></li></ul><hr><hr><h3 id="2-3-5-规范化层"><a href="#2-3-5-规范化层" class="headerlink" title="2.3.5 规范化层"></a>2.3.5 规范化层</h3><hr><ul><li>学习目标:<ul><li>了解规范化层的作用.</li><li>掌握规范化层的实现过程.</li></ul></li></ul><hr><ul><li>规范化层的作用:<ul><li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li></ul></li></ul><hr><ul><li>规范化层的代码实现:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 通过LayerNorm实现规范化层的类class LayerNorm(nn.Module):    def __init__(self, features, eps=1e-6):        """初始化函数有两个参数, 一个是features, 表示词嵌入的维度,           另一个是eps它是一个足够小的数, 在规范化公式的分母中出现,           防止分母为0.默认是1e-6."""        super(LayerNorm, self).__init__()        # 根据features的形状初始化两个参数张量a2，和b2，第一个初始化为1张量，        # 也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，这两个张量就是规范化层的参数，        # 因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，        # 使其即能满足规范化要求，又能不改变针对目标的表征.最后使用nn.parameter封装，代表他们是模型的参数。        self.a2 = nn.Parameter(torch.ones(features))        self.b2 = nn.Parameter(torch.zeros(features))        # 把eps传到类中        self.eps = eps    def forward(self, x):        """输入参数x代表来自上一层的输出"""        # 在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致.        # 接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果，        # 最后对结果乘以我们的缩放参数，即a2，*号代表同型点乘，即对应位置进行乘法操作，加上位移参数b2.返回即可.        mean = x.mean(-1, keepdim=True)        std = x.std(-1, keepdim=True)        return self.a2 * (x - mean) / (std + self.eps) + self.b2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">features = d_model = 512eps = 1e-6<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 输入x来自前馈全连接层的输出x = ff_resulttensor([[[-1.9488e+00, -3.4060e-01, -1.1216e+00,  ...,  1.8203e-01,          -2.6336e+00,  2.0917e-03],         [-2.5875e-02,  1.1523e-01, -9.5437e-01,  ..., -2.6257e-01,          -5.7620e-01, -1.9225e-01],         [-8.7508e-01,  1.0092e+00, -1.6515e+00,  ...,  3.4446e-02,          -1.5933e+00, -3.1760e-01],         [-2.7507e-01,  4.7225e-01, -2.0318e-01,  ...,  1.0530e+00,          -3.7910e-01, -9.7730e-01]],        [[-2.2575e+00, -2.0904e+00,  2.9427e+00,  ...,  9.6574e-01,          -1.9754e+00,  1.2797e+00],         [-1.5114e+00, -4.7963e-01,  1.2881e+00,  ..., -2.4882e-02,          -1.5896e+00, -1.0350e+00],         [ 1.7416e-01, -4.0688e-01,  1.9289e+00,  ..., -4.9754e-01,          -1.6320e+00, -1.5217e+00],         [-1.0874e-01, -3.3842e-01,  2.9379e-01,  ..., -5.1276e-01,          -1.6150e+00, -1.1295e+00]]], grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">ln = LayerNorm(feature, eps)ln_result = ln(x)print(ln_result)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[ 2.2697,  1.3911, -0.4417,  ...,  0.9937,  0.6589, -1.1902],         [ 1.5876,  0.5182,  0.6220,  ...,  0.9836,  0.0338, -1.3393],         [ 1.8261,  2.0161,  0.2272,  ...,  0.3004,  0.5660, -0.9044],         [ 1.5429,  1.3221, -0.2933,  ...,  0.0406,  1.0603,  1.4666]],        [[ 0.2378,  0.9952,  1.2621,  ..., -0.4334, -1.1644,  1.2082],         [-1.0209,  0.6435,  0.4235,  ..., -0.3448, -1.0560,  1.2347],         [-0.8158,  0.7118,  0.4110,  ...,  0.0990, -1.4833,  1.9434],         [ 0.9857,  2.3924,  0.3819,  ...,  0.0157, -1.6300,  1.2251]]],       grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.3.5 规范化层总结:</p><ul><li>学习了规范化层的作用:<ul><li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li></ul></li></ul><hr><ul><li>学习并实现了规范化层的类: LayerNorm<ul><li>它的实例化参数有两个, features和eps，分别表示词嵌入特征大小，和一个足够小的数.</li><li>它的输入参数x代表来自上一层的输出.</li><li>它的输出就是经过规范化的特征表示.</li></ul></li></ul></li></ul><hr><hr><h3 id="2-3-6-子层连接结构"><a href="#2-3-6-子层连接结构" class="headerlink" title="2.3.6 子层连接结构"></a>2.3.6 子层连接结构</h3><hr><ul><li>学习目标:<ul><li>了解什么是子层连接结构.</li><li>掌握子层连接结构的实现过程.</li></ul></li></ul><hr><ul><li>什么是子层连接结构:<ul><li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构），在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li></ul></li></ul><hr><ul><li>子层连接结构图:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/15.png" alt="avatar"></p><hr><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/16.png" alt="avatar"></p><hr><ul><li>子层连接结构的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 使用SublayerConnection来实现子层连接结构的类class SublayerConnection(nn.Module):    def __init__(self, size, dropout=0.1):        """它输入参数有两个, size以及dropout， size一般是都是词嵌入维度的大小，            dropout本身是对模型结构中的节点数进行随机抑制的比率，            又因为节点被抑制等效就是该节点的输出都是0，因此也可以把dropout看作是对输出矩阵的随机置0的比率.        """        super(SublayerConnection, self).__init__()        # 实例化了规范化对象self.norm        self.norm = LayerNorm(size)        # 又使用nn中预定义的droupout实例化一个self.dropout对象.        self.dropout = nn.Dropout(p=dropout)    def forward(self, x, sublayer):        """前向逻辑函数中, 接收上一个层或者子层的输入作为第一个参数，           将该子层连接中的子层函数作为第二个参数"""        # 我们首先对输出进行规范化，然后将结果传给子层处理，之后再对子层进行dropout操作，        # 随机停止一些网络中神经元的作用，来防止过拟合. 最后还有一个add操作，         # 因为存在跳跃连接，所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出.        return x + self.dropout(sublayer(self.norm(x)))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>实例化参数</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">size = 512dropout = 0.2head = 8d_model = 512<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 令x为位置编码器的输出x = pe_resultmask = Variable(torch.zeros(8, 4, 4))# 假设子层中装的是多头注意力层, 实例化这个类self_attn =  MultiHeadedAttention(head, d_model)# 使用lambda获得一个函数类型的子层sublayer = lambda x: self_attn(x, x, x, mask)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">sc = SublayerConnection(size, dropout)sc_result = sc(x, sublayer)print(sc_result)print(sc_result.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[ 14.8830,  22.4106, -31.4739,  ...,  21.0882, -10.0338,  -0.2588],         [-25.1435,   2.9246, -16.1235,  ...,  10.5069,  -7.1007,  -3.7396],         [  0.1374,  32.6438,  12.3680,  ..., -12.0251, -40.5829,   2.2297],         [-13.3123,  55.4689,   9.5420,  ..., -12.6622,  23.4496,  21.1531]],        [[ 13.3533,  17.5674, -13.3354,  ...,  29.1366,  -6.4898,  35.8614],         [-35.2286,  18.7378, -31.4337,  ...,  11.1726,  20.6372,  29.8689],         [-30.7627,   0.0000, -57.0587,  ...,  15.0724, -10.7196, -18.6290],         [ -2.7757, -19.6408,   0.0000,  ...,  12.7660,  21.6843, -35.4784]]],       grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.3.6 子层连接结构总结:</p><ul><li>什么是子层连接结构:<ul><li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构）, 在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li></ul></li></ul><hr><ul><li>学习并实现了子层连接结构的类: SublayerConnection<ul><li>类的初始化函数输入参数是size, dropout, 分别代表词嵌入大小和置零比率.</li><li>它的实例化对象输入参数是x, sublayer, 分别代表上一层输出以及子层的函数表示.</li><li>它的输出就是通过子层连接结构处理的输出.</li></ul></li></ul></li></ul><hr><hr><h3 id="2-3-7-编码器层"><a href="#2-3-7-编码器层" class="headerlink" title="2.3.7 编码器层"></a>2.3.7 编码器层</h3><hr><ul><li>学习目标:<ul><li>了解编码器层的作用.</li><li>掌握编码器层的实现过程.</li></ul></li></ul><hr><ul><li>编码器层的作用:<ul><li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li></ul></li></ul><hr><ul><li>编码器层的构成图:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/17.png" alt="avatar"></p><hr><ul><li>编码器层的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 使用EncoderLayer类实现编码器层class EncoderLayer(nn.Module):    def __init__(self, size, self_attn, feed_forward, dropout):        """它的初始化函数参数有四个，分别是size，其实就是我们词嵌入维度的大小，它也将作为我们编码器层的大小,            第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制,            第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象, 最后一个是置0比率dropout."""        super(EncoderLayer, self).__init__()        # 首先将self_attn和feed_forward传入其中.        self.self_attn = self_attn        self.feed_forward = feed_forward        # 如图所示, 编码器层中有两个子层连接结构, 所以使用clones函数进行克隆        self.sublayer = clones(SublayerConnection(size, dropout), 2)        # 把size传入其中        self.size = size    def forward(self, x, mask):        """forward函数中有两个输入参数，x和mask，分别代表上一层的输出，和掩码张量mask."""        # 里面就是按照结构图左侧的流程. 首先通过第一个子层连接结构，其中包含多头自注意力子层，        # 然后通过第二个子层连接结构，其中包含前馈全连接子层. 最后返回结果.        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))        return self.sublayer[1](x, self.feed_forward)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">size = 512head = 8d_model = 512d_ff = 64x = pe_resultdropout = 0.2self_attn = MultiHeadedAttention(head, d_model)ff = PositionwiseFeedForward(d_model, d_ff, dropout)mask = Variable(torch.zeros(8, 4, 4))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">el = EncoderLayer(size, self_attn, ff, dropout)el_result = el(x, mask)print(el_result)print(el_result.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[ 33.6988, -30.7224,  20.9575,  ...,   5.2968, -48.5658,  20.0734],         [-18.1999,  34.2358,  40.3094,  ...,  10.1102,  58.3381,  58.4962],         [ 32.1243,  16.7921,  -6.8024,  ...,  23.0022, -18.1463, -17.1263],         [ -9.3475,  -3.3605, -55.3494,  ...,  43.6333,  -0.1900,   0.1625]],        [[ 32.8937, -46.2808,   8.5047,  ...,  29.1837,  22.5962, -14.4349],         [ 21.3379,  20.0657, -31.7256,  ..., -13.4079, -44.0706,  -9.9504],         [ 19.7478,  -1.0848,  11.8884,  ...,  -9.5794,   0.0675,  -4.7123],         [ -6.8023, -16.1176,  20.9476,  ...,  -6.5469,  34.8391, -14.9798]]],       grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.3.7 编码器层总结:</p><ul><li>学习了编码器层的作用:<ul><li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li></ul></li></ul><hr><ul><li>学习并实现了编码器层的类: EncoderLayer<ul><li>类的初始化函数共有4个, 别是size，其实就是我们词嵌入维度的大小. 第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制. 第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象. 最后一个是置0比率dropout.</li><li>实例化对象的输入参数有2个，x代表来自上一层的输出, mask代表掩码张量.</li><li>它的输出代表经过整个编码层的特征表示.</li></ul></li></ul></li></ul><hr><hr><h3 id="2-3-8-编码器"><a href="#2-3-8-编码器" class="headerlink" title="2.3.8 编码器"></a>2.3.8 编码器</h3><hr><ul><li>学习目标:<ul><li>了解编码器的作用.</li><li>掌握编码器的实现过程.</li></ul></li></ul><hr><ul><li>编码器的作用:<ul><li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li></ul></li></ul><hr><ul><li>编码器的结构图:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/7.png" alt="avatar"></p><hr><ul><li>编码器的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 使用Encoder类来实现编码器class Encoder(nn.Module):    def __init__(self, layer, N):        """初始化函数的两个参数分别代表编码器层和编码器层的个数"""        super(Encoder, self).__init__()        # 首先使用clones函数克隆N个编码器层放在self.layers中        self.layers = clones(layer, N)        # 再初始化一个规范化层, 它将用在编码器的最后面.        self.norm = LayerNorm(layer.size)    def forward(self, x, mask):        """forward函数的输入和编码器层相同, x代表上一层的输出, mask代表掩码张量"""        # 首先就是对我们克隆的编码器层进行循环，每次都会得到一个新的x，        # 这个循环的过程，就相当于输出的x经过了N个编码器层的处理.         # 最后再通过规范化层的对象self.norm进行处理，最后返回结果.         for layer in self.layers:            x = layer(x, mask)        return self.norm(x)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 第一个实例化参数layer, 它是一个编码器层的实例化对象, 因此需要传入编码器层的参数# 又因为编码器层中的子层是不共享的, 因此需要使用深度拷贝各个对象.size = 512head = 8d_model = 512d_ff = 64c = copy.deepcopyattn = MultiHeadedAttention(head, d_model)ff = PositionwiseFeedForward(d_model, d_ff, dropout)dropout = 0.2layer = EncoderLayer(size, c(attn), c(ff), dropout)# 编码器中编码器层的个数NN = 8mask = Variable(torch.zeros(8, 4, 4))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">en = Encoder(layer, N)en_result = en(x, mask)print(en_result)print(en_result.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[-0.2081, -0.3586, -0.2353,  ...,  2.5646, -0.2851,  0.0238],         [ 0.7957, -0.5481,  1.2443,  ...,  0.7927,  0.6404, -0.0484],         [-0.1212,  0.4320, -0.5644,  ...,  1.3287, -0.0935, -0.6861],         [-0.3937, -0.6150,  2.2394,  ..., -1.5354,  0.7981,  1.7907]],        [[-2.3005,  0.3757,  1.0360,  ...,  1.4019,  0.6493, -0.1467],         [ 0.5653,  0.1569,  0.4075,  ..., -0.3205,  1.4774, -0.5856],         [-1.0555,  0.0061, -1.8165,  ..., -0.4339, -1.8780,  0.2467],         [-2.1617, -1.5532, -1.4330,  ..., -0.9433, -0.5304, -1.7022]]],       grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.3.8 编码器总结:</p><ul><li>学习了编码器的作用:<ul><li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li></ul></li></ul><hr><ul><li>学习并实现了编码器的类: Encoder<ul><li>类的初始化函数参数有两个，分别是layer和N，代表编码器层和编码器层的个数.</li><li>forward函数的输入参数也有两个, 和编码器层的forward相同, x代表上一层的输出, mask代码掩码张量.</li><li>编码器类的输出就是Transformer中编码器的特征提取表示, 它将成为解码器的输入的一部分.</li></ul></li></ul></li></ul><hr><hr><hr><h2 id="2-4-解码器部分实现"><a href="#2-4-解码器部分实现" class="headerlink" title="2.4 解码器部分实现"></a>2.4 解码器部分实现</h2><hr><h3 id="学习目标-3"><a href="#学习目标-3" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解解码器中各个组成部分的作用.</li><li>掌握解码器中各个组成部分的实现过程.</li></ul><hr><ul><li>解码器部分:<ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul></li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/8.png" alt="avatar"></p><hr><ul><li>说明:<ul><li>解码器层中的各个部分，如，多头注意力机制，规范化层，前馈全连接网络，子层连接结构都与编码器中的实现相同. 因此这里可以直接拿来构建解码器层.</li></ul></li></ul><hr><hr><h3 id="2-4-1-解码器层"><a href="#2-4-1-解码器层" class="headerlink" title="2.4.1 解码器层"></a>2.4.1 解码器层</h3><hr><ul><li>学习目标:<ul><li>了解解码器层的作用.</li><li>掌握解码器层的实现过程.</li></ul></li></ul><hr><ul><li>解码器层的作用:<ul><li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li></ul></li></ul><hr><ul><li>解码器层的代码实现:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 使用DecoderLayer的类实现解码器层class DecoderLayer(nn.Module):    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):        """初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，            第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，             第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.        """        super(DecoderLayer, self).__init__()        # 在初始化函数中， 主要就是将这些输入传到类中        self.size = size        self.self_attn = self_attn        self.src_attn = src_attn        self.feed_forward = feed_forward        # 按照结构图使用clones函数克隆三个子层连接对象.        self.sublayer = clones(SublayerConnection(size, dropout), 3)    def forward(self, x, memory, source_mask, target_mask):        """forward函数中的参数有4个，分别是来自上一层的输入x，           来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.        """        # 将memory表示成m方便之后使用        m = memory        # 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，        # 最后一个参数是目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据，        # 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，        # 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，        # 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用.        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))        # 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k，v是编码层输出memory，         # 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，而是遮蔽掉对结果没有意义的字符而产生的注意力值，        # 以此提升模型效果和训练速度. 这样就完成了第二个子层的处理.        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, source_mask))        # 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果.这就是我们的解码器层结构.        return self.sublayer[2](x, self.feed_forward)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 类的实例化参数与解码器层类似, 相比多出了src_attn, 但是和self_attn是同一个类.head = 8size = 512d_model = 512d_ff = 64dropout = 0.2self_attn = src_attn = MultiHeadedAttention(head, d_model, dropout)# 前馈全连接层也和之前相同 ff = PositionwiseFeedForward(d_model, d_ff, dropout)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># x是来自目标数据的词嵌入表示, 但形式和源数据的词嵌入表示相同, 这里使用per充当.x = pe_result# memory是来自编码器的输出memory = en_result# 实际中source_mask和target_mask并不相同, 这里为了方便计算使他们都为maskmask = Variable(torch.zeros(8, 4, 4))source_mask = target_mask = mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)dl_result = dl(x, memory, source_mask, target_mask)print(dl_result)print(dl_result.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[ 1.9604e+00,  3.9288e+01, -5.2422e+01,  ...,  2.1041e-01,          -5.5063e+01,  1.5233e-01],         [ 1.0135e-01, -3.7779e-01,  6.5491e+01,  ...,  2.8062e+01,          -3.7780e+01, -3.9577e+01],         [ 1.9526e+01, -2.5741e+01,  2.6926e-01,  ..., -1.5316e+01,           1.4543e+00,  2.7714e+00],         [-2.1528e+01,  2.0141e+01,  2.1999e+01,  ...,  2.2099e+00,          -1.7267e+01, -1.6687e+01]],        [[ 6.7259e+00, -2.6918e+01,  1.1807e+01,  ..., -3.6453e+01,          -2.9231e+01,  1.1288e+01],         [ 7.7484e+01, -5.0572e-01, -1.3096e+01,  ...,  3.6302e-01,           1.9907e+01, -1.2160e+00],         [ 2.6703e+01,  4.4737e+01, -3.1590e+01,  ...,  4.1540e-03,           5.2587e+00,  5.2382e+00],         [ 4.7435e+01, -3.7599e-01,  5.0898e+01,  ...,  5.6361e+00,           3.5891e+01,  1.5697e+01]]], grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.4.1 解码器层总结:</p><ul><li>学习了解码器层的作用:<ul><li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li></ul></li></ul><hr><ul><li>学习并实现了解码器层的类: DecoderLayer<ul><li>类的初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.</li><li>forward函数的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</li><li>最终输出了由编码器输入和目标数据一同作用的特征提取结果.</li></ul></li></ul></li></ul><hr><hr><h3 id="2-4-2-解码器"><a href="#2-4-2-解码器" class="headerlink" title="2.4.2 解码器"></a>2.4.2 解码器</h3><hr><ul><li>学习目标:<ul><li>了解解码器的作用.</li><li>掌握解码器的实现过程.</li></ul></li></ul><hr><ul><li>解码器的作用:<ul><li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示.</li></ul></li></ul><hr><ul><li>解码器的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 使用类Decoder来实现解码器class Decoder(nn.Module):    def __init__(self, layer, N):        """初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N."""        super(Decoder, self).__init__()        # 首先使用clones方法克隆了N个layer，然后实例化了一个规范化层.         # 因为数据走过了所有的解码器层后最后要做规范化处理.         self.layers = clones(layer, N)        self.norm = LayerNorm(layer.size)    def forward(self, x, memory, source_mask, target_mask):        """forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，           source_mask, target_mask代表源数据和目标数据的掩码张量"""        # 然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，        # 得出最后的结果，再进行一次规范化返回即可.         for layer in self.layers:            x = layer(x, memory, source_mask, target_mask)        return self.norm(x)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 分别是解码器层layer和解码器层的个数Nsize = 512d_model = 512head = 8d_ff = 64dropout = 0.2c = copy.deepcopyattn = MultiHeadedAttention(head, d_model)ff = PositionwiseFeedForward(d_model, d_ff, dropout)layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)N = 8<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 输入参数与解码器层的输入参数相同x = pe_resultmemory = en_resultmask = Variable(torch.zeros(8, 4, 4))source_mask = target_mask = mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">de = Decoder(layer, N)de_result = de(x, memory, source_mask, target_mask)print(de_result)print(de_result.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[ 0.9898, -0.3216, -1.2439,  ...,  0.7427, -0.0717, -0.0814],         [-0.7432,  0.6985,  1.5551,  ...,  0.5232, -0.5685,  1.3387],         [ 0.2149,  0.5274, -1.6414,  ...,  0.7476,  0.5082, -3.0132],         [ 0.4408,  0.9416,  0.4522,  ..., -0.1506,  1.5591, -0.6453]],        [[-0.9027,  0.5874,  0.6981,  ...,  2.2899,  0.2933, -0.7508],         [ 1.2246, -1.0856, -0.2497,  ..., -1.2377,  0.0847, -0.0221],         [ 3.4012, -0.4181, -2.0968,  ..., -1.5427,  0.1090, -0.3882],         [-0.1050, -0.5140, -0.6494,  ..., -0.4358, -1.2173,  0.4161]]],       grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li><p>2.4.2 解码器总结:</p><ul><li>学习了解码器的作用:<ul><li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示.</li></ul></li></ul><hr><ul><li>学习并实现了解码器的类: Decoder<ul><li>类的初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N.</li><li>forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，src_mask, tgt_mask代表源数据和目标数据的掩码张量.</li><li>输出解码过程的最终特征表示.</li></ul></li></ul></li></ul><hr><hr><hr><h2 id="2-5-输出部分实现"><a href="#2-5-输出部分实现" class="headerlink" title="2.5 输出部分实现"></a>2.5 输出部分实现</h2><hr><h3 id="学习目标-4"><a href="#学习目标-4" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解线性层和softmax的作用.</li><li>掌握线性层和softmax的实现过程.</li></ul><hr><ul><li>输出部分包含:<ul><li>线性层</li><li>softmax层</li></ul></li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/6.png" alt="avatar"></p><hr><h3 id="线性层的作用"><a href="#线性层的作用" class="headerlink" title="线性层的作用"></a>线性层的作用</h3><ul><li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li></ul><hr><h3 id="softmax层的作用"><a href="#softmax层的作用" class="headerlink" title="softmax层的作用"></a>softmax层的作用</h3><ul><li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li></ul><hr><ul><li>线性层和softmax层的代码分析:</li></ul><pre class="line-numbers language-none"><code class="language-none"># nn.functional工具包装载了网络层中那些只进行计算, 而没有参数的层import torch.nn.functional as F# 将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构# 因此把类的名字叫做Generator, 生成器类class Generator(nn.Module):    def __init__(self, d_model, vocab_size):        """初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小."""        super(Generator, self).__init__()        # 首先就是使用nn中的预定义线性层进行实例化, 得到一个对象self.project等待使用,         # 这个线性层的参数有两个, 就是初始化函数传进来的两个参数: d_model, vocab_size        self.project = nn.Linear(d_model, vocab_size)    def forward(self, x):        """前向逻辑函数中输入是上一层的输出张量x"""        # 在函数中, 首先使用上一步得到的self.project对x进行线性变化,         # 然后使用F中已经实现的log_softmax进行的softmax处理.        # 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.        # log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数,         # 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.        return F.log_softmax(self.project(x), dim=-1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>nn.Linear演示:</li></ul><pre class="line-numbers language-none"><code class="language-none">&gt;&gt;&gt; m = nn.Linear(20, 30)&gt;&gt;&gt; input = torch.randn(128, 20)&gt;&gt;&gt; output = m(input)&gt;&gt;&gt; print(output.size())torch.Size([128, 30])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 词嵌入维度是512维d_model = 512# 词表大小是1000vocab_size = 1000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 输入x是上一层网络的输出, 我们使用来自解码器层的输出x = de_result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">gen = Generator(d_model, vocab_size)gen_result = gen(x)print(gen_result)print(gen_result.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[-7.8098, -7.5260, -6.9244,  ..., -7.6340, -6.9026, -7.5232],         [-6.9093, -7.3295, -7.2972,  ..., -6.6221, -7.2268, -7.0772],         [-7.0263, -7.2229, -7.8533,  ..., -6.7307, -6.9294, -7.3042],         [-6.5045, -6.0504, -6.6241,  ..., -5.9063, -6.5361, -7.1484]],        [[-7.1651, -6.0224, -7.4931,  ..., -7.9565, -8.0460, -6.6490],         [-6.3779, -7.6133, -8.3572,  ..., -6.6565, -7.1867, -6.5112],         [-6.4914, -6.9289, -6.2634,  ..., -6.2471, -7.5348, -6.8541],         [-6.8651, -7.0460, -7.6239,  ..., -7.1411, -6.5496, -7.3749]]],       grad_fn=&lt;LogSoftmaxBackward&gt;)torch.Size([2, 4, 1000])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="小节总结-2"><a href="#小节总结-2" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了输出部分包含:</p><ul><li>线性层</li><li>softmax层</li></ul><hr></li><li><p>线性层的作用:</p><ul><li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li></ul><hr></li><li><p>softmax层的作用:</p><ul><li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li></ul><hr></li><li><p>学习并实现了线性层和softmax层的类: Generator</p><ul><li>初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小.</li><li>forward函数接受上一层的输出.</li><li>最终获得经过线性层和softmax层处理的结果.</li></ul></li></ul><hr><hr><hr><h2 id="2-6-模型构建"><a href="#2-6-模型构建" class="headerlink" title="2.6 模型构建"></a>2.6 模型构建</h2><hr><h3 id="学习目标-5"><a href="#学习目标-5" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握编码器-解码器结构的实现过程.</li><li>掌握Transformer模型的构建过程.</li></ul><hr><ul><li>通过上面的小节, 我们已经完成了所有组成部分的实现, 接下来就来实现完整的编码器-解码器结构.</li></ul><hr><ul><li>Transformer总体架构图:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/4.png" alt="avatar"></p><hr><h3 id="编码器-解码器结构的代码实现"><a href="#编码器-解码器结构的代码实现" class="headerlink" title="编码器-解码器结构的代码实现"></a>编码器-解码器结构的代码实现</h3><pre class="line-numbers language-none"><code class="language-none"># 使用EncoderDecoder类来实现编码器-解码器结构class EncoderDecoder(nn.Module):    def __init__(self, encoder, decoder, source_embed, target_embed, generator):        """初始化函数中有5个参数, 分别是编码器对象, 解码器对象,            源数据嵌入函数, 目标数据嵌入函数,  以及输出部分的类别生成器对象        """        super(EncoderDecoder, self).__init__()        # 将参数传入到类中        self.encoder = encoder        self.decoder = decoder        self.src_embed = source_embed        self.tgt_embed = target_embed        self.generator = generator    def forward(self, source, target, source_mask, target_mask):        """在forward函数中，有四个参数, source代表源数据, target代表目标数据,            source_mask和target_mask代表对应的掩码张量"""        # 在函数中, 将source, source_mask传入编码函数, 得到结果后,        # 与source_mask，target，和target_mask一同传给解码函数.        return self.decode(self.encode(source, source_mask), source_mask,                            target, target_mask)    def encode(self, source, source_mask):        """编码函数, 以source和source_mask为参数"""        # 使用src_embed对source做处理, 然后和source_mask一起传给self.encoder        return self.encoder(self.src_embed(source), source_mask)    def decode(self, memory, source_mask, target, target_mask):        """解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数"""        # 使用tgt_embed对target做处理, 然后和source_mask, target_mask, memory一起传给self.decoder        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>实例化参数</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">vocab_size = 1000d_model = 512encoder = endecoder = desource_embed = nn.Embedding(vocab_size, d_model)target_embed = nn.Embedding(vocab_size, d_model)generator = gen<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 假设源数据与目标数据相同, 实际中并不相同source = target = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))# 假设src_mask与tgt_mask相同，实际中并不相同source_mask = target_mask = Variable(torch.zeros(8, 4, 4))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">ed = EncoderDecoder(encoder, decoder, source_embed, target_embed, generator)ed_result = ed(source, target, source_mask, target_mask)print(ed_result)print(ed_result.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[[ 0.2102, -0.0826, -0.0550,  ...,  1.5555,  1.3025, -0.6296],         [ 0.8270, -0.5372, -0.9559,  ...,  0.3665,  0.4338, -0.7505],         [ 0.4956, -0.5133, -0.9323,  ...,  1.0773,  1.1913, -0.6240],         [ 0.5770, -0.6258, -0.4833,  ...,  0.1171,  1.0069, -1.9030]],        [[-0.4355, -1.7115, -1.5685,  ..., -0.6941, -0.1878, -0.1137],         [-0.8867, -1.2207, -1.4151,  ..., -0.9618,  0.1722, -0.9562],         [-0.0946, -0.9012, -1.6388,  ..., -0.2604, -0.3357, -0.6436],         [-1.1204, -1.4481, -1.5888,  ..., -0.8816, -0.6497,  0.0606]]],       grad_fn=&lt;AddBackward0&gt;)torch.Size([2, 4, 512])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>接着将基于以上结构构建用于训练的模型.</li></ul><hr><h3 id="Tansformer模型构建过程的代码分析"><a href="#Tansformer模型构建过程的代码分析" class="headerlink" title="Tansformer模型构建过程的代码分析"></a>Tansformer模型构建过程的代码分析</h3><pre class="line-numbers language-none"><code class="language-none">def make_model(source_vocab, target_vocab, N=6,                d_model=512, d_ff=2048, head=8, dropout=0.1):    """该函数用来构建模型, 有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，       编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，       多头注意力结构中的多头数，以及置零比率dropout."""    # 首先得到一个深度拷贝命令，接下来很多结构都需要进行深度拷贝，    # 来保证他们彼此之间相互独立，不受干扰.    c = copy.deepcopy    # 实例化了多头注意力类，得到对象attn    attn = MultiHeadedAttention(head, d_model)    # 然后实例化前馈全连接类，得到对象ff     ff = PositionwiseFeedForward(d_model, d_ff, dropout)    # 实例化位置编码类，得到对象position    position = PositionalEncoding(d_model, dropout)    # 根据结构图, 最外层是EncoderDecoder，在EncoderDecoder中，    # 分别是编码器层，解码器层，源数据Embedding层和位置编码组成的有序结构，    # 目标数据Embedding层和位置编码组成的有序结构，以及类别生成器层.     # 在编码器层中有attention子层以及前馈全连接子层，    # 在解码器层中有两个attention子层以及前馈全连接层.    model = EncoderDecoder(        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),        Decoder(DecoderLayer(d_model, c(attn), c(attn),                              c(ff), dropout), N),        nn.Sequential(Embeddings(d_model, source_vocab), c(position)),        nn.Sequential(Embeddings(d_model, target_vocab), c(position)),        Generator(d_model, target_vocab))    # 模型结构完成后，接下来就是初始化模型中的参数，比如线性层中的变换矩阵    # 这里一但判断参数的维度大于1，则会将其初始化成一个服从均匀分布的矩阵，    for p in model.parameters():        if p.dim() &gt; 1:            nn.init.xavier_uniform(p)    return model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>nn.init.xavier_uniform演示:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 结果服从均匀分布U(-a, a)&gt;&gt;&gt; w = torch.empty(3, 5)&gt;&gt;&gt; w = nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))&gt;&gt;&gt; wtensor([[-0.7742,  0.5413,  0.5478, -0.4806, -0.2555],        [-0.8358,  0.4673,  0.3012,  0.3882, -0.6375],        [ 0.4622, -0.0794,  0.1851,  0.8462, -0.3591]])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">source_vocab = 11target_vocab = 11 N = 6# 其他参数都使用默认值 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">if __name__ == '__main__':    res = make_model(source_vocab, target_vocab, N)    print(res)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 根据Transformer结构图构建的最终模型结构EncoderDecoder(  (encoder): Encoder(    (layers): ModuleList(      (0): EncoderLayer(        (self_attn): MultiHeadedAttention(          (linears): ModuleList(            (0): Linear(in_features=512, out_features=512)            (1): Linear(in_features=512, out_features=512)            (2): Linear(in_features=512, out_features=512)            (3): Linear(in_features=512, out_features=512)          )          (dropout): Dropout(p=0.1)        )        (feed_forward): PositionwiseFeedForward(          (w_1): Linear(in_features=512, out_features=2048)          (w_2): Linear(in_features=2048, out_features=512)          (dropout): Dropout(p=0.1)        )        (sublayer): ModuleList(          (0): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )          (1): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )        )      )      (1): EncoderLayer(        (self_attn): MultiHeadedAttention(          (linears): ModuleList(            (0): Linear(in_features=512, out_features=512)            (1): Linear(in_features=512, out_features=512)            (2): Linear(in_features=512, out_features=512)            (3): Linear(in_features=512, out_features=512)          )          (dropout): Dropout(p=0.1)        )        (feed_forward): PositionwiseFeedForward(          (w_1): Linear(in_features=512, out_features=2048)          (w_2): Linear(in_features=2048, out_features=512)          (dropout): Dropout(p=0.1)        )        (sublayer): ModuleList(          (0): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )          (1): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )        )      )    )    (norm): LayerNorm(    )  )  (decoder): Decoder(    (layers): ModuleList(      (0): DecoderLayer(        (self_attn): MultiHeadedAttention(          (linears): ModuleList(            (0): Linear(in_features=512, out_features=512)            (1): Linear(in_features=512, out_features=512)            (2): Linear(in_features=512, out_features=512)            (3): Linear(in_features=512, out_features=512)          )          (dropout): Dropout(p=0.1)        )        (src_attn): MultiHeadedAttention(          (linears): ModuleList(            (0): Linear(in_features=512, out_features=512)            (1): Linear(in_features=512, out_features=512)            (2): Linear(in_features=512, out_features=512)            (3): Linear(in_features=512, out_features=512)          )          (dropout): Dropout(p=0.1)        )        (feed_forward): PositionwiseFeedForward(          (w_1): Linear(in_features=512, out_features=2048)          (w_2): Linear(in_features=2048, out_features=512)          (dropout): Dropout(p=0.1)        )        (sublayer): ModuleList(          (0): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )          (1): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )          (2): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )        )      )      (1): DecoderLayer(        (self_attn): MultiHeadedAttention(          (linears): ModuleList(            (0): Linear(in_features=512, out_features=512)            (1): Linear(in_features=512, out_features=512)            (2): Linear(in_features=512, out_features=512)            (3): Linear(in_features=512, out_features=512)          )          (dropout): Dropout(p=0.1)        )        (src_attn): MultiHeadedAttention(          (linears): ModuleList(            (0): Linear(in_features=512, out_features=512)            (1): Linear(in_features=512, out_features=512)            (2): Linear(in_features=512, out_features=512)            (3): Linear(in_features=512, out_features=512)          )          (dropout): Dropout(p=0.1)        )        (feed_forward): PositionwiseFeedForward(          (w_1): Linear(in_features=512, out_features=2048)          (w_2): Linear(in_features=2048, out_features=512)          (dropout): Dropout(p=0.1)        )        (sublayer): ModuleList(          (0): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )          (1): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )          (2): SublayerConnection(            (norm): LayerNorm(            )            (dropout): Dropout(p=0.1)          )        )      )    )    (norm): LayerNorm(    )  )  (src_embed): Sequential(    (0): Embeddings(      (lut): Embedding(11, 512)    )    (1): PositionalEncoding(      (dropout): Dropout(p=0.1)    )  )  (tgt_embed): Sequential(    (0): Embeddings(      (lut): Embedding(11, 512)    )    (1): PositionalEncoding(      (dropout): Dropout(p=0.1)    )  )  (generator): Generator(    (proj): Linear(in_features=512, out_features=11)  ))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="小节总结-3"><a href="#小节总结-3" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习并实现了编码器-解码器结构的类: EncoderDecoder</p><ul><li>类的初始化函数传入5个参数, 分别是编码器对象, 解码器对象, 源数据嵌入函数, 目标数据嵌入函数, 以及输出部分的类别生成器对象.</li><li>类中共实现三个函数, forward, encode, decode</li><li>forward是主要逻辑函数, 有四个参数, source代表源数据, target代表目标数据, source_mask和target_mask代表对应的掩码张量.</li><li>encode是编码函数, 以source和source_mask为参数.</li><li>decode是解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数</li></ul><hr></li><li><p>学习并实现了模型构建函数: make_model</p><ul><li>有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，多头注意力结构中的多头数，以及置零比率dropout.</li><li>该函数最后返回一个构建好的模型对象.</li></ul></li></ul><hr><hr><hr><h2 id="2-7-模型基本测试运行"><a href="#2-7-模型基本测试运行" class="headerlink" title="2.7 模型基本测试运行"></a>2.7 模型基本测试运行</h2><hr><h3 id="学习目标-6"><a href="#学习目标-6" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解Transformer模型基本测试的copy任务.</li><li>掌握实现copy任务的四步曲.</li></ul><hr><ul><li>我们将通过一个小的copy任务完成模型的基本测试工作.</li></ul><hr><ul><li>copy任务介绍:<ul><li>任务描述: 针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</li><li>任务意义: copy任务在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力.</li></ul></li></ul><hr><h3 id="使用copy任务进行模型基本测试的四步曲"><a href="#使用copy任务进行模型基本测试的四步曲" class="headerlink" title="使用copy任务进行模型基本测试的四步曲"></a>使用copy任务进行模型基本测试的四步曲</h3><ul><li>第一步: 构建数据集生成器</li><li>第二步: 获得Transformer模型及其优化器和损失函数</li><li>第三步: 运行模型进行训练和评估</li><li>第四步: 使用模型进行贪婪解码</li></ul><hr><ul><li>第一步: 构建数据集生成器</li></ul><pre class="line-numbers language-none"><code class="language-none"># 导入工具包Batch, 它能够对原始样本数据生成对应批次的掩码张量from pyitcast.transformer_utils import Batch  def data_generator(V, batch, num_batch):    """该函数用于随机生成copy任务的数据, 它的三个输入参数是V: 随机生成数字的最大值+1,        batch: 每次输送给模型更新一次参数的数据量, num_batch: 一共输送num_batch次完成一轮    """    # 使用for循环遍历nbatches    for i in range(num_batch):        # 在循环中使用np的random.randint方法随机生成[1, V)的整数,         # 分布在(batch, 10)形状的矩阵中, 然后再把numpy形式转换称torch中的tensor.        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))        # 接着使数据矩阵中的第一列数字都为1, 这一列也就成为了起始标志列,         # 当解码器进行第一次解码的时候, 会使用起始标志列作为输入.        data[:, 0] = 1        # 因为是copy任务, 所有source与target是完全相同的, 且数据样本作用变量不需要求梯度        # 因此requires_grad设置为False        source = Variable(data, requires_grad=False)        target = Variable(data, requires_grad=False)        # 使用Batch对source和target进行对应批次的掩码张量生成, 最后使用yield返回        yield Batch(source, target) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 将生成0-10的整数V = 11# 每次喂给模型20个数据进行参数更新batch = 20 # 连续喂30次完成全部数据的遍历, 也就是1轮num_batch = 30<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">if __name__ == '__main__':    res = data_generator(V, batch, num_batch)    print(res)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 会得到一个数据生成器(生成器对象)&lt;generator object data_gen at 0x10c053e08&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><ul><li>第二步: 获得Transformer模型及其优化器和损失函数</li></ul><pre class="line-numbers language-none"><code class="language-none"># 导入优化器工具包get_std_opt, 该工具用于获得标准的针对Transformer模型的优化器 # 该标准优化器基于Adam优化器, 使其对序列到序列的任务更有效.from pyitcast.transformer_utils import get_std_opt# 导入标签平滑工具包, 该工具用于标签平滑, 标签平滑的作用就是小幅度的改变原有标签值的值域# 因为在理论上即使是人工的标注数据也可能并非完全正确, 会受到一些外界因素的影响而产生一些微小的偏差# 因此使用标签平滑来弥补这种偏差, 减少模型对某一条规律的绝对认知, 以防止过拟合. 通过下面示例了解更多.from pyitcast.transformer_utils import LabelSmoothing# 导入损失计算工具包, 该工具能够使用标签平滑后的结果进行损失的计算, # 损失的计算方法可以认为是交叉熵损失函数.from pyitcast.transformer_utils import SimpleLossCompute# 使用make_model获得modelmodel = make_model(V, V, N=2)# 使用get_std_opt获得模型优化器model_optimizer = get_std_opt(model)# 使用LabelSmoothing获得标签平滑对象criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)# 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法loss = SimpleLossCompute(model.generator, criterion, model_optimizer)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>标签平滑示例:</li></ul><pre class="line-numbers language-none"><code class="language-none">from pyitcast.transformer_utils import LabelSmoothing# 使用LabelSmoothing实例化一个crit对象.# 第一个参数size代表目标数据的词汇总数, 也是模型最后一层得到张量的最后一维大小# 这里是5说明目标词汇总数是5个. 第二个参数padding_idx表示要将那些tensor中的数字# 替换成0, 一般padding_idx=0表示不进行替换. 第三个参数smoothing, 表示标签的平滑程度# 如原来标签的表示值为1, 则平滑后它的值域变为[1-smoothing, 1+smoothing].crit = LabelSmoothing(size=5, padding_idx=0, smoothing=0.5)# 假定一个任意的模型最后输出预测结果和真实结果predict = Variable(torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],                             [0, 0.2, 0.7, 0.1, 0],                              [0, 0.2, 0.7, 0.1, 0]]))# 标签的表示值是0，1，2target = Variable(torch.LongTensor([2, 1, 0]))# 将predict, target传入到对象中crit(predict, target)# 绘制标签平滑图像plt.imshow(crit.true_dist)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>标签平滑图像:</li></ul><p><img src="/2021/04/27/transformer-jia-gou-jie-xi/18.png" alt="avatar"></p><hr><ul><li>标签平滑图像分析:<ul><li>我们目光集中在黄色小方块上, 它相对于横坐标横跨的值域就是标签平滑后的正向平滑值域, 我们可以看到大致是从0.5到2.5.</li><li>它相对于纵坐标横跨的值域就是标签平滑后的负向平滑值域, 我们可以看到大致是从-0.5到1.5, 总的值域空间由原来的[0, 2]变成了[-0.5, 2.5].</li></ul></li></ul><hr><ul><li>第三步: 运行模型进行训练和评估</li></ul><pre class="line-numbers language-none"><code class="language-none"># 导入模型单轮训练工具包run_epoch, 该工具将对模型使用给定的损失函数计算方法进行单轮参数更新.# 并打印每轮参数更新的损失结果.from pyitcast.transformer_utils import run_epochdef run(model, loss, epochs=10):    """模型训练函数, 共有三个参数, model代表将要进行训练的模型       loss代表使用的损失计算方法, epochs代表模型训练的轮数"""    # 遍历轮数    for epoch in range(epochs):        # 模型使用训练模式, 所有参数将被更新        model.train()        # 训练时, batch_size是20        run_epoch(data_generator(V, 8, 20), model, loss)        # 模型使用评估模式, 参数将不会变化         model.eval()        # 评估时, batch_size是5        run_epoch(data_generator(V, 8, 5), model, loss)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 进行10轮训练epochs = 10# model和loss都是来自上一步的结果<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">Epoch Step: 1 Loss: 3.315704 Tokens per Sec: 309.740843Epoch Step: 1 Loss: 2.602743 Tokens per Sec: 393.885743Epoch Step: 1 Loss: 2.563469 Tokens per Sec: 347.746994Epoch Step: 1 Loss: 2.065951 Tokens per Sec: 422.632783Epoch Step: 1 Loss: 2.218468 Tokens per Sec: 346.982987Epoch Step: 1 Loss: 1.771149 Tokens per Sec: 396.451901Epoch Step: 1 Loss: 1.979203 Tokens per Sec: 350.384045Epoch Step: 1 Loss: 1.648887 Tokens per Sec: 361.534817Epoch Step: 1 Loss: 1.824539 Tokens per Sec: 349.660287Epoch Step: 1 Loss: 1.550169 Tokens per Sec: 319.302558Epoch Step: 1 Loss: 1.676636 Tokens per Sec: 369.678638Epoch Step: 1 Loss: 1.394759 Tokens per Sec: 364.660371Epoch Step: 1 Loss: 1.473153 Tokens per Sec: 324.016068Epoch Step: 1 Loss: 1.142609 Tokens per Sec: 422.345444Epoch Step: 1 Loss: 1.410883 Tokens per Sec: 365.395922Epoch Step: 1 Loss: 0.828656 Tokens per Sec: 401.538655Epoch Step: 1 Loss: 1.254409 Tokens per Sec: 346.133228Epoch Step: 1 Loss: 0.745532 Tokens per Sec: 402.395937Epoch Step: 1 Loss: 0.952969 Tokens per Sec: 324.858870Epoch Step: 1 Loss: 0.373509 Tokens per Sec: 358.814760<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>第四步: 使用模型进行贪婪解码</li></ul><pre class="line-numbers language-none"><code class="language-none"># 导入贪婪解码工具包greedy_decode, 该工具将对最终结进行贪婪解码# 贪婪解码的方式是每次预测都选择概率最大的结果作为输出, # 它不一定能获得全局最优性, 但却拥有最高的执行效率.from pyitcast.transformer_utils import greedy_decode def run(model, loss, epochs=10):    for epoch in range(epochs):        model.train()        run_epoch(data_generator(V, 8, 20), model, loss)        model.eval()        run_epoch(data_generator(V, 8, 5), model, loss)    # 模型进入测试模式    model.eval()    # 假定的输入张量    source = Variable(torch.LongTensor([[1,3,2,5,4,6,7,8,9,10]]))    # 定义源数据掩码张量, 因为元素都是1, 在我们这里1代表不遮掩    # 因此相当于对源数据没有任何遮掩.    source_mask = Variable(torch.ones(1, 1, 10))    # 最后将model, src, src_mask, 解码的最大长度限制max_len, 默认为10    # 以及起始标志数字, 默认为1, 我们这里使用的也是1    result = greedy_decode(model, source, source_mask, max_len=10, start_symbol=1)    print(result)if __name__ == '__main__':    run(model, loss) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">    1     3     2     5     4     6     7     8     9    10[torch.LongTensor of size 1x10]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><h3 id="小节总结-4"><a href="#小节总结-4" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了copy任务的相关知识:</p><ul><li>任务描述: 针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</li><li>任务意义: copy任务在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力.</li></ul><hr></li><li><p>学习了使用copy任务进行模型基本测试的四步曲:</p><ul><li>第一步: 构建数据集生成器</li><li>第二步: 获得Transformer模型及其优化器和损失函数</li><li>第三步: 运行模型进行训练和评估</li><li>第四步: 使用模型进行贪婪解码</li></ul><hr></li><li><p>学习并实现了构建数据集生成器函数: data_gen</p><ul><li>它有三个输入参数, 分别是V: 随机生成数字的最大值+1, batch: 每次输送给模型更新一次参数的数据量, nbatches: 一共输送nbatches次完成一轮.</li><li>该函数最终得到一个生成器对象.</li></ul><hr></li><li><p>学习了获得Transformer模型及其优化器和损失函数:</p><ul><li>通过导入优化器工具包get_std_opt, 获得标准优化器.</li><li>通过导入标签平滑工具包LabelSmoothing, 进行标签平滑.</li><li>通过导入损失计算工具包SimpleLossCompute, 计算损失.</li></ul><hr></li><li><p>学习并实现了运行模型进行训练和评估函数: run</p><ul><li>在函数中导入模型单轮训练工具包run_epoch, 对模型进行单轮训练.</li><li>函数共有三个参数, model代表将要进行训练的模型, slc代表使用的损失计算方法, epochs代表模型训练的轮数.</li><li>函数最终打印了模型训练和评估两个过程的损失.</li></ul><hr></li><li><p>学习并实现了使用模型进行贪婪解码:</p><ul><li>通过导入贪婪解码工具包greedy_decode, 根据输入得到最后输出, 完成了copy任务.</li></ul></li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HMM与CRF经典序列模型</title>
      <link href="2021/04/26/hmm-yu-crf-jing-dian-xu-lie-mo-xing/"/>
      <url>2021/04/26/hmm-yu-crf-jing-dian-xu-lie-mo-xing/</url>
      
        <content type="html"><![CDATA[<h1 id="HMM与CRF"><a href="#HMM与CRF" class="headerlink" title="HMM与CRF"></a>HMM与CRF</h1><h2 id="认识HMM与CRF模型"><a href="#认识HMM与CRF模型" class="headerlink" title="认识HMM与CRF模型"></a>认识HMM与CRF模型</h2><hr><h3 id="HMM模型的输入和输出"><a href="#HMM模型的输入和输出" class="headerlink" title="HMM模型的输入和输出"></a>HMM模型的输入和输出</h3><ul><li>HMM(Hidden Markov Model), 中文称作隐含马尔科夫模型, 因俄国数学家马尔可夫而得名. 它一般以文本序列数据为输入, 以该序列对应的隐含序列为输出.</li></ul><hr><ul><li>什么是隐含序列:<ul><li>序列数据中每个单元包含的隐性信息, 这些隐性信息之间也存在一定关联.</li></ul></li></ul><hr><ul><li>举个栗子:</li></ul><pre class="line-numbers language-none"><code class="language-none">给定一段文本: "人生该如何起头"我们看到的这句话可以叫做: 观测序列我们可以将这句话以词为单位进行划分得到:["人生", "该", "如何", "起头"]那么每个词对应的词性就是它的隐含序列, 如: ["n", "r", "r", "v"]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="HMM模型的作用"><a href="#HMM模型的作用" class="headerlink" title="HMM模型的作用"></a>HMM模型的作用</h3><ul><li>在NLP领域, HMM用来解决文本序列标注问题. 如分词, 词性标注, 命名实体识别都可以看作是序列标注问题.</li></ul><hr><h3 id="HMM模型使用过程简述"><a href="#HMM模型使用过程简述" class="headerlink" title="HMM模型使用过程简述"></a>HMM模型使用过程简述</h3><ul><li>首先, HMM模型表示为: lambda = HMM(A, B, pi), 其中A, B, pi都是模型的参数, 分别称作: 转移概率矩阵, 发射概率矩阵和初始概率矩阵.</li><li>接着, 我们开始训练HMM模型, 语料就是事先准备好的一定数量的观测序列及其对应的隐含序列, 通过极大似然估计求得一组参数, 使由观测序列到对应隐含序列的概率最大.</li><li>在训练过程中, 为了简化计算, 马尔可夫提出一种假设: 隐含序列中每个单元的可能性只与上一个单元有关. 这个假设就是著名的隐含假设.</li><li>训练后, 我们就得到了具备预测能力的新模型: lambda = HMM(A, B, pi), 其中的模型参数已经改变.</li><li>之后给定输入序列(x1, x2, …, xn), 经过模型计算lambda(x1, x2, …, xn)得到对应隐含序列的条件概率分布.</li><li>最后, 使用维特比算法从隐含序列的条件概率分布中找出概率最大的一条序列路径就是我们需要的隐含序列: (y1, y2, …, yn).</li></ul><hr><h3 id="CRF模型的输入和输出"><a href="#CRF模型的输入和输出" class="headerlink" title="CRF模型的输入和输出"></a>CRF模型的输入和输出</h3><ul><li>CRF(Conditional Random Fields), 中文称作条件随机场, 同HMM一样, 它一般也以文本序列数据为输入, 以该序列对应的隐含序列为输出.</li></ul><hr><h3 id="CRF模型的作用"><a href="#CRF模型的作用" class="headerlink" title="CRF模型的作用"></a>CRF模型的作用</h3><ul><li>同HMM一样, 在NLP领域, CRF用来解决文本序列标注问题. 如分词, 词性标注, 命名实体识别.</li></ul><hr><h3 id="CRF模型使用过程简述"><a href="#CRF模型使用过程简述" class="headerlink" title="CRF模型使用过程简述"></a>CRF模型使用过程简述</h3><ul><li>首先, CRF模型表示为: lambda = CRF(w1, w2, …, wn), 其中w1到wn是模型参数.</li><li>接着, 我们开始训练CRF模型, 语料同样是事先准备好的一定数量的观测序列及其对应的隐含序列.</li><li>与此同时我们还需要做人工特征工程, 然后通过不断训练求得一组参数, 使由观测序列到对应隐含序列的概率最大.</li><li>训练后, 我们就得到了具备预测能力的新模型: lambda = CRF(w1, w2, …, wn), 其中的模型参数已经改变.</li><li>之后给定输入序列(x1, x2, …, xn), 经过模型计算lambda(x1, x2, …, xn)得到对应隐含序列的条件概率分布.</li><li>最后, 还是使用维特比算法从隐含序列的条件概率分布中找出概率最大的一条序列路径就是我们需要的隐含序列: (y1, y2, …, yn).</li></ul><hr><h3 id="HMM与CRF模型之间差异"><a href="#HMM与CRF模型之间差异" class="headerlink" title="HMM与CRF模型之间差异"></a>HMM与CRF模型之间差异</h3><ul><li>HMM模型存在隐马假设, 而CRF不存在, 因此HMM的计算速度要比CRF模型快很多, 适用于对预测性能要求较高的场合.</li><li>同样因为隐马假设, 当预测问题中隐含序列单元并不是只与上一个单元有关时, HMM的准确率会大大降低, 而CRF不受这样限制, 准确率明显高于HMM.</li></ul><hr><h3 id="HMM和CRF的发展现状"><a href="#HMM和CRF的发展现状" class="headerlink" title="HMM和CRF的发展现状"></a>HMM和CRF的发展现状</h3><ul><li>HMM和CRF模型曾在多种序列任务中表现出色, 伴随NLP工程师度过漫长的一段时期.</li><li>但由于近年来深度学习发展迅速, 经典序列模型, 如HMM和CRF, 已经开始慢慢淡出人们的视野.</li><li>因此, 我们这里也是对其做了简洁的总结知识, 让大家对其有一定的基本认识.</li></ul><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>学习了HMM与CRF模型的输入和输出.</li><li>学习了HMM与CRF模型的作用.</li><li>学习了HMM与CRF模型的使用过程.</li><li>学习了HMM与CRF模型之间的差异.</li><li>学习了HMM和CRF的发展现状.</li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HMM </tag>
            
            <tag> CRF </tag>
            
            <tag> 经典序列模型 </tag>
            
            <tag> 马尔科夫模型 </tag>
            
            <tag> 条件随机场 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能网站大全</title>
      <link href="2021/04/25/ren-gong-zhi-neng-wang-zhan-da-quan/"/>
      <url>2021/04/25/ren-gong-zhi-neng-wang-zhan-da-quan/</url>
      
        <content type="html"><![CDATA[<p>转载于<a href="https://github.com/howie6879/mlhub123">https://github.com/howie6879/mlhub123</a></p><h3 id="新闻资讯"><a href="#新闻资讯" class="headerlink" title="新闻资讯"></a>新闻资讯</h3><ul><li><a href="https://www.analyticsvidhya.com/blog/">Analytics Vidhya</a>: 为数据科学专业人员提供基于社区的知识门户</li><li><a href="https://distill.pub/">Distill</a>: 展示机器学习的最新文章</li><li><a href="https://news.google.com/topics/CAAqIggKIhxDQkFTRHdvSkwyMHZNREZvZVdoZkVnSmxiaWdBUAE?hl=en-US&amp;gl=US&amp;ceid=US:en">Google News</a>: Google News Machine learning</li><li><a href="https://www.kdnuggets.com/?from=www.mlhub123.com">kdnuggets</a>: Machine Learning, Data Science, Big Data, Analytics, AI</li><li><a href="http://news.mit.edu/topic/machine-learning?from=www.mlhub123.com">MIT News</a>: Machine learning | MIT News</li><li><a href="http://www.17bigdata.com/?from=www.mlhub123.com">17bigdata</a>: 专注数据分析、挖掘、大数据相关领域的技术分享、交流</li><li><a href="https://www.jiqizhixin.com/?from=www.mlhub123.com">机器之心</a>: 机器之心 | 全球人工智能信息服务</li><li><a href="https://www.leiphone.com/?from=www.mlhub123.com">雷锋网</a>: 雷锋网 | 读懂智能，未来</li><li><a href="https://www.afenxi.com/?from=www.mlhub123.com">数据分析网</a>: 数据分析网 - 大数据学习交流第一平台</li><li><a href="https://www.zhihu.com/topic/19559450/hot?from=www.mlhub123.com">知乎主题</a>: 知乎机器学习热门主题</li><li><a href="http://www.zhuanzhi.ai/">专知</a>：AI知识分发服务平台</li></ul><h3 id="社区交流"><a href="#社区交流" class="headerlink" title="社区交流"></a>社区交流</h3><ul><li><a href="http://www.6aiq.com/?from=www.mlhub123.com">AIQ</a>: 机器学习大数据技术社区</li><li><a href="https://www.datatau.com/?from=www.mlhub123.com">DataTau</a>: 人工智能领域的Hacker News</li><li><a href="https://mathoverflow.net/?from=www.mlhub123.com">MathOverflow</a>: 数学知识问答社区</li><li><a href="https://medium.com/">Medium</a>: 一个涵盖人工智能、机器学习和深度学习相关领域的自由、开放平台</li><li><a href="http://www.paperweekly.site/?from=www.mlhub123.com">PaperWeekly</a>: 一个推荐、解读、讨论和报道人工智能前沿论文成果的学术平台</li><li><a href="https://www.quora.com/pinned/Machine-Learning?from=www.mlhub123.com">Quora</a>: Quora | 机器学习主题</li><li><a href="https://www.reddit.com/r/MachineLearning/?from=www.mlhub123.com">Reddit</a>: Reddit | 机器学习板块</li><li><a href="http://www.shortscience.org/?from=www.mlhub123.com">ShortScience</a>: 用最简单的篇幅去概况科学著作</li><li><a href="http://sofasofa.io/index.php?from=www.mlhub123.com">SofaSofa</a>: 做最好的数据科学社区</li><li><a href="https://twitter.com/StatMLPapers">Twitter</a>: Twitter | 机器学习论文版块</li><li><a href="http://www.ziiai.com/?from=www.mlhub123.com">极智能</a>: 人工智能技术社区</li></ul><h3 id="优质博文"><a href="#优质博文" class="headerlink" title="优质博文"></a>优质博文</h3><ul><li><a href="https://ai.googleblog.com/">Google AI Blog</a>: 谷歌AI博客</li><li><a href="https://handong1587.github.io/">handong1587</a>: 深度学习各个方向资源汇总，及各大顶级会议/期刊资源</li><li><a href="https://machinelearningmastery.com/blog?from=www.mlhub123.com">Machine Learning Mastery</a>: 帮助开发人员使用机器学习的知识解决复杂的问题</li><li><a href="https://blog.paralleldots.com/">paralleldots</a>：一个提供随时可用的一流AI解决方案的博客</li><li><a href="https://blog.statsbot.co/?from=www.mlhub123.com">Stats and Bots - Medium</a>: 机器学习应用程序和代码的实用指南</li><li><a href="https://www.cnblogs.com/tornadomeet/archive/2012/06/24/2560261.html?from=www.mlhub123.com">tornadomeet的博客</a>: 很详细的ML&amp;DL学习博客</li><li><a href="http://www.wildml.com/">wildml</a>：Artificial Intelligence, Deep Learning, and NLP</li><li><a href="https://weibo.com/fly51fly?topnav=1&amp;wvr=6&amp;topsug=1">爱可可-爱生活</a>: 知名互联网资讯博主</li><li><a href="https://zhuanlan.zhihu.com/YJango">超智能体</a>: 分享最通俗易懂的深度学习教程</li><li><a href="https://zhuanlan.zhihu.com/ainote">人工智能笔记</a>: 人工智能从入门到AI统治世界</li></ul><h3 id="资源检索"><a href="#资源检索" class="headerlink" title="资源检索"></a>资源检索</h3><ul><li><a href="https://arxiv.org/">arXiv</a>: 康奈尔大学运营的学术预印本发布的平台</li><li><a href="http://www.arxiv-sanity.com/?from=www.mlhub123.com">Arxiv Sanity</a>: 论文查询推荐</li><li><a href="https://www.cn-ki.net/">iData</a>: iData-知识检索</li><li><a href="https://modeldepot.io/search">ModelDepot</a>：文献模型源代码搜索下载</li><li><a href="https://paperswithcode.com/?from=www.mlhub123.com">Papers with Code</a>: 将论文与开源代码实现结合</li><li><a href="https://sci-hub.se/">SCI-HUB</a>: 找论文必备</li></ul><h3 id="比赛实践"><a href="#比赛实践" class="headerlink" title="比赛实践"></a>比赛实践</h3><ul><li><a href="https://biendata.com/">biendata</a>：数据科学竞赛平台</li><li><a href="http://www.pkbigdata.com/?from=www.mlhub123.com">DataCastle</a>: 中国领先的数据科学竞赛平台</li><li><a href="http://www.datafountain.cn/#/?from=www.mlhub123.com">DataFountain</a>: DF,CCF指定专业大数据竞赛平台</li><li><a href="https://www.kaggle.com/?from=www.mlhub123.com">Kaggle</a>: 为数据科学家提供举办机器学习竞赛</li><li><a href="http://www.kdd.org/kdd-cup?from=www.mlhub123.com">KDD-CUP</a>: 国际知识发现和数据挖掘竞赛</li><li><a href="http://research.xiaojukeji.com/trainee.html?from=www.mlhub123.com">滴滴新锐</a>: 滴滴面向全球高校博士、硕士、优秀本科生的精英人才计划</li><li><a href="https://jdder.jd.com/">JDD空间站</a>: 京东算法赛事平台</li><li><a href="http://www.saikr.com/?from=www.mlhub123.com">赛氪网</a>: 汇集以高校竞赛为主，活动、社区为辅的大学生竞赛活动平台</li><li><a href="https://tianchi.aliyun.com/?from=www.mlhub123.com">天池大数据</a>: 大数据竞赛、大数据解决方案、数据科学家社区、人工智能、机器学习</li></ul><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><h3 id="课程学习"><a href="#课程学习" class="headerlink" title="课程学习"></a>课程学习</h3><ul><li><a href="https://github.com/zekelabs/data-science-complete-tutorial">data-science-complete-tutorial</a>: 数据科学完整入门指南</li><li><a href="https://v.youku.com/v_show/id_XMjcwMDQyOTcxMg==.html?spm=a2h0j.11185381.listitem_page1.5!4~A&amp;&amp;f=49376145">David Silver</a>: David Silver 深度强化学习课程</li><li><a href="http://www.fast.ai/">fast.ai</a>: Making neural nets uncool again</li><li><a href="https://www.zybuluo.com/hanbingtao/note/433855">hanbt</a>: 零基础入门深度学习，深入浅出，很不错的入门教程</li><li><a href="https://coding.imooc.com/class/169.html">liuyubobobo</a>: Python3 入门机器学习</li><li><a href="https://metacademy.org/">Metacademy</a>: 知识点检索并画出通向这个知识点的知识图谱</li><li><a href="https://www.kaggle.com/learn/time-series-with-siraj">Siraj Raval：时序预测</a>: Kaggle免费课程：时序预测</li><li><a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">Two Minute Papers</a>: YouTube | 最简短的语言概况最新的热点论文</li><li><a href="https://github.com/yandexdataschool/nlp_course">YSDA nlp_course</a>: YSDA course in Natural Language Processing</li><li><a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw">3Blue1Brown</a>: YouTube | 数学基础频道</li><li><a href="http://space.bilibili.com/88461692/#/">3Blue1Brown 中文</a>: Bilibili | 数学基础频道</li><li><a href="https://zh.diveintodeeplearning.org/">动手学深度学习</a>: 面向中文读者的能运行、可讨论的深度学习教科书</li><li><a href="https://developers.google.cn/machine-learning/crash-course/">谷歌：机器学习速成课程</a>: Google制作的节奏紧凑、内容实用的机器学习简介课程</li><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses.html">李宏毅</a>: 李宏毅深度学习课程</li><li><a href="https://www.bilibili.com/video/av4294020/">林轩田</a>: 机器学习基石</li><li><a href="https://www.bilibili.com/video/av12469267">林轩田</a>: 机器学习技法</li><li><a href="https://github.com/roboticcam/machine-learning-notes">徐亦达</a>: 徐亦达老师机器学习课程</li><li><a href="https://github.com/nndl/nndl.github.io">邱锡鹏（复旦大学）</a>: 神经网络与深度学习</li><li><a href="http://study.163.com/course/introduction/1004570029.htm">吴恩达</a>: 机器学习课程</li><li><a href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm">吴恩达</a>: 深度学习课程</li><li><a href="https://github.com/MLEveryday">MLEveryday</a>: machine learning everyday</li></ul><h3 id="资源收集"><a href="#资源收集" class="headerlink" title="资源收集"></a>资源收集</h3><ul><li><a href="https://github.com/jobbole/awesome-machine-learning-cn">awesome-machine-learning-cn</a>: 机器学习资源大全中文版，包括机器学习领域的框架、库以及软件</li><li><a href="https://github.com/awesomedata/awesome-public-datasets">awesome-public-datasets</a>: 各领域公开数据集下载</li><li><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">Coursera-ML-AndrewNg-Notes</a>: 吴恩达老师的机器学习课程个人笔记</li><li><a href="https://github.com/amusi/daily-paper-computer-vision">daily-paper-computer-vision</a>: 记录每天整理的计算机视觉/深度学习/机器学习相关方向的论文</li><li><a href="https://github.com/scutan90/DeepLearning-500-questions">DeepLearning-500-questions</a>：深度学习500问</li><li><a href="https://github.com/fengdu78/deeplearning_ai_books">deeplearning_ai_books</a>: 吴恩达老师的深度学习课程笔记及资源</li><li><a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap">Deep-Learning-Papers-Reading-Roadmap</a>: 深度学习论文阅读路线图</li><li><a href="https://github.com/fighting41love/funNLP">funNLP</a>：中文语料库资源收集项目</li><li><a href="https://sites.google.com/site/mostafasibrahim/research/articles/how-to-start">Getting Started in Computer Vision Research</a>：计算机视觉研究入门全指南</li><li><a href="https://github.com/WenDesi/lihang_book_algorithm">lihang_book_algorithm</a>: 《统计学习方法》算法python实现</li><li><a href="https://github.com/ty4z2008/Qix/blob/master/dl.md">Machine Learning、Deep Learning</a>: ML&amp;DL资料</li><li><a href="https://github.com/lawlite19/MachineLearning_Python">MachineLearning_Python</a>: 机器学习算法python实现</li><li><a href="https://github.com/linxid/Machine_Learning_Study_Path">Machine_Learning_Study_Path</a>：机器学习过程中所看的书，视频和源码</li><li><a href="https://github.com/remicnrd/ml_cheatsheet">ml_cheatsheet</a>：机器学习算法速查手册</li><li><a href="https://github.com/MorvanZhou/tutorials">ml_tutorials</a>: 机器学习相关教程</li><li><a href="https://github.com/sebastianruder/NLP-progress">NLP-progress</a>：跟踪NLP各项技术的state-of-the-art进展</li><li><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code">100-Days-Of-ML-Code 英文版</a>：100 Days of Machine Learning Coding as proposed by Siraj Raval</li><li><a href="https://github.com/MLEveryday/100-Days-Of-ML-Code">100-Days-Of-ML-Code 中文版</a>：100-Days-Of-ML-Code 中文版</li><li><a href="https://github.com/Vay-keen/Machine-learning-learning-notes">周志华 - 机器学习</a>: 周志华《机器学习》笔记</li></ul><h3 id="开源书籍"><a href="#开源书籍" class="headerlink" title="开源书籍"></a>开源书籍</h3><ul><li><a href="https://github.com/exacity/deeplearningbook-chinese">deeplearningbook-chinese</a>: 深度学习中文版</li><li><a href="https://github.com/d2l-ai/d2l-zh">动手学深度学习</a>: 《动手学深度学习》：面向中文读者、能运行、可讨论。中英文版被全球175所大学采用教学</li><li><a href="https://github.com/DOsinga/deep_learning_cookbook">deep_learning_cookbook</a>: 深度学习手册</li><li><a href="https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF">hands_on_Ml_with_Sklearn_and_TF</a>: Sklearn与TensorFlow机器学习实用指南</li><li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a>: 一份指南，教你如何构建具有可解释性的黑盒模型</li><li><a href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a>: 深度学习开源书籍</li><li><a href="https://github.com/zhanggyb/nndl">Neural Networks and Deep Learning</a>: 深度学习开源书籍 - 中文</li><li><a href="https://github.com/jakevdp/PythonDataScienceHandbook">PythonDataScienceHandbook</a>: Python数据科学手册</li><li><a href="https://github.com/open-source-for-science/TensorFlow-Course">TensorFlow-Course</a>: 简单易学的TensorFlow教程</li><li><a href="https://github.com/apachecn/MachineLearning">机器学习实战</a>: Machine Learning in Action（机器学习实战）</li><li><a href="https://tf.wiki/">简单粗暴 TensorFlow 2</a>: 一本简明的 TensorFlow 2 入门指导手册</li></ul><h3 id="实战项目"><a href="#实战项目" class="headerlink" title="实战项目"></a>实战项目</h3><ul><li><a href="https://github.com/ageitgey/face_recognition">face_recognition</a>: 世界上最简单的人脸识别库</li><li><a href="https://github.com/lllyasviel/style2paints">style2paints</a>: 线稿自动上色</li></ul><h3 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h3><ul><li><a href="https://space.bilibili.com/344849038/dynamic">face_recognition</a>: 学习观</li></ul><h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><ul><li><a href="http://caffe.berkeleyvision.org/">Caffe</a>: 一个基于表达式，速度和模块化原则创建的深度学习框架</li><li><a href="https://caffe2.ai/docs/getting-started.html?platform=windows&amp;configuration=compile">Caffe2</a>: Caffe2官方文档</li><li><a href="https://docs.chainer.org/en/stable/">Chainer</a>: 基于Python的独立的深度学习模型开源框架</li><li><a href="https://docs.microsoft.com/en-us/cognitive-toolkit/">CNTK</a>: CNTK官方文档</li><li><a href="https://radimrehurek.com/gensim/index.html">Gensim</a>: 包含可扩展的统计语义，分析纯文本文档的语义结构，以及检索相似语义的文档等功能</li><li><a href="https://keras.io/">Keras</a>: Keras官方文档</li><li><a href="https://matplotlib.org/tutorials/index.html">Matplotlib</a>: Matplotlib官方文档</li><li><a href="http://mxnet.incubator.apache.org/tutorials/index.html">MXNet</a>: MXNet官方文档</li><li><a href="http://neon.nervanasys.com/index.html/">Neon</a>: Nervana公司一个基于Python的深度学习库</li><li><a href="http://www.numpy.org/">NumPy</a>: NumPy官方文档</li><li><a href="http://pandas.pydata.org/pandas-docs/stable/">pandas</a>: pandas官方文档</li><li><a href="http://pybrain.org/docs/">PyBrain</a>: 一个模块化的Python机器学习库</li><li><a href="http://deeplearning.net/software/pylearn2/">Pylearn2</a>: 构建于Theano之上的机器学习库</li><li><a href="https://pytorch.org/tutorials/">PyTorch</a>: PyTorch官方文档</li><li><a href="https://seaborn.pydata.org/">Seaborn</a>: Seaborn官方文档</li><li><a href="http://scikit-learn.org/stable/documentation.html">scikit-learn</a>: scikit-learn官方文档</li><li><a href="http://www.statsmodels.org/stable/index.html">Statsmodels</a>: 用来探索数据，估计统计模型，进行统计测试</li><li><a href="https://www.tensorflow.org/tutorials/">TensorFlow</a>: TF官方文档</li><li><a href="http://deeplearning.net/software/theano/">Theano</a>: 允许高效地定义、优化以及评估涉及多维数组的数学表达式</li><li><a href="https://spinningup.openai.com/en/latest/">openai</a>: 强化学习</li></ul><h3 id="C-amp-C"><a href="#C-amp-C" class="headerlink" title="C &amp; C++"></a>C &amp; C++</h3><ul><li><a href="http://dlib.net/">dlib</a>: 实用的机器学习和数据分析工具包</li></ul><h3 id="Java-amp-Scala"><a href="#Java-amp-Scala" class="headerlink" title="Java &amp; Scala"></a>Java &amp; Scala</h3><ul><li><a href="https://deeplearning4j.org/">DeepLearning4j</a>: 基于JAVA和Scala的商业级开源分布式深度学习框架</li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> 学习网站 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer学习笔记总结</title>
      <link href="2021/04/24/transformer-xue-xi-bi-ji-zong-jie/"/>
      <url>2021/04/24/transformer-xue-xi-bi-ji-zong-jie/</url>
      
        <content type="html"><![CDATA[<ol><li>每个单词被嵌入到大小为512的向量中</li><li>编码器接受一个向量列表作为输入，首先将这些向量传到自注意力层，然后传递到前馈神经网络</li><li>多头注意力下没个头维护不同的qkv，从而得到不同的qkv，所以最终将的到八个不同的z矩阵</li><li>前馈全连接层需要的是一个单一的矩阵而不是八个矩阵，所以要想办法对这八个矩阵进行压缩</li><li>所以这里将八个矩阵拼接，并将它们乘以一个另外的权重矩阵wo，如图</li></ol><p><img src="/2021/04/24/transformer-xue-xi-bi-ji-zong-jie/image-20210424205002779.png" alt="image-20210424205002779"></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT,Transformer模型架构与详解</title>
      <link href="2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/"/>
      <url>2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="1-BERT-Transformer的模型架构与详解"><a href="#1-BERT-Transformer的模型架构与详解" class="headerlink" title="1. BERT,Transformer的模型架构与详解"></a>1. BERT,Transformer的模型架构与详解</h1><h2 id="1-1-认识BERT"><a href="#1-1-认识BERT" class="headerlink" title="1.1 认识BERT"></a>1.1 认识BERT</h2><hr><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解什么是BERT.</li><li>掌握BERT的架构.</li><li>掌握BERT的预训练任务.</li></ul><hr><h3 id="什么是BERT"><a href="#什么是BERT" class="headerlink" title="什么是BERT"></a>什么是BERT</h3><ul><li>BERT是2018年10月由Google AI研究院提出的一种预训练模型.<ul><li>BERT的全称是Bidirectional Encoder Representation from Transformers.</li><li>BERT在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类, 并且在11种不同NLP测试中创出SOTA表现. 包括将GLUE基准推高至80.4% (绝对改进7.6%), MultiNLI准确度达到86.7% (绝对改进5.6%). 成为NLP发展史上的里程碑式的模型成就.</li></ul></li></ul><hr><h3 id="BERT的架构"><a href="#BERT的架构" class="headerlink" title="BERT的架构"></a>BERT的架构</h3><ul><li>总体架构: 如下图所示, 最左边的就是BERT的架构图, 可以很清楚的看到BERT采用了Transformer Encoder block进行连接, 因为是一个典型的双向编码模型.</li></ul><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/BERT.png" alt="img"></p><hr><ul><li>从上面的架构图中可以看到, 宏观上BERT分三个主要模块.<ul><li>最底层黄色标记的Embedding模块.</li><li>中间层蓝色标记的Transformer模块.</li><li>最上层绿色标记的预微调模块.</li></ul></li></ul><hr><ul><li>Embedding模块: BERT中的该模块是由三种Embedding共同组成而成, 如下图</li></ul><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/BERT2.png" alt="img"></p><hr><blockquote><ul><li>Token Embeddings 是词嵌入张量, 第一个单词是CLS标志, 可以用于之后的分类任务.</li><li>Segment Embeddings 是句子分段嵌入张量, 是为了服务后续的两个句子为输入的预训练任务.</li><li>Position Embeddings 是位置编码张量, 此处注意和传统的Transformer不同, 不是三角函数计算的固定位置编码, 而是通过学习得出来的.</li><li>整个Embedding模块的输出张量就是这3个张量的直接加和结果.</li></ul></blockquote><hr><ul><li>双向Transformer模块: BERT中只使用了经典Transformer架构中的Encoder部分, 完全舍弃了Decoder部分. 而两大预训练任务也集中体现在训练Transformer模块中.</li></ul><hr><ul><li>预微调模块:<ul><li>经过中间层Transformer的处理后, BERT的最后一层根据任务的不同需求而做不同的调整即可.</li><li>比如对于sequence-level的分类任务, BERT直接取第一个[CLS] token 的final hidden state, 再加一层全连接层后进行softmax来预测最终的标签.</li></ul></li></ul><hr><blockquote><ul><li>对于不同的任务, 微调都集中在预微调模块, 几种重要的NLP微调任务架构图展示如下</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/BERT3.png" alt="img"></p><hr><blockquote><ul><li>从上图中可以发现, 在面对特定任务时, 只需要对预微调层进行微调, 就可以利用Transformer强大的注意力机制来模拟很多下游任务, 并得到SOTA的结果. (句子对关系判断, 单文本主题分类, 问答任务(QA), 单句贴标签(NER))</li></ul></blockquote><hr><blockquote><ul><li>若干可选的超参数建议如下:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">Batch size: 16, 32Learning rate (Adam): 5e-5, 3e-5, 2e-5Epochs: 3, 4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><h3 id="BERT的预训练任务"><a href="#BERT的预训练任务" class="headerlink" title="BERT的预训练任务"></a>BERT的预训练任务</h3><ul><li>BERT包含两个预训练任务:<ul><li>任务一: Masked LM (带mask的语言模型训练)</li><li>任务二: Next Sentence Prediction (下一句话预测任务)</li></ul></li></ul><hr><ul><li>任务一: Masked LM (带mask的语言模型训练)<ul><li>关于传统的语言模型训练, 都是采用left-to-right, 或者left-to-right + right-to-left结合的方式, 但这种单向方式或者拼接的方式提取特征的能力有限. 为此BERT提出一个深度双向表达模型(deep bidirectional representation). 即采用MASK任务来训练模型.</li><li>1: 在原始训练文本中, 随机的抽取15%的token作为参与MASK任务的对象.</li><li>2: 在这些被选中的token中, 数据生成器并不是把它们全部变成[MASK], 而是有下列3种情况.<ul><li>2.1: 在80%的概率下, 用[MASK]标记替换该token, 比如my dog is hairy -&gt; my dog is [MASK]</li><li>2.2: 在10%的概率下, 用一个随机的单词替换token, 比如my dog is hairy -&gt; my dog is apple</li><li>2.3: 在10%的概率下, 保持该token不变, 比如my dog is hairy -&gt; my dog is hairy</li></ul></li><li>3: 模型在训练的过程中, 并不知道它将要预测哪些单词? 哪些单词是原始的样子? 哪些单词被遮掩成了[MASK]? 哪些单词被替换成了其他单词? 正是在这样一种高度不确定的情况下, 反倒逼着模型快速学习该token的分布式上下文的语义, 尽最大努力学习原始语言说话的样子. 同时因为原始文本中只有15%的token参与了MASK操作, 并不会破坏原语言的表达能力和语言规则.</li></ul></li></ul><hr><ul><li>任务二: Next Sentence Prediction (下一句话预测任务)<ul><li>在NLP中有一类重要的问题比如QA(Quention-Answer), NLI(Natural Language Inference), 需要模型能够很好的理解两个句子之间的关系, 从而需要在模型的训练中引入对应的任务. 在BERT中引入的就是Next Sentence Prediction任务. 采用的方式是输入句子对(A, B), 模型来预测句子B是不是句子A的真实的下一句话.</li><li>1: 所有参与任务训练的语句都被选中作为句子A.<ul><li>1.1: 其中50%的B是原始文本中真实跟随A的下一句话. (标记为IsNext, 代表正样本)</li><li>1.2: 其中50%的B是原始文本中随机抽取的一句话. (标记为NotNext, 代表负样本)</li></ul></li><li>2: 在任务二中, BERT模型可以在测试集上取得97%-98%的准确率.</li></ul></li></ul><hr><h3 id="小节总结"><a href="#小节总结" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>学习了什么是BERT.<ul><li>BERT是一个基于Transformer Encoder的预训练语言模型.</li><li>BERT在11种NLP测试任务中创出SOAT表现.</li></ul></li><li>学习了BERT的结构.<ul><li>最底层的Embedding模块, 包括Token Embeddings, Segment Embeddings, Position Embeddings.</li><li>中间层的Transformer模块, 只使用了经典Transformer架构中的Encoder部分.</li><li>最上层的预微调模块, 具体根据不同的任务类型来做相应的处理.</li></ul></li><li>学习了BERT的两大预训练任务.<ul><li>MLM任务(Masked Language Model), 在原始文本中随机抽取15%的token参与任务.<ul><li>在80%概率下, 用[MASK]替换该token.</li><li>在10%概率下, 用一个随机的单词替换该token.</li><li>在10%概率下, 保持该token不变.</li></ul></li><li>NSP任务(Next Sentence Prediction), 采用的方式是输入句子对(A, B), 模型预测句子B是不是句子A的真实的下一句话.<ul><li>其中50%的B是原始文本中真实跟随A的下一句话.(标记为IsNext, 代表正样本)</li><li>其中50%的B是原始文本中随机抽取的一句话. (标记为NotNext, 代表负样本)</li></ul></li></ul></li></ul><hr><hr><hr><h2 id="1-2-Transformer的结构是什么样的-各个子模块各有什么作用"><a href="#1-2-Transformer的结构是什么样的-各个子模块各有什么作用" class="headerlink" title="1.2 Transformer的结构是什么样的? 各个子模块各有什么作用?"></a>1.2 Transformer的结构是什么样的? 各个子模块各有什么作用?</h2><hr><h3 id="学习目标-1"><a href="#学习目标-1" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握Encoder模块的结构和作用</li><li>掌握Decoder模块的结构和作用</li><li>掌握其他模块的结构和作用</li></ul><hr><h3 id="Encoder模块"><a href="#Encoder模块" class="headerlink" title="Encoder模块"></a>Encoder模块</h3><ul><li>Encoder模块的结构和作用:<ul><li>经典的Transformer结构中的Encoder模块包含6个Encoder Block.</li><li>每个Encoder Block包含一个多头自注意力层, 和一个前馈全连接层.</li></ul></li></ul><hr><ul><li>关于Encoder Block:<ul><li>在Transformer架构中, 6个一模一样的Encoder Block层层堆叠在一起, 共同组成完整的Encoder, 因此剖析一个Block就可以对整个Encoder的内部结构有清晰的认识.</li></ul></li></ul><hr><ul><li>多头自注意力层(self-attention):</li></ul><blockquote><ul><li>首先来看self-attention的计算规则图:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_2.png" alt="img"></p><hr><blockquote><ul><li>上述attention可以被描述为将query和key-value键值对的一组集合映射到输出, 输出被计算为values的加权和, 其中分配给每个value的权重由query与对应key的相似性函数计算得来. 这种attention的形式被称为Scaled Dot-Product Attention, 对应的数学公式形式如下:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_3.png" alt="img"></p><hr><blockquote><ul><li>所谓的多头self-attention层, 则是先将Q, K, V经过参数矩阵进行映射, 再做self-attention, 最后将结果拼接起来送入一个全连接层即可.</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_1.png" alt="img"></p><hr><blockquote><ul><li>上述的多头self-attention, 对应的数学公式形式如下:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_4.png" alt="img"></p><hr><blockquote><ul><li>多头self-attention层的作用: 实验结果表明, Multi-head可以在更细致的层面上提取不同head的特征, 总体计算量和单一head相同的情况下, 提取特征的效果更佳.</li></ul></blockquote><hr><ul><li>前馈全连接层模块<ul><li>前馈全连接层模块, 由两个线性变换组成, 中间有一个Relu激活函数, 对应的数学公式形式如下:</li></ul></li></ul><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_5.png" alt="img"></p><hr><blockquote><ul><li>注意: 原版论文中的前馈全连接层, 输入和输出的维度均为d_model = 512, 层内的连接维度d_ff = 2048, 均采用4倍的大小关系.</li></ul></blockquote><hr><blockquote><ul><li>前馈全连接层的作用: 单纯的多头注意力机制并不足以提取到理想的特征, 因此增加全连接层来提升网络的能力.</li></ul></blockquote><hr><h3 id="Decoder模块"><a href="#Decoder模块" class="headerlink" title="Decoder模块"></a>Decoder模块</h3><ul><li>Decoder模块的结构和作用:<ul><li>经典的Transformer结构中的Decoder模块包含6个Decoder Block.</li><li>每个Decoder Block包含三个子层.<ul><li>一个多头self-attention层</li><li>一个Encoder-Decoder attention层</li><li>一个前馈全连接层</li></ul></li></ul></li></ul><hr><ul><li>Decoder Block中的多头self-attention层<ul><li>Decoder中的多头self-attention层与Encoder模块一致, 但需要注意的是Decoder模块的多头self-attention需要做look-ahead-mask, 因为在预测的时候”不能看见未来的信息”, 所以要将当前的token和之后的token全部mask.</li></ul></li></ul><hr><ul><li>Decoder Block中的Encoder-Decoder attention层<ul><li>这一层区别于自注意力机制的Q = K = V, 此处矩阵Q来源于Decoder端经过上一个Decoder Block的输出, 而矩阵K, V则来源于Encoder端的输出, 造成了Q != K = V的情况.</li><li>这样设计是为了让Decoder端的token能够给予Encoder端对应的token更多的关注.</li></ul></li></ul><hr><ul><li>Decoder Block中的前馈全连接层<ul><li>此处的前馈全连接层和Encoder模块中的完全一样.</li></ul></li></ul><hr><ul><li>Decoder Block中有2个注意力层的作用: 多头self-attention层是为了拟合Decoder端自身的信息, 而Encoder-Decoder attention层是为了整合Encoder和Decoder的信息.</li></ul><hr><h3 id="Add-amp-Norm模块"><a href="#Add-amp-Norm模块" class="headerlink" title="Add &amp; Norm模块"></a>Add &amp; Norm模块</h3><ul><li>Add &amp; Norm模块接在每一个Encoder Block和Decoder Block中的每一个子层的后面. 具体来说Add表示残差连接, Norm表示LayerNorm.<ul><li>对于每一个Encoder Block, 里面的两个子层后面都有Add &amp; Norm.</li><li>对于每一个Decoder Block, 里面的三个子层后面都有Add &amp; Norm.</li><li>具体的数学表达形式为: LayerNorm(x + Sublayer(x)), 其中Sublayer(x)为子层的输出.</li></ul></li></ul><hr><ul><li>Add残差连接的作用: 和其他神经网络模型中的残差连接作用一致, 都是为了将信息传递的更深, 增强模型的拟合能力. 试验表明残差连接的确增强了模型的表现.</li></ul><hr><ul><li>Norm的作用: 随着网络层数的额增加, 通过多层的计算后参数可能会出现过大, 过小, 方差变大等现象, 这会导致学习过程出现异常, 模型的收敛非常慢. 因此对每一层计算后的数值进行规范化可以提升模型的表现.</li></ul><hr><h3 id="位置编码器Positional-Encoding"><a href="#位置编码器Positional-Encoding" class="headerlink" title="位置编码器Positional Encoding"></a>位置编码器Positional Encoding</h3><ul><li>Transformer中直接采用正弦函数和余弦函数来编码位置信息, 如下图所示:</li></ul><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_7.png" alt="img"></p><hr><ul><li>需要注意: 三角函数应用在此处的一个重要的优点, 因为对于任意的PE(pos+k), 都可以表示为PE(pos)的线性函数, 大大方便计算. 而且周期性函数不受序列长度的限制, 也可以增强模型的泛化能力.</li></ul><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_6.png" alt="img"></p><hr><h3 id="小节总结-1"><a href="#小节总结-1" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>Encoder模块<ul><li>经典的Transformer架构中的Encoder模块包含6个Encoder Block.</li><li>每个Encoder Block包含两个子模块, 分别是多头自注意力层, 和前馈全连接层.<ul><li>多头自注意力层采用的是一种Scaled Dot-Product Attention的计算方式, 实验结果表明, Mul ti-head可以在更细致的层面上提取不同head的特征, 比单一head提取特征的效果更佳.</li><li>前馈全连接层是由两个全连接层组成, 线性变换中间增添一个Relu激活函数, 具体的维度采用4倍关系, 即多头自注意力的d_model=512, 则层内的变换维度d_ff=2048.</li></ul></li></ul></li><li>2: Decoder模块<ul><li>经典的Transformer架构中的Decoder模块包含6个Decoder Block.</li><li>每个Decoder Block包含3个子模块, 分别是多头自注意力层, Encoder-Decoder Attention层, 和前馈全连接层.<ul><li>多头自注意力层采用和Encoder模块一样的Scaled Dot-Product Attention的计算方式, 最大的 区别在于需要添加look-ahead-mask, 即遮掩”未来的信息”.</li><li>Encoder-Decoder Attention层和上一层多头自注意力层最主要的区别在于Q != K = V, 矩阵Q来源于上一层Decoder Block的输出, 同时K, V来源于Encoder端的输出.</li><li>前馈全连接层和Encoder中完全一样.</li></ul></li></ul></li><li>3: Add &amp; Norm模块<ul><li>Add &amp; Norm模块接在每一个Encoder Block和Decoder Block中的每一个子层的后面.</li><li>对于每一个Encoder Block, 里面的两个子层后面都有Add &amp; Norm.</li><li>对于每一个Decoder Block, 里面的三个子层后面都有Add &amp; Norm.</li><li>Add表示残差连接, 作用是为了将信息无损耗的传递的更深, 来增强模型的拟合能力.</li><li>Norm表示LayerNorm, 层级别的数值标准化操作, 作用是防止参数过大过小导致的学习过程异常 , 模型收敛特别慢的问题.</li></ul></li><li>4: 位置编码器Positional Encoding<ul><li>Transformer中采用三角函数来计算位置编码.</li><li>因为三角函数是周期性函数, 不受序列长度的限制, 而且这种计算方式可以对序列中不同位置的编码的重要程度同等看待.</li></ul></li></ul><hr><hr><hr><h2 id="1-3-Transformer结构中的Decoder端具体输入是什么-在训练阶段和预测阶段一致吗"><a href="#1-3-Transformer结构中的Decoder端具体输入是什么-在训练阶段和预测阶段一致吗" class="headerlink" title="1.3 Transformer结构中的Decoder端具体输入是什么? 在训练阶段和预测阶段一致吗?"></a>1.3 Transformer结构中的Decoder端具体输入是什么? 在训练阶段和预测阶段一致吗?</h2><hr><h3 id="学习目标-2"><a href="#学习目标-2" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握Transformer结构中的Decoder端的输入张量特点和含义.</li><li>掌握Decoder在训练阶段的输入是什么.</li><li>掌握Decoder在预测阶段的输入是什么.</li></ul><hr><h3 id="Decoder端的输入解析"><a href="#Decoder端的输入解析" class="headerlink" title="Decoder端的输入解析"></a>Decoder端的输入解析</h3><ul><li>Decoder端的架构: Transformer原始论文中的Decoder模块是由N=6个相同的Decoder Block堆叠而成, 其中每一个Block是由3个子模块构成, 分别是多头self-attention模块, Encoder-Decoder attention模块, 前馈全连接层模块.</li></ul><hr><ul><li>6个Block的输入不完全相同:<ul><li>最下面的一层Block接收的输入是经历了MASK之后的Decoder端的输入 + Encoder端的输出.</li><li>其他5层Block接收的输入模式一致, 都是前一层Block的输出 + Encoder端的输出.</li></ul></li></ul><hr><ul><li>Decoder在训练阶段的输入解析:<ul><li>从第二层Block到第六层Block的输入模式一致, 无需特殊处理, 都是固定操作的循环处理.</li><li>聚焦在第一层的Block上: 训练阶段每一个time step的输入是上一个time step的输入加上真实标签序列向后移一位. 具体来说, 假设现在的真实标签序列等于”How are you?”, 当time step=1时, 输入张量为一个特殊的token, 比如”SOS”; 当time step=2时, 输入张量为”SOS How”; 当time step=3时, 输入张量为”SOS How are”, 以此类推…</li><li>注意: 在真实的代码实现中, 训练阶段不会这样动态输入, 而是一次性的把目标序列全部输入给第一层的Block, 然后通过多头self-attention中的MASK机制对序列进行同样的遮掩即可.</li></ul></li></ul><hr><ul><li>Decoder在预测阶段的输入解析:<ul><li>同理于训练阶段, 预测时从第二层Block到第六层Block的输入模式一致, 无需特殊处理, 都是固定操作的循环处理.</li><li>聚焦在第一层的Block上: 因为每一步的输入都会有Encoder的输出张量, 因此这里不做特殊讨论, 只专注于纯粹从Decoder端接收的输入. 预测阶段每一个time step的输入是从time step=0, input_tensor=”SOS”开始, 一直到上一个time step的预测输出的累计拼接张量. 具体来说:<ul><li>当time step=1时, 输入的input_tensor=”SOS”, 预测出来的输出值是output_tensor=”What”;</li><li>当time step=2时, 输入的input_tensor=”SOS What”, 预测出来的输出值是output_tensor=”is”;</li><li>当time step=3时, 输入的input_tensor=”SOS What is”, 预测出来的输出值是output_tensor=”the”;</li><li>当time step=4时, 输入的input_tensor=”SOS What is the”, 预测出来的输出值是output_tensor=”matter”;</li><li>当time step=5时, 输入的input_tensor=”SOS What is the matter”, 预测出来的输出值是output_tensor=”?”;</li><li>当time step=6时, 输入的input_tensor=”SOS What is the matter ?”, 预测出来的输出值是output_tensor=”EOS”, 代表句子的结束符, 说明解码结束, 预测结束.</li></ul></li></ul></li></ul><hr><h3 id="小节总结-2"><a href="#小节总结-2" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>1: 在Transformer结构中的Decoder模块的输入, 区分于不同的Block, 最底层的Block输入有其特殊的地方. 第二层到第六层的输入一致, 都是上一层的输出和Encoder的输出.</li><li>2: 最底层的Block在训练阶段, 每一个time step的输入是上一个time step的输入加上真实标签序列向后移一位. 具体来看, 就是每一个time step的输入序列会越来越长, 不断的将之前的输入融合进来.</li><li>3: 最底层的Block在训练阶段, 真实的代码实现中, 采用的是MASK机制来模拟输入序列不断添加的过程.</li><li>4: 最底层的Block在预测阶段, 每一个time step的输入是从time step=0开始, 一直到上一个time step的预测值的累积拼接张量. 具体来看, 也是随着每一个time step的输入序列会越来越长. 相比于训练阶段最大的不同是这里不断拼接进来的token是每一个time step的预测值, 而不是训练阶段每一个time step取得的groud truth值.</li></ul><hr><hr><hr><h2 id="1-4-Transformer中一直强调的self-attention是什么-为什么能发挥如此大的作用-计算的时候如果不使用三元组-Q-K-V-而仅仅使用-Q-V-或者-K-V-或者-V-行不行"><a href="#1-4-Transformer中一直强调的self-attention是什么-为什么能发挥如此大的作用-计算的时候如果不使用三元组-Q-K-V-而仅仅使用-Q-V-或者-K-V-或者-V-行不行" class="headerlink" title="1.4 Transformer中一直强调的self-attention是什么? 为什么能发挥如此大的作用? 计算的时候如果不使用三元组(Q, K, V), 而仅仅使用(Q, V)或者(K, V)或者(V)行不行?"></a>1.4 Transformer中一直强调的self-attention是什么? 为什么能发挥如此大的作用? 计算的时候如果不使用三元组(Q, K, V), 而仅仅使用(Q, V)或者(K, V)或者(V)行不行?</h2><hr><h3 id="学习目标-3"><a href="#学习目标-3" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握self-attention的机制和原理.</li><li>掌握为什么要使用三元组(Q, K, V)来计算self-attention.</li></ul><hr><h3 id="self-attention的机制和原理"><a href="#self-attention的机制和原理" class="headerlink" title="self-attention的机制和原理"></a>self-attention的机制和原理</h3><ul><li>self-attention是一种通过自身和自身进行关联的attention机制, 从而得到更好的representation来表达自身.</li><li>self-attention是attention机制的一种特殊情况:<ul><li>在self-attention中, Q=K=V, 序列中的每个单词(token)都和该序列中的其他所有单词(token)进行attention规则的计算.</li></ul></li><li>attention机制计算的特点在于, 可以直接跨越一句话中不同距离的token, 可以远距离的学习到序列的知识依赖和语序结构.</li></ul><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_8.png" alt="img"></p><hr><blockquote><ul><li>从上图中可以看到, self-attention可以远距离的捕捉到语义层面的特征(its的指代对象是Law).</li><li>应用传统的RNN, LSTM, 在获取长距离语义特征和结构特征的时候, 需要按照序列顺序依次计算, 距离越远的联系信息的损耗越大, 有效提取和捕获的可能性越小.</li><li>但是应用self-attention时, 计算过程中会直接将句子中任意两个token的联系通过一个计算步骤直接联系起来,</li></ul></blockquote><hr><ul><li>关于self-attention为什么要使用(Q, K, V)三元组而不是其他形式:<ul><li>首先一条就是从分析的角度看, 查询Query是一条独立的序列信息, 通过关键词Key的提示作用, 得到最终语义的真实值Value表达, 数学意义更充分, 完备.</li><li>这里不使用(K, V)或者(V)没有什么必须的理由, 也没有相关的论文来严格阐述比较试验的结果差异, 所以可以作为开放性问题未来去探索, 只要明确在经典self-attention实现中用的是三元组就好.</li></ul></li></ul><hr><h3 id="小节总结-3"><a href="#小节总结-3" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>self-attention机制的重点是使用三元组(Q, K, V)参与规则运算, 这里面Q=K=V.</li><li>self-attention最大的优势是可以方便有效的提取远距离依赖的特征和结构信息, 不必向RNN那样依次计算产生传递损耗.</li><li>关于self-attention采用三元组的原因, 经典实现的方式数学意义明确, 理由充分, 至于其他方式的可行性暂时没有论文做充分的对比试验研究.</li></ul><hr><hr><hr><h2 id="1-5-Transformer为什么需要进行Multi-head-Attention-Multi-head-Attention的计算过程是什么"><a href="#1-5-Transformer为什么需要进行Multi-head-Attention-Multi-head-Attention的计算过程是什么" class="headerlink" title="1.5 Transformer为什么需要进行Multi-head Attention? Multi-head Attention的计算过程是什么?"></a>1.5 Transformer为什么需要进行Multi-head Attention? Multi-head Attention的计算过程是什么?</h2><hr><h3 id="学习目标-4"><a href="#学习目标-4" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握Transformer中应用多头注意力的原因.</li><li>掌握Transformer中多头注意力的计算方式.</li></ul><hr><h3 id="采用Multi-head-Attention的原因"><a href="#采用Multi-head-Attention的原因" class="headerlink" title="采用Multi-head Attention的原因"></a>采用Multi-head Attention的原因</h3><ul><li>1: 原始论文中提到进行Multi-head Attention的原因是将模型分为多个头, 可以形成多个子空间, 让模型去关注不同方面的信息, 最后再将各个方面的信息综合起来得到更好的效果.</li><li>2: 多个头进行attention计算最后再综合起来, 类似于CNN中采用多个卷积核的作用, 不同的卷积核提取不同的特征, 关注不同的部分, 最后再进行融合.</li><li>3: 直观上讲, 多头注意力有助于神经网络捕捉到更丰富的特征信息.</li></ul><hr><h3 id="Multi-head-Attention的计算方式"><a href="#Multi-head-Attention的计算方式" class="headerlink" title="Multi-head Attention的计算方式"></a>Multi-head Attention的计算方式</h3><ul><li>1: Multi-head Attention和单一head的Attention唯一的区别就在于, 其对特征张量的最后一个维度进行了分割, 一般是对词嵌入的embedding_dim=512进行切割成head=8, 这样每一个head的嵌入维度就是512/8=64, 后续的Attention计算公式完全一致, 只不过是在64这个维度上进行一系列的矩阵运算而已.</li><li>2: 在head=8个头上分别进行注意力规则的运算后, 简单采用拼接concat的方式对结果张量进行融合就得到了Multi-head Attention的计算结果.</li></ul><hr><h3 id="小节总结-4"><a href="#小节总结-4" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>学习了Transformer架构采用Multi-head Attention的原因.<ul><li>将模型划分为多个头, 分别进行Attention计算, 可以形成多个子空间, 让模型去关注不同方面的信息特征, 更好的提升模型的效果.</li><li>多头注意力有助于神经网络捕捉到更丰富的特征信息.</li></ul></li><li>学习了Multi-head Attention的计算方式.<ul><li>对特征张量的最后一个维度进行了分割, 一般是对词嵌入的维度embedding_dim进行切割, 切割后的计算规则和单一head完全一致.</li><li>在不同的head上应用了注意力计算规则后, 得到的结果张量直接采用拼接concat的方式进行融合, 就得到了Multi-head Attention的结果张量.</li></ul></li></ul><hr><hr><hr><h2 id="1-6-Transformer相比于RNN-LSTM有什么优势-为什么"><a href="#1-6-Transformer相比于RNN-LSTM有什么优势-为什么" class="headerlink" title="1.6 Transformer相比于RNN/LSTM有什么优势? 为什么?"></a>1.6 Transformer相比于RNN/LSTM有什么优势? 为什么?</h2><hr><h3 id="学习目标-5"><a href="#学习目标-5" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握Transformer相比于RNN/LSTM的优势和背后的原因.</li></ul><hr><h3 id="Transformer的并行计算"><a href="#Transformer的并行计算" class="headerlink" title="Transformer的并行计算"></a>Transformer的并行计算</h3><ul><li>对于Transformer比传统序列模型RNN/LSTM具备优势的第一大原因就是强大的并行计算能力.<ul><li>对于RNN来说, 任意时刻t的输入是时刻t的输入x(t)和上一时刻的隐藏层输出h(t-1), 经过运算后得到当前时刻隐藏层的输出h(t), 这个h(t)也即将作为下一时刻t+1的输入的一部分. 这个计算过程是RNN的本质特征, RNN的历史信息是需要通过这个时间步一步一步向后传递的. 而这就意味着RNN序列后面的信息只能等到前面的计算结束后, 将历史信息通过hidden state传递给后面才能开始计算, 形成链式的序列依赖关系, 无法实现并行.</li><li>对于Transformer结构来说, 在self-attention层, 无论序列的长度是多少, 都可以一次性计算所有单词之间的注意力关系, 这个attention的计算是同步的, 可以实现并行.</li></ul></li></ul><hr><h3 id="Transformer的特征抽取能力"><a href="#Transformer的特征抽取能力" class="headerlink" title="Transformer的特征抽取能力"></a>Transformer的特征抽取能力</h3><ul><li>对于Transformer比传统序列模型RNN/LSTM具备优势的第二大原因就是强大的特征抽取能力.<ul><li>Transformer因为采用了Multi-head Attention结构和计算机制, 拥有比RNN/LSTM更强大的特征抽取能力, 这里并不仅仅由理论分析得来, 而是大量的试验数据和对比结果, 清楚的展示了Transformer的特征抽取能力远远胜于RNN/LSTM.</li><li>注意: 不是越先进的模型就越无敌, 在很多具体的应用中RNN/LSTM依然大有用武之地, 要具体问题具体分析.</li></ul></li></ul><hr><h3 id="小节总结-5"><a href="#小节总结-5" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>学习了Transformer相比于RNN/LSTM的优势和原因.<ul><li>1: 第一大优势是并行计算的优势.</li><li>2: 第二大优势是特征提取能力强.</li></ul></li></ul><hr><hr><hr><h2 id="1-7-为什么说Transformer可以代替seq2seq"><a href="#1-7-为什么说Transformer可以代替seq2seq" class="headerlink" title="1.7 为什么说Transformer可以代替seq2seq?"></a>1.7 为什么说Transformer可以代替seq2seq?</h2><hr><h3 id="学习目标-6"><a href="#学习目标-6" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握Transformer可以替代seq2seq的核心原因.</li></ul><hr><h3 id="seq2seq的两大缺陷"><a href="#seq2seq的两大缺陷" class="headerlink" title="seq2seq的两大缺陷"></a>seq2seq的两大缺陷</h3><ul><li>1: seq2seq架构的第一大缺陷是将Encoder端的所有信息压缩成一个固定长度的语义向量中, 用这个固定的向量来代表编码器端的全部信息. 这样既会造成信息的损耗, 也无法让Decoder端在解码的时候去用注意力聚焦哪些是更重要的信息.</li><li>2: seq2seq架构的第二大缺陷是无法并行, 本质上和RNN/LSTM无法并行的原因一样.</li></ul><hr><h3 id="Transformer的改进"><a href="#Transformer的改进" class="headerlink" title="Transformer的改进"></a>Transformer的改进</h3><ul><li>Transformer架构同时解决了seq2seq的两大缺陷, 既可以并行计算, 又应用Multi-head Attention机制来解决Encoder固定编码的问题, 让Decoder在解码的每一步可以通过注意力去关注编码器输出中最重要的那些部分.</li></ul><hr><h3 id="小节总结-6"><a href="#小节总结-6" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>学习了seq2seq架构的两大缺陷.<ul><li>第一个缺陷是Encoder端的所有信息被压缩成一个固定的输出张量, 当序列长度较长时会造成比较严重的信息损耗.</li><li>第二个缺陷是无法并行计算.</li></ul></li><li>学习了Transformer架构对seq2seq两大缺陷的改进.<ul><li>Transformer应用Multi-head Attention机制让编码器信息可以更好的展示给解码器.</li><li>Transformer可以实现Encoder端的并行计算.</li></ul></li></ul><hr><hr><hr><h2 id="1-8-self-attention公式中的归一化有什么作用-为什么要添加scaled"><a href="#1-8-self-attention公式中的归一化有什么作用-为什么要添加scaled" class="headerlink" title="1.8 self-attention公式中的归一化有什么作用? 为什么要添加scaled?"></a>1.8 self-attention公式中的归一化有什么作用? 为什么要添加scaled?</h2><hr><h3 id="学习目标-7"><a href="#学习目标-7" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>理解softmax函数的输入是如何影响输出分布的.</li><li>理解softmax函数反向传播进行梯度求导的数学过程.</li><li>理解softmax函数出现梯度消失的原因.</li><li>理解self-attention计算规则中归一化的原因.</li></ul><hr><h3 id="self-attention中的归一化概述"><a href="#self-attention中的归一化概述" class="headerlink" title="self-attention中的归一化概述"></a>self-attention中的归一化概述</h3><ul><li>训练上的意义: 随着词嵌入维度d_k的增大, q * k 点积后的结果也会增大, 在训练时会将softmax函数推入梯度非常小的区域, 可能出现梯度消失的现象, 造成模型收敛困难.</li><li>数学上的意义: 假设q和k的统计变量是满足标准正态分布的独立随机变量, 意味着q和k满足均值为0, 方差为1. 那么q和k的点积结果就是均值为0, 方差为d_k, 为了抵消这种方差被放大d_k倍的影响, 在计算中主动将点积缩放1/sqrt(d_k), 这样点积后的结果依然满足均值为0, 方差为1.</li></ul><hr><h3 id="softmax的梯度变化"><a href="#softmax的梯度变化" class="headerlink" title="softmax的梯度变化"></a>softmax的梯度变化</h3><ul><li>这里我们分3个步骤来解释softmax的梯度问题:<ul><li>第一步: softmax函数的输入分布是如何影响输出的.</li><li>第二步: softmax函数在反向传播的过程中是如何梯度求导的.</li><li>第三步: softmax函数出现梯度消失现象的原因.</li></ul></li></ul><hr><ul><li>第一步: softmax函数的输入分布是如何影响输出的.<ul><li>对于一个输入向量x, softmax函数将其做了一个归一化的映射, 首先通过自然底数e将输入元素之间的差距先”拉大”, 然后再归一化为一个新的分布. 在这个过程中假设某个输入x中最大的元素下标是k, 如果输入的数量级变大(就是x中的每个分量绝对值都很大), 那么在数学上会造成y_k的值非常接近1.</li><li>具体用一个例子来演示, 假设输入的向量x = [a, a, 2a], 那么随便给几个不同数量级的值来看看对y3产生的影响</li></ul></li></ul><pre class="line-numbers language-none"><code class="language-none">a = 1时,   y3 = 0.5761168847658291a = 10时,  y3 = 0.9999092083843412a = 100时, y3 = 1.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>采用一段实例代码将a在不同取值下, 对应的y3全部画出来, 以曲线的形式展示:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">from math import expfrom matplotlib import pyplot as pltimport numpy as np f = lambda x: exp(x * 2) / (exp(x) + exp(x) + exp(x * 2))x = np.linspace(0, 100, 100)y_3 = [f(x_i) for x_i in x]plt.plot(x, y_3)plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>得到如下的曲线:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_13.png" alt="img"></p><hr><blockquote><ul><li>从上图可以很清楚的看到输入元素的数量级对softmax最终的分布影响非常之大.</li><li>结论: 在输入元素的数量级较大时, softmax函数几乎将全部的概率分布都分配给了最大值分量所对应的标签.</li></ul></blockquote><hr><ul><li>第二步: softmax函数在反向传播的过程中是如何梯度求导的.</li></ul><blockquote><ul><li>首先定义神经网络的输入和输出:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_9.png" alt="img"></p><hr><blockquote><ul><li>反向传播就是输出端的损失函数对输入端求偏导的过程, 这里要分两种情况, 第一种如下所示:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_10.png" alt="img"></p><hr><blockquote><ul><li>第二种如下所示:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_11.png" alt="img"></p><hr><blockquote><ul><li>经过对两种情况分别的求导计算, 可以得出最终的结论如下:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_12.png" alt="img"></p><hr><ul><li>第三步: softmax函数出现梯度消失现象的原因.</li></ul><blockquote><ul><li>根据第二步中softmax函数的求导结果, 可以将最终的结果以矩阵形式展开如下:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_14.png" alt="img"></p><hr><blockquote><ul><li>根据第一步中的讨论结果, 当输入x的分量值较大时, softmax函数会将大部分概率分配给最大的元素, 假设最大元素是x1, 那么softmax的输出分布将产生一个接近one-hot的结果张量y_ = [1, 0, 0,…, 0], 此时结果矩阵变为:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_15.png" alt="img"></p><hr><blockquote><ul><li>结论: 综上可以得出, 所有的梯度都消失为0(接近于0), 参数几乎无法更新, 模型收敛困难.</li></ul></blockquote><hr><hr><h3 id="维度与点积大小的关系"><a href="#维度与点积大小的关系" class="headerlink" title="维度与点积大小的关系"></a>维度与点积大小的关系</h3><ul><li>针对为什么维度会影响点积的大小, 原始论文中有这样的一点解释如下:</li></ul><pre class="line-numbers language-none"><code class="language-none">To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their doct product,q*k = (q1k1+q2k2+......+q(d_k)k(d_k)), has mean 0 and variance d_k.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>我们分两步对其进行一个推导, 首先就是假设向量q和k的各个分量是相互独立的随机变量, X = q_i, Y = k_i, X和Y各自有d_k个分量, 也就是向量的维度等于d_k, 有E(X) = E(Y) = 0, 以及D(X) = D(Y) = 1.</li><li>可以得到E(XY) = E(X)E(Y) = 0 * 0 = 0</li><li>同理, 对于D(XY)推导如下:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_16.png" alt="img"></p><hr><blockquote><ul><li>根据期望和方差的性质, 对于互相独立的变量满足下式:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_17.png" alt="img"></p><hr><blockquote><ul><li>根据上面的公式, 可以很轻松的得出q*k的均值为E(qk) = 0, D(qk) = d_k.</li><li>所以方差越大, 对应的qk的点积就越大, 这样softmax的输出分布就会更偏向最大值所在的分量.</li><li>一个技巧就是将点积除以sqrt(d_k), 将方差在数学上重新”拉回1”, 如下所示:</li></ul></blockquote><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_18.png" alt="img"></p><hr><blockquote><ul><li>最终的结论: 通过数学上的技巧将方差控制在1, 也就有效的控制了点积结果的发散, 也就控制了对应的梯度消失的问题!</li></ul></blockquote><hr><h3 id="小节总结-7"><a href="#小节总结-7" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>1: 学习了softmax函数的输入是如何影响输出分布的.<ul><li>softmax函数本质是对输入的数据分布做一次归一化处理, 但是输入元素的数量级对softmax最终的分布影响非常之大.</li><li>在输入元素的数量级较大时, softmax函数几乎将全部的概率分布都分配给了最大值分量所对应的标签.</li></ul></li><li>2: 学习了softmax函数在反向传播的过程中是如何梯度求导的.<ul><li>具体的推导过程见讲义正文部分, 注意要分两种情况讨论, 分别处理.</li></ul></li><li>3: 学习了softmax函数出现梯度消失现象的原因.<ul><li>结合第一步, 第二步的结论, 可以很清楚的看到最终的梯度矩阵接近于零矩阵, 这样在进行参数更新的时候就会产生梯度消失现象.</li></ul></li><li>4: 学习了维度和点积大小的关系推导.<ul><li>通过期望和方差的推导理解了为什么点积会造成方差变大.</li><li>理解了通过数学技巧除以sqrt(d_k)就可以让方差恢复成1.</li></ul></li></ul><hr><hr><hr><h2 id="1-9-Transformer架构的并行化是如何进行的-具体体现在哪里"><a href="#1-9-Transformer架构的并行化是如何进行的-具体体现在哪里" class="headerlink" title="1.9 Transformer架构的并行化是如何进行的? 具体体现在哪里?"></a>1.9 Transformer架构的并行化是如何进行的? 具体体现在哪里?</h2><hr><h3 id="学习目标-8"><a href="#学习目标-8" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握Transformer架构的并行化是如何进行的.</li><li>理解为什么采用这样的方式可以实现Transformer的并行化.</li></ul><hr><h3 id="Transformer架构中Encoder的并行化"><a href="#Transformer架构中Encoder的并行化" class="headerlink" title="Transformer架构中Encoder的并行化"></a>Transformer架构中Encoder的并行化</h3><ul><li>首先Transformer的并行化主要体现在Encoder模块上.</li></ul><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_19.png" alt="img"></p><hr><blockquote><ul><li>1: 上图最底层绿色的部分, 整个序列所有的token可以并行的进行Embedding操作, 这一层的处理是没有依赖关系的.</li><li>2: 上图第二层土黄色的部分, 也就是Transformer中最重要的self-attention部分, 这里对于任意一个单词比如x1, 要计算x1对于其他所有token的注意力分布, 得到z1. 这个过程是具有依赖性的, 必须等到序列中所有的单词完成Embedding才可以进行. 因此这一步是不能并行处理的. 但是从另一个角度看, 我们真实计算注意力分布的时候, 采用的都是矩阵运算, 也就是可以一次性的计算出所有token的注意力张量, 从这个角度看也算是实现了并行, 只是矩阵运算的”并行”和词嵌入的”并行”概念上不同而已.</li><li>3: 上图第三层蓝色的部分, 也就是前馈全连接层, 对于不同的向量z之间也是没有依赖关系的, 所以这一层是可以实现并行化处理的. 也就是所有的向量z输入Feed Forward网络的计算可以同步进行, 互不干扰.</li></ul></blockquote><hr><h3 id="Transformer架构中Decoder的并行化"><a href="#Transformer架构中Decoder的并行化" class="headerlink" title="Transformer架构中Decoder的并行化"></a>Transformer架构中Decoder的并行化</h3><ul><li>其次Transformer的并行化也部分的体现在Decoder模块上.</li></ul><p><img src="/2021/04/23/bert-transformer-mo-xing-jia-gou-yu-xiang-jie/picture_20.png" alt="img"></p><hr><blockquote><ul><li>1: Decoder模块在训练阶段采用了并行化处理. 其中Self-Attention和Encoder-Decoder Attention两个子层的并行化也是在进行矩阵乘法, 和Encoder的理解是一致的. 在进行Embedding和Feed Forward的处理时, 因为各个token之间没有依赖关系, 所以也是可以完全并行化处理的, 这里和Encoder的理解也是一致的.</li><li>2: Decoder模块在预测阶段基本上不认为采用了并行化处理. 因为第一个time step的输入只是一个”SOS”, 后续每一个time step的输入也只是依次添加之前所有的预测token.</li><li>3: 注意: 最重要的区别是训练阶段目标文本如果有20个token, 在训练过程中是一次性的输入给Decoder端, 可以做到一些子层的并行化处理. 但是在预测阶段, 如果预测的结果语句总共有20个token, 则需要重复处理20次循环的过程, 每次的输入添加进去一个token, 每次的输入序列比上一次多一个token, 所以不认为是并行处理.</li></ul></blockquote><hr><h3 id="小节总结-8"><a href="#小节总结-8" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>学习了Transformer架构中Encoder模块的并行化机制.<ul><li>Encoder模块在训练阶段和测试阶段都可以实现完全相同的并行化.</li><li>Encoder模块在Embedding层, Feed Forward层, Add &amp; Norm层都是可以并行化的.</li><li>Encoder模块在self-attention层, 因为各个token之间存在依赖关系, 无法独立计算, 不是真正意义上的并行化.</li><li>Encoder模块在self-attention层, 因为采用了矩阵运算的实现方式, 可以一次性的完成所有注意力张量的计算, 也是另一种”并行化”的体现.</li></ul></li><li>学习了Transformer架构中Decoder模块的并行化机制.<ul><li>Decoder模块在训练阶段可以实现并行化.</li><li>Decoder模块在训练阶段的Embedding层, Feed Forward层, Add &amp; Norm层都是可以并行化的.</li><li>Decoder模块在self-attention层, 以及Encoder-Decoder Attention层, 因为各个token之间存在依赖关系, 无法独立计算, 不是真正意义上的并行化.</li><li>Decoder模块在self-attention层, 以及Encoder-Decoder Attention层, 因为采用了矩阵运算的实现方式, 可以一次性的完成所有注意力张量的计算, 也是另一种”并行化”的体现.</li><li>Decoder模块在预测计算不能并行化处理.</li></ul></li></ul><hr><hr><hr><h2 id="1-10-BERT模型的优点和缺点"><a href="#1-10-BERT模型的优点和缺点" class="headerlink" title="1.10 BERT模型的优点和缺点?"></a>1.10 BERT模型的优点和缺点?</h2><hr><h3 id="学习目标-9"><a href="#学习目标-9" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>理解BERT模型的优点和原因.</li><li>理解BERT模型的缺点和原因.</li></ul><hr><h3 id="BERT的优点"><a href="#BERT的优点" class="headerlink" title="BERT的优点"></a>BERT的优点</h3><ul><li>1: 通过预训练, 加上Fine-tunning, 在11项NLP任务上取得最优结果.</li><li>2: BERT的根基源于Transformer, 相比传统RNN更加高效, 可以并行化处理同时能捕捉长距离的语义和结构依赖.</li><li>3: BERT采用了Transformer架构中的Encoder模块, 不仅仅获得了真正意义上的bidirectional context, 而且为后续微调任务留出了足够的调整空间.</li></ul><hr><h3 id="BERT的缺点"><a href="#BERT的缺点" class="headerlink" title="BERT的缺点"></a>BERT的缺点</h3><ul><li>1: BERT模型过于庞大, 参数太多, 不利于资源紧张的应用场景, 也不利于上线的实时处理.</li><li>2: BERT目前给出的中文模型中, 是以字为基本token单位的, 很多需要词向量的应用无法直接使用. 同时该模型无法识别很多生僻词, 只能以UNK代替.</li><li>3: BERT中第一个预训练任务MLM中, [MASK]标记只在训练阶段出现, 而在预测阶段不会出现, 这就造成了一定的信息偏差, 因此训练时不能过多的使用[MASK], 否则会影响模型的表现.</li><li>4: 按照BERT的MLM任务中的约定, 每个batch数据中只有15%的token参与了训练, 被模型学习和预测, 所以BERT收敛的速度比left-to-right模型要慢很多(left-to-right模型中每一个token都会参与训练).</li></ul><hr><h3 id="小节总结-9"><a href="#小节总结-9" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>学习了BERT模型的3个优点:<ul><li>在11个NLP任务上取得SOAT成绩.</li><li>利用了Transformer的并行化能力以及长语句捕捉语义依赖和结构依赖.</li><li>BERT实现了双向Transformer并为后续的微调任务留出足够的空间.</li></ul></li><li>学习了BERT模型的4个缺点:<ul><li>BERT模型太大, 太慢.</li><li>BERT模型中的中文模型是以字为基本token单位的, 无法利用词向量, 无法识别生僻词.</li><li>BERT模型中的MLM任务, [MASK]标记在训练阶段出现, 预测阶段不出现, 这种偏差会对模型有一定影响.</li><li>BERT模型的MLM任务, 每个batch只有15%的token参与了训练, 造成大量文本数据的”无用”, 收敛速度慢, 需要的算力和算时都大大提高.</li></ul></li></ul><hr><hr><hr><h2 id="1-11-BERT的MLM任务中为什么采用了80-10-10-的策略"><a href="#1-11-BERT的MLM任务中为什么采用了80-10-10-的策略" class="headerlink" title="1.11 BERT的MLM任务中为什么采用了80%, 10%, 10%的策略?"></a>1.11 BERT的MLM任务中为什么采用了80%, 10%, 10%的策略?</h2><hr><h3 id="学习目标-10"><a href="#学习目标-10" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>理解在MLM任务中采用80%, 10%, 10%策略的原因.</li></ul><hr><h3 id="MLM任务中的策略约定分析"><a href="#MLM任务中的策略约定分析" class="headerlink" title="MLM任务中的策略约定分析"></a>MLM任务中的策略约定分析</h3><ul><li>1: 首先, 如果所有参与训练的token被100%的[MASK], 那么在fine-tunning的时候所有单词都是已知的, 不存在[MASK], 那么模型就只能根据其他token的信息和语序结构来预测当前词, 而无法利用到这个词本身的信息, 因为它们从未出现在训练过程中, 等于模型从未接触到它们的信息, 等于整个语义空间损失了部分信息. 采用80%的概率下应用[MASK], 既可以让模型去学着预测这些单词, 又以20%的概率保留了语义信息展示给模型.</li><li>2: 保留下来的信息如果全部使用原始token, 那么模型在预训练的时候可能会偷懒, 直接照抄当前token信息. 采用10%概率下random token来随机替换当前token, 会让模型不能去死记硬背当前的token, 而去尽力学习单词周边的语义表达和远距离的信息依赖, 尝试建模完整的语言信息.</li><li>3: 最后再以10%的概率保留原始的token, 意义就是保留语言本来的面貌, 让信息不至于完全被遮掩, 使得模型可以”看清”真实的语言面貌.</li></ul><hr><h3 id="小节总结-10"><a href="#小节总结-10" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>BERT中MLM任务中的[MASK]是以一种显示的方式告诉模型”这个词我不告诉你, 你自己从上下文里猜”, 非常类似于同学们在做完形填空. 如果[MASK]意外的部分全部都用原始token, 模型会学习到”如果当前词是[MASK], 就根据其他词的信息推断这个词; 如果当前词是一个正常的单词, 就直接照抄”. 这样一来, 到了fine-tunning阶段, 所有单词都是正常单词了, 模型就会照抄所有单词, 不再提取单词之间的依赖关系了.</li><li>BERT中MLM任务以10%的概率填入random token, 就是让模型时刻处于”紧张情绪”中, 让模型搞不清楚当前看到的token是真实的单词还是被随机替换掉的单词, 这样模型在任意的token位置就只能把当前token的信息和上下文信息结合起来做综合的判断和建模. 这样一来, 到了fine-tunning阶段, 模型也会同时提取这两方面的信息, 因为模型”心理很紧张”, 它不知道当前看到的这个token, 所谓的”正常单词”到底有没有”提前被动过手脚”.</li></ul><hr><hr><hr><h2 id="1-12-长文本预测任务如果想用BERT来实现-要如何构造训练样本"><a href="#1-12-长文本预测任务如果想用BERT来实现-要如何构造训练样本" class="headerlink" title="1.12 长文本预测任务如果想用BERT来实现, 要如何构造训练样本?"></a>1.12 长文本预测任务如果想用BERT来实现, 要如何构造训练样本?</h2><hr><h3 id="学习目标-11"><a href="#学习目标-11" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>掌握利用BERT处理长文本的任务如何构造训练样本.</li></ul><hr><h3 id="BERT处理长文本的方法"><a href="#BERT处理长文本的方法" class="headerlink" title="BERT处理长文本的方法"></a>BERT处理长文本的方法</h3><ul><li>首选要明确一点, BERT预训练模型所接收的最大sequence长度是512.</li><li>那么对于长文本(文本长度超过512的句子), 就需要特殊的方式来构造训练样本. 核心就是如何进行截断.<ul><li>1: head-only方式: 这是只保留长文本头部信息的截断方式, 具体为保存前510个token (要留两个位置给[CLS]和[SEP]).</li><li>2: tail-only方式: 这是只保留长文本尾部信息的截断方式, 具体为保存最后510个token (要留两个位置给[CLS]和[SEP]).</li><li>3: head+only方式: 选择前128个token和最后382个token (文本总长度在800以内), 或者前256个token和最后254个token (文本总长度大于800).</li></ul></li></ul><hr><h3 id="小节总结-11"><a href="#小节总结-11" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>学习了长文本处理如果要利用BERT的话, 需要进行截断处理.<ul><li>第一种方式就是只保留前面510个token.</li><li>第二种方式就是只保留后面510个token.</li><li>第三种方式就是前后分别保留一部分token, 总数是510.</li></ul></li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> Transformer </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fasttext使用教程</title>
      <link href="2021/04/23/fasttext-shi-yong-jiao-cheng/"/>
      <url>2021/04/23/fasttext-shi-yong-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="第一章-fasttext工具的使用"><a href="#第一章-fasttext工具的使用" class="headerlink" title="第一章:fasttext工具的使用"></a>第一章:fasttext工具的使用</h1><h2 id="1-1-认识fasttext工具"><a href="#1-1-认识fasttext工具" class="headerlink" title="1.1 认识fasttext工具"></a>1.1 认识fasttext工具</h2><hr><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解fasttext工具的作用.</li><li>了解fasttext工具的优势及其原因.</li><li>掌握fasttext的安装方法.</li></ul><hr><p><img src="/2021/04/23/fasttext-shi-yong-jiao-cheng/fasttext-logo-color-web.png" alt="img"></p><hr><ul><li>作为NLP工程领域常用的工具包, fasttext有两大作用:<ul><li>进行文本分类</li><li>训练词向量</li></ul></li></ul><hr><ul><li>fasttext工具包的优势:<ul><li>正如它的名字, 在保持较高精度的情况下, 快速的进行训练和预测是fasttext的最大优势.</li></ul></li></ul><hr><ul><li>fasttext优势的原因:<ul><li>fasttext工具包中内含的fasttext模型具有十分简单的网络结构.</li><li>使用fasttext模型训练词向量时使用层次softmax结构, 来提升超多类别下的模型性能.</li><li>由于fasttext模型过于简单无法捕捉词序特征, 因此会进行n-gram特征提取以弥补模型缺陷提升精度.</li></ul></li></ul><hr><ul><li>fasttext的安装:</li></ul><pre class="line-numbers language-none"><code class="language-none">$ git clone https://github.com/facebookresearch/fastText.git$ cd fastText# 使用pip安装python中的fasttext工具包$ sudo pip install .<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>验证安装:</li></ul><pre class="line-numbers language-none"><code class="language-none">Python 3.7.3 (default, Mar 27 2019, 22:11:17)[GCC 7.3.0] :: Anaconda, Inc. on linuxType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import fasttext&gt;&gt;&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="1-2-进行文本分类"><a href="#1-2-进行文本分类" class="headerlink" title="1.2 进行文本分类"></a>1.2 进行文本分类</h2><hr><h3 id="学习目标-1"><a href="#学习目标-1" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解什么是文本分类及其种类.</li><li>掌握fasttext工具进行文本分类的过程.</li></ul><hr><h3 id="什么是文本分类"><a href="#什么是文本分类" class="headerlink" title="什么是文本分类"></a>什么是文本分类</h3><ul><li>文本分类的是将文档（例如电子邮件，帖子，文本消息，产品评论等）分配给一个或多个类别. 当今文本分类的实现多是使用机器学习方法从训练数据中提取分类规则以进行分类, 因此构建文本分类器需要带标签的数据.</li></ul><hr><h3 id="文本分类的种类"><a href="#文本分类的种类" class="headerlink" title="文本分类的种类"></a>文本分类的种类</h3><ul><li>二分类:<ul><li>文本被分类两个类别中, 往往这两个类别是对立面, 比如: 判断一句评论是好评还是差评.</li></ul></li><li>单标签多分类:<ul><li>文本被分入到多个类别中, 且每条文本只能属于某一个类别(即被打上某一个标签), 比如: 输入一个人名, 判断它是来自哪个国家的人名.</li></ul></li><li>多标签多分类:<ul><li>文本被分人到多个类别中, 但每条文本可以属于多个类别(即被打上多个标签), 比如: 输入一段描述, 判断可能是和哪些兴趣爱好有关, 一段描述中可能即讨论了美食, 又太讨论了游戏爱好.</li></ul></li></ul><hr><h3 id="使用fasttext工具进行文本分类的过程"><a href="#使用fasttext工具进行文本分类的过程" class="headerlink" title="使用fasttext工具进行文本分类的过程"></a>使用fasttext工具进行文本分类的过程</h3><ul><li>第一步: 获取数据</li><li>第二步: 训练集与验证集的划分</li><li>第三步: 训练模型</li><li>第四步: 使用模型进行预测并评估</li><li>第五步: 模型调优</li><li>第六步: 模型保存与重加载</li></ul><hr><h4 id="第一步-获取数据"><a href="#第一步-获取数据" class="headerlink" title="第一步: 获取数据"></a>第一步: 获取数据</h4><pre class="line-numbers language-none"><code class="language-none"># 获取烹饪相关的数据集, 它是由facebook AI实验室提供的演示数据集$ wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz &amp;&amp; tar xvzf cooking.stackexchange.tar.gz# 查看数据的前10条$ head cooking.stackexchange.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><pre class="line-numbers language-none"><code class="language-none">__label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe?__label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments__label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove?__label__restaurant Michelin Three Star Restaurant; but if the chef is not there__label__knife-skills __label__dicing Without knife skills, how can I quickly and accurately dice vegetables?__label__storage-method __label__equipment __label__bread What's the purpose of a bread box?__label__baking __label__food-safety __label__substitutions __label__peanuts how to seperate peanut oil from roasted peanuts at home?__label__chocolate American equivalent for British chocolate terms__label__baking __label__oven __label__convection Fan bake vs bake__label__sauce __label__storage-lifetime __label__acidity __label__mayonnaise Regulation and balancing of readymade packed mayonnaise and other sauces<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>数据说明:<ul><li>cooking.stackexchange.txt中的每一行都包含一个标签列表，后跟相应的文档, 标签列表以类似”__label__sauce __label__cheese”的形式展现, 代表有两个标签sauce和cheese, 所有标签__label__均以前缀开头，这是fastText识别标签或单词的方式. 标签之后的一段话就是文本信息.如: How much does potato starch affect a cheese sauce recipe?</li></ul></li></ul></blockquote><hr><h4 id="第二步-训练集与验证集的划分"><a href="#第二步-训练集与验证集的划分" class="headerlink" title="第二步: 训练集与验证集的划分"></a>第二步: 训练集与验证集的划分</h4><pre class="line-numbers language-none"><code class="language-none"># 查看数据总数$ wc cooking.stackexchange.txt <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">15404  169582 1401900 cooking.stackexchange.txt <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none"># 12404条数据作为训练数据$ head -n 12404 cooking.stackexchange.txt &gt; cooking.train# 3000条数据作为验证数据$ tail -n 3000 cooking.stackexchange.txt &gt; cooking.valid<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第三步-训练模型"><a href="#第三步-训练模型" class="headerlink" title="第三步: 训练模型"></a>第三步: 训练模型</h4><pre class="line-numbers language-none"><code class="language-none"># 代码运行在python解释器中# 导入fasttext&gt;&gt;&gt; import fasttext# 使用fasttext的train_supervised方法进行文本分类模型的训练&gt;&gt;&gt; model = fasttext.train_supervised(input="cooking.train")# 获得结果Read 0M words# 不重复的词汇总数Number of words:  14543# 标签总数Number of labels: 735# Progress: 训练进度, 因为我们这里显示的是最后的训练完成信息, 所以进度是100%# words/sec/thread: 每个线程每秒处理的平均词汇数# lr: 当前的学习率, 因为训练完成所以学习率是0# avg.loss: 训练过程的平均损失 # ETA: 预计剩余训练时间, 因为已训练完成所以是0Progress: 100.0% words/sec/thread:   60162 lr:  0.000000 avg.loss: 10.056812 ETA:   0h 0m 0s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第四步-使用模型进行预测并评估"><a href="#第四步-使用模型进行预测并评估" class="headerlink" title="第四步: 使用模型进行预测并评估"></a>第四步: 使用模型进行预测并评估</h4><pre class="line-numbers language-none"><code class="language-none"># 使用模型预测一段输入文本, 通过我们常识, 可知预测是正确的, 但是对应预测概率并不大&gt;&gt;&gt; model.predict("Which baking dish is best to bake a banana bread ?")# 元组中的第一项代表标签, 第二项代表对应的概率(('__label__baking',), array([0.06550845]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none"># 通过我们常识可知预测是错误的&gt;&gt;&gt; model.predict("Why not put knives in the dishwasher?")(('__label__food-safety',), array([0.07541209]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none"># 为了评估模型到底表现如何, 我们在3000条的验证集上进行测试&gt;&gt;&gt; model.test("cooking.valid")# 元组中的每项分别代表, 验证集样本数量, 精度以及召回率 # 我们看到模型精度和召回率表现都很差, 接下来我们讲学习如何进行优化.(3000, 0.124, 0.0541)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第五步-模型调优"><a href="#第五步-模型调优" class="headerlink" title="第五步: 模型调优"></a>第五步: 模型调优</h4><ul><li>原始数据处理:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 通过查看数据, 我们发现数据中存在许多标点符号与单词相连以及大小写不统一, # 这些因素对我们最终的分类目标没有益处, 反是增加了模型提取分类规律的难度,# 因此我们选择将它们去除或转化# 处理前的部分数据__label__fish Arctic char available in North-America__label__pasta __label__salt __label__boiling When cooking pasta in salted water how much of the salt is absorbed?__label__coffee Emergency Coffee via Chocolate Covered Coffee Beans?__label__cake Non-beet alternatives to standard red food dye__label__cheese __label__lentils Could cheese "halt" the tenderness of cooking lentils?__label__asian-cuisine __label__chili-peppers __label__kimchi __label__korean-cuisine What kind of peppers are used in Gochugaru ()?__label__consistency Pavlova Roll failure__label__eggs __label__bread What qualities should I be looking for when making the best French Toast?__label__meat __label__flour __label__stews __label__braising Coating meat in flour before browning, bad idea?__label__food-safety Raw roast beef on the edge of safe?__label__pork __label__food-identification How do I determine the cut of a pork steak prior to purchasing it?<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none"># 通过服务器终端进行简单的数据预处理# 使标点符号与单词分离并统一使用小写字母&gt;&gt; cat cooking.stackexchange.txt | sed -e "s/\([.\!?,'/()]\)/ \1 /g" | tr "[:upper:]" "[:lower:]" &gt; cooking.preprocessed.txt&gt;&gt; head -n 12404 cooking.preprocessed.txt &gt; cooking.train&gt;&gt; tail -n 3000 cooking.preprocessed.txt &gt; cooking.valid<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none"># 处理后的部分数据__label__fish arctic char available in north-america__label__pasta __label__salt __label__boiling when cooking pasta in salted water how much of the salt is absorbed ?__label__coffee emergency coffee via chocolate covered coffee beans ?__label__cake non-beet alternatives to standard red food dye__label__cheese __label__lentils could cheese "halt" the tenderness of cooking lentils ?__label__asian-cuisine __label__chili-peppers __label__kimchi __label__korean-cuisine what kind of peppers are used in gochugaru  (  )  ?__label__consistency pavlova roll failure__label__eggs __label__bread what qualities should i be looking for when making the best french toast ?__label__meat __label__flour __label__stews __label__braising coating meat in flour before browning ,  bad idea ?__label__food-safety raw roast beef on the edge of safe ?__label__pork __label__food-identification how do i determine the cut of a pork steak prior to purchasing it ?<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>数据处理后进行训练并测试:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 重新训练&gt;&gt;&gt; model = fasttext.train_supervised(input="cooking.train")Read 0M words# 不重复的词汇总数减少很多, 因为之前会把带大写字母或者与标点符号相连接的单词都认为是新的单词Number of words:  8952Number of labels: 735# 我们看到平均损失有所下降Progress: 100.0% words/sec/thread:   65737 lr:  0.000000 avg.loss:  9.966091 ETA:   0h 0m 0s# 重新测试&gt;&gt;&gt; model.test("cooking.valid")# 我们看到精度和召回率都有所提升(3000, 0.161, 0.06962663975782038)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>增加训练轮数:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 设置train_supervised方法中的参数epoch来增加训练轮数, 默认的轮数是5次# 增加轮数意味着模型能够有更多机会在有限数据中调整分类规律, 当然这也会增加训练时间&gt;&gt;&gt; model = fasttext.train_supervised(input="cooking.train", epoch=25)Read 0M wordsNumber of words:  8952Number of labels: 735# 我们看到平均损失继续下降Progress: 100.0% words/sec/thread:   66283 lr:  0.000000 avg.loss:  7.203885 ETA:   0h 0m 0s&gt;&gt;&gt; model.test("cooking.valid")# 我们看到精度已经提升到了42%, 召回率提升至18%.(3000, 0.4206666666666667, 0.1819230214790255)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>调整学习率:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 设置train_supervised方法中的参数lr来调整学习率, 默认的学习率大小是0.1# 增大学习率意味着增大了梯度下降的步长使其在有限的迭代步骤下更接近最优点&gt;&gt;&gt; model = fasttext.train_supervised(input="cooking.train", lr=1.0, epoch=25)Read 0M wordsNumber of words:  8952Number of labels: 735# 平均损失继续下降Progress: 100.0% words/sec/thread:   66027 lr:  0.000000 avg.loss:  4.278283 ETA:   0h 0m 0s&gt;&gt;&gt; model.test("cooking.valid")# 我们看到精度已经提升到了47%, 召回率提升至20%.(3000, 0.47633333333333333, 0.20599682860025947)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>增加n-gram特征:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 设置train_supervised方法中的参数wordNgrams来添加n-gram特征, 默认是1, 也就是没有n-gram特征# 我们这里将其设置为2意味着添加2-gram特征, 这些特征帮助模型捕捉前后词汇之间的关联, 更好的提取分类规则用于模型分类, 当然这也会增加模型训时练占用的资源和时间.&gt;&gt;&gt; model = fasttext.train_supervised(input="cooking.train", lr=1.0, epoch=25, wordNgrams=2)Read 0M wordsNumber of words:  8952Number of labels: 735# 平均损失继续下降Progress: 100.0% words/sec/thread:   65084 lr:  0.000000 avg.loss:  3.189422 ETA:   0h 0m 0s&gt;&gt;&gt; model.test("cooking.valid")# 我们看到精度已经提升到了49%, 召回率提升至21%.(3000, 0.49233333333333335, 0.2129162462159435)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>修改损失计算方式:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 随着我们不断的添加优化策略, 模型训练速度也越来越慢# 为了能够提升fasttext模型的训练效率, 减小训练时间# 设置train_supervised方法中的参数loss来修改损失计算方式(等效于输出层的结构), 默认是softmax层结构# 我们这里将其设置为'hs', 代表层次softmax结构, 意味着输出层的结构(计算方式)发生了变化, 将以一种更低复杂度的方式来计算损失.&gt;&gt;&gt; model = fasttext.train_supervised(input="cooking.train", lr=1.0, epoch=25, wordNgrams=2, loss='hs')Read 0M wordsNumber of words:  8952Number of labels: 735Progress: 100.0% words/sec/thread: 1341740 lr:  0.000000 avg.loss:  2.225962 ETA:   0h 0m 0s&gt;&gt;&gt; model.test("cooking.valid")# 我们看到精度和召回率稍有波动, 但训练时间却缩短到仅仅几秒(3000, 0.483, 0.20887991927346114)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>自动超参数调优:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 手动调节和寻找超参数是非常困难的, 因为参数之间可能相关, 并且不同数据集需要的超参数也不同, # 因此可以使用fasttext的autotuneValidationFile参数进行自动超参数调优.# autotuneValidationFile参数需要指定验证数据集所在路径, 它将在验证集上使用随机搜索方法寻找可能最优的超参数.# 使用autotuneDuration参数可以控制随机搜索的时间, 默认是300s, 根据不同的需求, 我们可以延长或缩短时间.# 验证集路径'cooking.valid', 随机搜索600秒&gt;&gt;&gt; model = fasttext.train_supervised(input='cooking.train', autotuneValidationFile='cooking.valid', autotuneDuration=600)Progress: 100.0% Trials:   38 Best score:  0.376170 ETA:   0h 0m 0sTraining again with best argumentsRead 0M wordsNumber of words:  8952Number of labels: 735Progress: 100.0% words/sec/thread:   63791 lr:  0.000000 avg.loss:  1.888165 ETA:   0h 0m 0s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>实际生产中多标签多分类问题的损失计算方式:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 针对多标签多分类问题, 使用'softmax'或者'hs'有时并不是最佳选择, 因为我们最终得到的应该是多个标签, 而softmax却只能最大化一个标签. # 所以我们往往会选择为每个标签使用独立的二分类器作为输出层结构, # 对应的损失计算方式为'ova'表示one vs all.# 这种输出层的改变意味着我们在统一语料下同时训练多个二分类模型,# 对于二分类模型来讲, lr不宜过大, 这里我们设置为0.2&gt;&gt;&gt; model = fasttext.train_supervised(input="cooking.train", lr=0.2, epoch=25, wordNgrams=2, loss='ova')Read 0M wordsNumber of words:  8952Number of labels: 735Progress: 100.0% words/sec/thread:   65044 lr:  0.000000 avg.loss:  7.713312 ETA:   0h 0m 0s # 我们使用模型进行单条样本的预测, 来看一下它的输出结果.# 参数k代表指定模型输出多少个标签, 默认为1, 这里设置为-1, 意味着尽可能多的输出.# 参数threshold代表显示的标签概率阈值, 设置为0.5, 意味着显示概率大于0.5的标签&gt;&gt;&gt; model.predict("Which baking dish is best to bake a banana bread ?", k=-1, threshold=0.5)# 我看到根据输入文本, 输出了它的三个最有可能的标签((u'__label__baking', u'__label__bananas', u'__label__bread'), array([1.00000, 0.939923, 0.592677]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第六步-模型保存与重加载"><a href="#第六步-模型保存与重加载" class="headerlink" title="第六步: 模型保存与重加载"></a>第六步: 模型保存与重加载</h4><pre class="line-numbers language-none"><code class="language-none"># 使用model的save_model方法保存模型到指定目录# 你可以在指定目录下找到model_cooking.bin文件&gt;&gt;&gt; model.save_model("./model_cooking.bin")# 使用fasttext的load_model进行模型的重加载&gt;&gt;&gt; model = fasttext.load_model("./model_cooking.bin")# 重加载后的模型使用方法和之前完全相同&gt;&gt;&gt; model.predict("Which baking dish is best to bake a banana bread ?", k=-1, threshold=0.5)((u'__label__baking', u'__label__bananas', u'__label__bread'), array([1.00000, 0.939923, 0.592677]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="小节总结"><a href="#小节总结" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了什么是文本分类:</p><ul><li>文本分类的是将文档（例如电子邮件，帖子，文本消息，产品评论等）分配给一个或多个类别. 当今文本分类的实现多是使用机器学习方法从训练数据中提取分类规则以进行分类, 因此构建文本分类器需要带标签的数据.</li></ul><hr></li><li><p>文本分类的种类:</p><ul><li>二分类:<ul><li>文本被分类两个类别中, 往往这两个类别是对立面, 比如: 判断一句评论是好评还是差评.</li></ul></li><li>单标签多分类:<ul><li>文本被分入到多个类别中, 且每条文本只能属于某一个类别(即被打上某一个标签), 比如: 输入一个人名, 判断它是来自哪个国家的人名.</li></ul></li><li>多标签多分类:<ul><li>文本被分人到多个类别中, 但每条文本可以属于多个类别(即被打上多个标签), 比如: 输入一段描述, 判断可能是和哪些兴趣爱好有关, 一段描述中可能即讨论了美食, 又太讨论了游戏爱好.</li></ul></li></ul><hr></li><li><p>使用fasttext工具进行文本分类的过程:</p><ul><li>第一步: 获取数据</li><li>第二步: 训练集与验证集的划分</li><li>第三步: 训练模型</li><li>第四步: 使用模型进行预测并评估</li><li>第五步: 模型调优</li><li>第六步: 模型保存与重加载</li></ul></li></ul><hr><h2 id="1-3-训练词向量"><a href="#1-3-训练词向量" class="headerlink" title="1.3 训练词向量"></a>1.3 训练词向量</h2><hr><h3 id="学习目标-2"><a href="#学习目标-2" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解词向量的相关知识.</li><li>掌握fasttext工具训练词向量的过程.</li></ul><hr><ul><li>词向量的相关知识:<ul><li>用向量表示文本中的词汇(或字符)是现代机器学习中最流行的做法, 这些向量能够很好的捕捉语言之间的关系, 从而提升基于词向量的各种NLP任务的效果.</li></ul></li></ul><hr><h3 id="使用fasttext工具训练词向量的过程"><a href="#使用fasttext工具训练词向量的过程" class="headerlink" title="使用fasttext工具训练词向量的过程"></a>使用fasttext工具训练词向量的过程</h3><ul><li>第一步: 获取数据</li><li>第二步: 训练词向量</li><li>第三步: 模型超参数设定</li><li>第四步: 模型效果检验</li><li>第五步: 模型的保存与重加载</li></ul><hr><h4 id="第一步-获取数据-1"><a href="#第一步-获取数据-1" class="headerlink" title="第一步: 获取数据"></a>第一步: 获取数据</h4><pre class="line-numbers language-none"><code class="language-none"># 在这里, 我们将研究英语维基百科的部分网页信息, 它的大小在300M左右# 这些语料已经被准备好, 我们可以通过Matt Mahoney的网站下载.# 首先创建一个存储数据的文件夹data$ mkdir data# 使用wget下载数据的zip压缩包, 它将存储在data目录中$ wget -c http://mattmahoney.net/dc/enwik9.zip -P data# 使用unzip解压, 如果你的服务器中还没有unzip命令, 请使用: yum install unzip -y# 解压后在data目录下会出现enwik9的文件夹$ unzip data/enwik9.zip -d data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>查看原始数据:</li></ul><pre class="line-numbers language-none"><code class="language-none">$ head -10 data/enwik9# 原始数据将输出很多包含XML/HTML格式的内容, 这些内容并不是我们需要的&lt;mediawiki xmlns="http://www.mediawiki.org/xml/export-0.3/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd" version="0.3" xml:lang="en"&gt;  &lt;siteinfo&gt;    &lt;sitename&gt;Wikipedia&lt;/sitename&gt;    &lt;base&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/base&gt;    &lt;generator&gt;MediaWiki 1.6alpha&lt;/generator&gt;    &lt;case&gt;first-letter&lt;/case&gt;      &lt;namespaces&gt;      &lt;namespace key="-2"&gt;Media&lt;/namespace&gt;      &lt;namespace key="-1"&gt;Special&lt;/namespace&gt;      &lt;namespace key="0" /&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>原始数据处理:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 使用wikifil.pl文件处理脚本来清除XML/HTML格式的内容# 注: wikifil.pl文件已为大家提供$ perl wikifil.pl data/enwik9 &gt; data/fil9<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><ul><li>查看预处理后的数据:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 查看前80个字符head -c 80 data/fil9# 输出结果为由空格分割的单词 anarchism originated as a term of abuse first used against early working class<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第二步-训练词向量"><a href="#第二步-训练词向量" class="headerlink" title="第二步: 训练词向量"></a>第二步: 训练词向量</h4><pre class="line-numbers language-none"><code class="language-none"># 代码运行在python解释器中# 导入fasttext&gt;&gt;&gt; import fasttext# 使用fasttext的train_unsupervised(无监督训练方法)进行词向量的训练# 它的参数是数据集的持久化文件路径'data/fil9'&gt;&gt;&gt; model = fasttext.train_unsupervised('data/fil9')# 有效训练词汇量为124M, 共218316个单词Read 124M wordsNumber of words:  218316Number of labels: 0Progress: 100.0% words/sec/thread:   53996 lr:  0.000000 loss:  0.734999 ETA:   0h 0m<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>查看单词对应的词向量:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 通过get_word_vector方法来获得指定词汇的词向量&gt;&gt;&gt; model.get_word_vector("the")array([-0.03087516,  0.09221972,  0.17660329,  0.17308897,  0.12863874,        0.13912526, -0.09851588,  0.00739991,  0.37038437, -0.00845221,        ...       -0.21184735, -0.05048715, -0.34571868,  0.23765688,  0.23726143],      dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第三步-模型超参数设定"><a href="#第三步-模型超参数设定" class="headerlink" title="第三步: 模型超参数设定"></a>第三步: 模型超参数设定</h4><pre class="line-numbers language-none"><code class="language-none"># 在训练词向量过程中, 我们可以设定很多常用超参数来调节我们的模型效果, 如:# 无监督训练模式: 'skipgram' 或者 'cbow', 默认为'skipgram', 在实践中，skipgram模式在利用子词方面比cbow更好.# 词嵌入维度dim: 默认为100, 但随着语料库的增大, 词嵌入的维度往往也要更大.# 数据循环次数epoch: 默认为5, 但当你的数据集足够大, 可能不需要那么多次.# 学习率lr: 默认为0.05, 根据经验, 建议选择[0.01，1]范围内.# 使用的线程数thread: 默认为12个线程, 一般建议和你的cpu核数相同.&gt;&gt;&gt; model = fasttext.train_unsupervised('data/fil9', "cbow", dim=300, epoch=1, lr=0.1, thread=8)Read 124M wordsNumber of words:  218316Number of labels: 0Progress: 100.0% words/sec/thread:   49523 lr:  0.000000 avg.loss:  1.777205 ETA:   0h 0m 0s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第四步-模型效果检验"><a href="#第四步-模型效果检验" class="headerlink" title="第四步: 模型效果检验"></a>第四步: 模型效果检验</h4><pre class="line-numbers language-none"><code class="language-none"># 检查单词向量质量的一种简单方法就是查看其邻近单词, 通过我们主观来判断这些邻近单词是否与目标单词相关来粗略评定模型效果好坏.# 查找"运动"的邻近单词, 我们可以发现"体育网", "运动汽车", "运动服"等. &gt;&gt;&gt; model.get_nearest_neighbors('sports')[(0.8414610624313354, 'sportsnet'), (0.8134572505950928, 'sport'), (0.8100415468215942, 'sportscars'), (0.8021156787872314, 'sportsground'), (0.7889881134033203, 'sportswomen'), (0.7863013744354248, 'sportsplex'), (0.7786710262298584, 'sporty'), (0.7696356177330017, 'sportscar'), (0.7619683146476746, 'sportswear'), (0.7600985765457153, 'sportin')]# 查找"音乐"的邻近单词, 我们可以发现与音乐有关的词汇.&gt;&gt;&gt; model.get_nearest_neighbors('music')[(0.8908010125160217, 'emusic'), (0.8464668393135071, 'musicmoz'), (0.8444250822067261, 'musics'), (0.8113634586334229, 'allmusic'), (0.8106718063354492, 'musices'), (0.8049437999725342, 'musicam'), (0.8004694581031799, 'musicom'), (0.7952923774719238, 'muchmusic'), (0.7852965593338013, 'musicweb'), (0.7767147421836853, 'musico')]# 查找"小狗"的邻近单词, 我们可以发现与小狗有关的词汇.&gt;&gt;&gt; model.get_nearest_neighbors('dog')[(0.8456876873970032, 'catdog'), (0.7480780482292175, 'dogcow'), (0.7289096117019653, 'sleddog'), (0.7269964218139648, 'hotdog'), (0.7114801406860352, 'sheepdog'), (0.6947550773620605, 'dogo'), (0.6897546648979187, 'bodog'), (0.6621081829071045, 'maddog'), (0.6605004072189331, 'dogs'), (0.6398137211799622, 'dogpile')]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第五步-模型的保存与重加载"><a href="#第五步-模型的保存与重加载" class="headerlink" title="第五步: 模型的保存与重加载"></a>第五步: 模型的保存与重加载</h4><pre class="line-numbers language-none"><code class="language-none"># 使用save_model保存模型&gt;&gt;&gt; model.save_model("fil9.bin")# 使用fasttext.load_model加载模型&gt;&gt;&gt; model = fasttext.load_model("fil9.bin")&gt;&gt;&gt; model.get_word_vector("the")array([-0.03087516,  0.09221972,  0.17660329,  0.17308897,  0.12863874,        0.13912526, -0.09851588,  0.00739991,  0.37038437, -0.00845221,        ...       -0.21184735, -0.05048715, -0.34571868,  0.23765688,  0.23726143],      dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="小节总结-1"><a href="#小节总结-1" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了词向量的相关知识:</p><ul><li>用向量表示文本中的词汇(或字符)是现代机器学习中最流行的做法, 这些向量能够很好的捕捉语言之间的关系, 从而提升基于词向量的各种NLP任务的效果.</li></ul><hr></li><li><p>使用fasttext工具训练词向量的过程:</p><ul><li>第一步: 获取数据</li><li>第二步: 训练词向量</li><li>第三步: 模型超参数设定</li><li>第四步: 模型效果检验</li><li>第五步: 模型的保存与重加载</li></ul></li></ul><hr><h2 id="1-4-词向量迁移"><a href="#1-4-词向量迁移" class="headerlink" title="1.4 词向量迁移"></a>1.4 词向量迁移</h2><hr><h3 id="学习目标-3"><a href="#学习目标-3" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解什么是词向量迁移.</li><li>了解fasttext工具中有哪些可迁移的词向量模型.</li><li>掌握如何使用fasttext进行词向量模型迁移.</li></ul><hr><ul><li>什么是词向量迁移:<ul><li>使用在大型语料库上已经进行训练完成的词向量模型.</li></ul></li></ul><hr><ul><li>fasttext工具中可以提供的可迁移的词向量:<ul><li>fasttext提供了157种语言的在CommonCrawl和Wikipedia语料上进行训练的可迁移词向量模型, 它们采用CBOW模式进行训练, 词向量维度为300维. 可通过该地址查看具体语言词向量模型: <a href="https://fasttext.cc/docs/en/crawl-vectors.html">https://fasttext.cc/docs/en/crawl-vectors.html</a></li><li>fasttext提供了294种语言的在Wikipedia语料上进行训练的可迁移词向量模型, 它们采用skipgram模式进行训练, 词向量维度同样是300维. 可通过该地址查看具体语言词向量模型: <a href="https://fasttext.cc/docs/en/pretrained-vectors.html">https://fasttext.cc/docs/en/pretrained-vectors.html</a></li></ul></li></ul><hr><h3 id="如何使用fasttext进行词向量模型迁移"><a href="#如何使用fasttext进行词向量模型迁移" class="headerlink" title="如何使用fasttext进行词向量模型迁移"></a>如何使用fasttext进行词向量模型迁移</h3><ul><li>第一步: 下载词向量模型压缩的bin.gz文件</li><li>第二步: 解压bin.gz文件到bin文件</li><li>第三步: 加载bin文件获取词向量</li><li>第四步: 利用邻近词进行效果检验</li></ul><hr><h4 id="第一步-下载词向量模型压缩的bin-gz文件"><a href="#第一步-下载词向量模型压缩的bin-gz文件" class="headerlink" title="第一步: 下载词向量模型压缩的bin.gz文件"></a>第一步: 下载词向量模型压缩的bin.gz文件</h4><pre class="line-numbers language-none"><code class="language-none"># 这里我们以迁移在CommonCrawl和Wikipedia语料上进行训练的中文词向量模型为例:# 下载中文词向量模型(bin.gz文件)wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.bin.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><h4 id="第二步-解压bin-gz文件到bin文件"><a href="#第二步-解压bin-gz文件到bin文件" class="headerlink" title="第二步: 解压bin.gz文件到bin文件"></a>第二步: 解压bin.gz文件到bin文件</h4><pre class="line-numbers language-none"><code class="language-none"># 使用gunzip进行解压, 获取cc.zh.300.bin文件gunzip cc.zh.300.bin.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><h4 id="第三步-加载bin文件获取词向量"><a href="#第三步-加载bin文件获取词向量" class="headerlink" title="第三步: 加载bin文件获取词向量"></a>第三步: 加载bin文件获取词向量</h4><pre class="line-numbers language-none"><code class="language-none"># 加载模型&gt;&gt;&gt; model = fasttext.load_model("cc.zh.300.bin")# 查看前100个词汇(这里的词汇是广义的, 可以是中文符号或汉字))&gt;&gt;&gt; model.words[:100]['，', '的', '。', '&lt;/s&gt;', '、', '是', '一', '在', '：', '了', '（', '）', "'", '和', '不', '有', '我', ',', ')', '(', '“', '”', '也', '人', '个', ':', '中', '.', '就', '他', '》', '《', '-', '你', '都', '上', '大', '！', '这', '为', '多', '与', '章', '「', '到', '」', '要', '？', '被', '而', '能', '等', '可以', '年', '；', '|', '以', '及', '之', '公司', '对', '中国', '很', '会', '小', '但', '我们', '最', '更', '/', '1', '三', '新', '自己', '可', '2', '或', '次', '好', '将', '第', '种', '她', '…', '3', '地', '對', '用', '工作', '下', '后', '由', '两', '使用', '还', '又', '您', '?', '其', '已']# 使用模型获得'音乐'这个名词的词向量&gt;&gt;&gt; model.get_word_vector("音乐")array([-6.81843981e-02,  3.84048335e-02,  4.63239700e-01,  6.11658543e-02,        9.38086119e-03, -9.63955745e-02,  1.28141120e-01, -6.51574507e-02,        ...        3.13430429e-02, -6.43611327e-02,  1.68979481e-01, -1.95011273e-01],      dtype=float32)    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第四步-利用邻近词进行效果检验"><a href="#第四步-利用邻近词进行效果检验" class="headerlink" title="第四步: 利用邻近词进行效果检验"></a>第四步: 利用邻近词进行效果检验</h4><pre class="line-numbers language-none"><code class="language-none"># 以'音乐'为例, 返回的邻近词基本上与音乐都有关系, 如乐曲, 音乐会, 声乐等.&gt;&gt;&gt; model.get_nearest_neighbors("音乐")[(0.6703276634216309, '乐曲'), (0.6569967269897461, '音乐人'), (0.6565821170806885, '声乐'), (0.6557438373565674, '轻音乐'), (0.6536258459091187, '音乐家'), (0.6502416133880615, '配乐'), (0.6501686573028564, '艺术'), (0.6437276005744934, '音乐会'), (0.639589250087738, '原声'), (0.6368917226791382, '音响')]# 以'美术'为例, 返回的邻近词基本上与美术都有关系, 如艺术, 绘画, 霍廷霄(满城尽带黄金甲的美术师)等.&gt;&gt;&gt; model.get_nearest_neighbors("美术")[(0.724744975566864, '艺术'), (0.7165924310684204, '绘画'), (0.6741853356361389, '霍廷霄'), (0.6470299363136292, '纯艺'), (0.6335071921348572, '美术家'), (0.6304370164871216, '美院'), (0.624431312084198, '艺术类'), (0.6244068741798401, '陈浩忠'), (0.62302166223526, '美术史'), (0.621710479259491, '环艺系')]# 以'周杰伦'为例, 返回的邻近词基本上与明星有关系, 如杰伦, 周董, 陈奕迅等.&gt;&gt;&gt; model.get_nearest_neighbors("周杰伦")[(0.6995140910148621, '杰伦'), (0.6967097520828247, '周杰倫'), (0.6859776377677917, '周董'), (0.6381043195724487, '陈奕迅'), (0.6367626190185547, '张靓颖'), (0.6313326358795166, '张韶涵'), (0.6271176338195801, '谢霆锋'), (0.6188404560089111, '周华健'), (0.6184280514717102, '林俊杰'), (0.6143589019775391, '王力宏')]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="小节总结-2"><a href="#小节总结-2" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了什么是词向量迁移:</p><ul><li>使用在大型语料库上已经进行训练完成的词向量模型.</li></ul><hr></li><li><p>学习了fasttext工具中可以提供的可迁移的词向量:</p><ul><li>fasttext提供了157种语言的在CommonCrawl和Wikipedia语料上进行训练的可迁移词向量模型, 它们采用CBOW模式进行训练, 词向量维度为300维. 可通过该地址查看具体语言词向量模型: <a href="https://fasttext.cc/docs/en/crawl-vectors.html">https://fasttext.cc/docs/en/crawl-vectors.html</a></li><li>fasttext提供了294种语言的在Wikipedia语料上进行训练的可迁移词向量模型, 它们采用skipgram模式进行训练, 词向量维度同样是300维. 可通过该地址查看具体语言词向量模型: <a href="https://fasttext.cc/docs/en/pretrained-vectors.html">https://fasttext.cc/docs/en/pretrained-vectors.html</a></li></ul><hr></li><li><p>如何使用fasttext进行词向量模型迁移:</p><ul><li>第一步: 下载词向量模型压缩的bin.gz文件</li><li>第二步: 解压bin.gz文件到bin文件</li><li>第三步: 加载bin文件获取词向量</li><li>第四步: 利用邻近词进行效果检验</li></ul></li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> fasttext </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fasttext迁移学习</title>
      <link href="2021/04/23/fasttext-qian-yi-xue-xi/"/>
      <url>2021/04/23/fasttext-qian-yi-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="2-1-迁移学习理论"><a href="#2-1-迁移学习理论" class="headerlink" title="2.1 迁移学习理论"></a>2.1 迁移学习理论</h2><hr><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解迁移学习中的有关概念.</li><li>掌握迁移学习的两种迁移方式.</li></ul><hr><ul><li>迁移学习中的有关概念:<ul><li>预训练模型</li><li>微调</li><li>微调脚本</li></ul></li></ul><hr><ul><li>预训练模型(Pretrained model):<ul><li>一般情况下预训练模型都是大型模型，具备复杂的网络结构，众多的参数量，以及在足够大的数据集下进行训练而产生的模型. 在NLP领域，预训练模型往往是语言模型，因为语言模型的训练是无监督的，可以获得大规模语料，同时语言模型又是许多典型NLP任务的基础，如机器翻译，文本生成，阅读理解等，常见的预训练模型有BERT, GPT, roBERTa, transformer-XL等.</li></ul></li></ul><hr><ul><li>微调(Fine-tuning):<ul><li>根据给定的预训练模型，改变它的部分参数或者为其新增部分输出结构后，通过在小部分数据集上训练，来使整个模型更好的适应特定任务.</li></ul></li></ul><hr><ul><li>微调脚本(Fine-tuning script):<ul><li>实现微调过程的代码文件。这些脚本文件中，应包括对预训练模型的调用，对微调参数的选定以及对微调结构的更改等，同时，因为微调是一个训练过程，它同样需要一些超参数的设定，以及损失函数和优化器的选取等, 因此微调脚本往往也包含了整个迁移学习的过程.</li></ul></li></ul><hr><ul><li>关于微调脚本的说明:<ul><li>一般情况下，微调脚本应该由不同的任务类型开发者自己编写，但是由于目前研究的NLP任务类型（分类，提取，生成）以及对应的微调输出结构都是有限的，有些微调方式已经在很多数据集上被验证是有效的，因此微调脚本也可以使用已经完成的规范脚本.</li></ul></li></ul><hr><ul><li>两种迁移方式:<ul><li>直接使用预训练模型，进行相同任务的处理，不需要调整参数或模型结构，这些模型开箱即用。但是这种情况一般只适用于普适任务, 如：fasttest工具包中预训练的词向量模型。另外，很多预训练模型开发者为了达到开箱即用的效果，将模型结构分各个部分保存为不同的预训练模型，提供对应的加载方法来完成特定目标.</li><li>更加主流的迁移学习方式是发挥预训练模型特征抽象的能力，然后再通过微调的方式，通过训练更新小部分参数以此来适应不同的任务。这种迁移方式需要提供小部分的标注数据来进行监督学习.</li></ul></li></ul><hr><ul><li>关于迁移方式的说明:<ul><li>直接使用预训练模型的方式, 已经在fasttext的词向量迁移中学习. 接下来的迁移学习实践将主要讲解通过微调的方式进行迁移学习.</li></ul></li></ul><hr><h2 id="2-2-NLP中的标准数据集"><a href="#2-2-NLP中的标准数据集" class="headerlink" title="2.2 NLP中的标准数据集"></a>2.2 NLP中的标准数据集</h2><hr><h3 id="学习目标-1"><a href="#学习目标-1" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解NLP中GLUE标准数据集合的相关知识.</li><li>掌握GLUE标准数据集合的下载方式, 数据样式及其对应的任务类型.</li></ul><hr><ul><li>GLUE数据集合的介绍:<ul><li>GLUE由纽约大学, 华盛顿大学, Google联合推出, 涵盖不同NLP任务类型, 截止至2020年1月其中包括11个子任务数据集, 成为衡量NLP研究发展的衡量标准.</li></ul></li></ul><hr><h3 id="GLUE数据集合包含以下数据集"><a href="#GLUE数据集合包含以下数据集" class="headerlink" title="GLUE数据集合包含以下数据集"></a>GLUE数据集合包含以下数据集</h3><ul><li>CoLA 数据集</li><li>SST-2 数据集</li><li>MRPC 数据集</li><li>STS-B 数据集</li><li>QQP 数据集</li><li>MNLI 数据集</li><li>SNLI 数据集</li><li>QNLI 数据集</li><li>RTE 数据集</li><li>WNLI 数据集</li><li>diagnostics数据集(官方未完善)</li></ul><hr><ul><li>GLUE数据集合的下载方式:</li></ul><blockquote><ul><li>下载脚本代码:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">''' Script for downloading all GLUE data.'''import osimport sysimport shutilimport argparseimport tempfileimport urllib.requestimport zipfileTASKS = ["CoLA", "SST", "MRPC", "QQP", "STS", "MNLI", "SNLI", "QNLI", "RTE", "WNLI", "diagnostic"]TASK2PATH = {"CoLA":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&amp;token=46d5e637-3411-4188-bc44-5809b5bfb5f4',             "SST":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&amp;token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8',             "MRPC":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&amp;token=ec5c0836-31d5-48f4-b431-7480817f1adc',             "QQP":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&amp;token=700c6acf-160d-4d89-81d1-de4191d02cb5',             "STS":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&amp;token=bddb94a7-8706-4e0d-a694-1109e12273b5',             "MNLI":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&amp;token=50329ea1-e339-40e2-809c-10c40afff3ce',             "SNLI":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&amp;token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df',             "QNLI": 'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&amp;token=6fdcf570-0fc5-4631-8456-9505272d1601',             "RTE":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&amp;token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb',             "WNLI":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&amp;token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf',             "diagnostic":'https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&amp;Expires=2498860800&amp;Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D'}MRPC_TRAIN = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt'MRPC_TEST = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt'def download_and_extract(task, data_dir):    print("Downloading and extracting %s..." % task)    data_file = "%s.zip" % task    urllib.request.urlretrieve(TASK2PATH[task], data_file)    with zipfile.ZipFile(data_file) as zip_ref:        zip_ref.extractall(data_dir)    os.remove(data_file)    print("\tCompleted!")def format_mrpc(data_dir, path_to_data):    print("Processing MRPC...")    mrpc_dir = os.path.join(data_dir, "MRPC")    if not os.path.isdir(mrpc_dir):        os.mkdir(mrpc_dir)    if path_to_data:        mrpc_train_file = os.path.join(path_to_data, "msr_paraphrase_train.txt")        mrpc_test_file = os.path.join(path_to_data, "msr_paraphrase_test.txt")    else:        print("Local MRPC data not specified, downloading data from %s" % MRPC_TRAIN)        mrpc_train_file = os.path.join(mrpc_dir, "msr_paraphrase_train.txt")        mrpc_test_file = os.path.join(mrpc_dir, "msr_paraphrase_test.txt")        urllib.request.urlretrieve(MRPC_TRAIN, mrpc_train_file)        urllib.request.urlretrieve(MRPC_TEST, mrpc_test_file)    assert os.path.isfile(mrpc_train_file), "Train data not found at %s" % mrpc_train_file    assert os.path.isfile(mrpc_test_file), "Test data not found at %s" % mrpc_test_file    urllib.request.urlretrieve(TASK2PATH["MRPC"], os.path.join(mrpc_dir, "dev_ids.tsv"))    dev_ids = []    with open(os.path.join(mrpc_dir, "dev_ids.tsv"), encoding="utf8") as ids_fh:        for row in ids_fh:            dev_ids.append(row.strip().split('\t'))    with open(mrpc_train_file, encoding="utf8") as data_fh, \         open(os.path.join(mrpc_dir, "train.tsv"), 'w', encoding="utf8") as train_fh, \         open(os.path.join(mrpc_dir, "dev.tsv"), 'w', encoding="utf8") as dev_fh:        header = data_fh.readline()        train_fh.write(header)        dev_fh.write(header)        for row in data_fh:            label, id1, id2, s1, s2 = row.strip().split('\t')            if [id1, id2] in dev_ids:                dev_fh.write("%s\t%s\t%s\t%s\t%s\n" % (label, id1, id2, s1, s2))            else:                train_fh.write("%s\t%s\t%s\t%s\t%s\n" % (label, id1, id2, s1, s2))    with open(mrpc_test_file, encoding="utf8") as data_fh, \            open(os.path.join(mrpc_dir, "test.tsv"), 'w', encoding="utf8") as test_fh:        header = data_fh.readline()        test_fh.write("index\t#1 ID\t#2 ID\t#1 String\t#2 String\n")        for idx, row in enumerate(data_fh):            label, id1, id2, s1, s2 = row.strip().split('\t')            test_fh.write("%d\t%s\t%s\t%s\t%s\n" % (idx, id1, id2, s1, s2))    print("\tCompleted!")def download_diagnostic(data_dir):    print("Downloading and extracting diagnostic...")    if not os.path.isdir(os.path.join(data_dir, "diagnostic")):        os.mkdir(os.path.join(data_dir, "diagnostic"))    data_file = os.path.join(data_dir, "diagnostic", "diagnostic.tsv")    urllib.request.urlretrieve(TASK2PATH["diagnostic"], data_file)    print("\tCompleted!")    returndef get_tasks(task_names):    task_names = task_names.split(',')    if "all" in task_names:        tasks = TASKS    else:        tasks = []        for task_name in task_names:            assert task_name in TASKS, "Task %s not found!" % task_name            tasks.append(task_name)    return tasksdef main(arguments):    parser = argparse.ArgumentParser()    parser.add_argument('--data_dir', help='directory to save data to', type=str, default='glue_data')    parser.add_argument('--tasks', help='tasks to download data for as a comma separated string',                        type=str, default='all')    parser.add_argument('--path_to_mrpc', help='path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt',                        type=str, default='')    args = parser.parse_args(arguments)    if not os.path.isdir(args.data_dir):        os.mkdir(args.data_dir)    tasks = get_tasks(args.tasks)    for task in tasks:        if task == 'MRPC':            format_mrpc(args.data_dir, args.path_to_mrpc)        elif task == 'diagnostic':            download_diagnostic(args.data_dir)        else:            download_and_extract(task, args.data_dir)if __name__ == '__main__':    sys.exit(main(sys.argv[1:]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>运行脚本下载所有数据集:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 假设你已经将以上代码copy到download_glue_data.py文件中# 运行这个python脚本, 你将同目录下得到一个glue文件夹python download_glue_data.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">Downloading and extracting CoLA...    Completed!Downloading and extracting SST...    Completed!Processing MRPC...Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt    Completed!Downloading and extracting QQP...    Completed!Downloading and extracting STS...    Completed!Downloading and extracting MNLI...    Completed!Downloading and extracting SNLI...    Completed!Downloading and extracting QNLI...    Completed!Downloading and extracting RTE...    Completed!Downloading and extracting WNLI...    Completed!Downloading and extracting diagnostic...    Completed!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="GLUE数据集合中子数据集的样式及其任务类型"><a href="#GLUE数据集合中子数据集的样式及其任务类型" class="headerlink" title="GLUE数据集合中子数据集的样式及其任务类型"></a>GLUE数据集合中子数据集的样式及其任务类型</h3><h4 id="CoLA数据集文件样式"><a href="#CoLA数据集文件样式" class="headerlink" title="CoLA数据集文件样式"></a>CoLA数据集文件样式</h4><pre class="line-numbers language-none"><code class="language-none">- CoLA/    - dev.tsv      - original/    - test.tsv      - train.tsv<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>文件样式说明:<ul><li>在使用中常用到的文件是train.tsv, dev.tsv, test.tsv, 分别代表训练集, 验证集和测试集. 其中train.tsv与dev.tsv数据样式相同, 都是带有标签的数据, 其中test.tsv是不带有标签的数据.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">...gj04    1       She coughed herself awake as the leaf landed on her nose.gj04    1       The worm wriggled onto the carpet.gj04    1       The chocolate melted onto the carpet.gj04    0   *   The ball wriggled itself loose.gj04    1       Bill wriggled himself loose.bc01    1       The sinking of the ship to collect the insurance was very devious.bc01    1       The ship's sinking was very devious.bc01    0   *   The ship's sinking to collect the insurance was very devious.bc01    1       The testing of such drugs on oneself is too risky.bc01    0   *   This drug's testing on oneself is too risky....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>train.tsv数据样式说明:<ul><li>train.tsv中的数据内容共分为4列, 第一列数据, 如gj04, bc01等代表每条文本数据的来源即出版物代号; 第二列数据, 0或1, 代表每条文本数据的语法是否正确, 0代表不正确, 1代表正确; 第三列数据, ‘<em>‘, 是作者最初的正负样本标记, 与第二列意义相同, ‘</em>‘表示不正确; 第四列即是被标注的语法使用是否正确的文本句子.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>test.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   sentence0   Bill whistled past the house.1   The car honked its way down the road.2   Bill pushed Harry off the sofa.3   the kittens yawned awake and played.4   I demand that the more John eats, the more he pay.5   If John eats more, keep your mouth shut tighter, OK?6   His expectations are always lower than mine are.7   The sooner you call, the more carefully I will word the letter.8   The more timid he feels, the more people he interviews without asking questions of.9   Once Janet left, Fred became a lot crazier....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>test.tsv数据样式说明:<ul><li>test.tsv中的数据内容共分为2列, 第一列数据代表每条文本数据的索引; 第二列数据代表用于测试的句子.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>CoLA数据集的任务类型:<ul><li>二分类任务</li><li>评估指标为: MCC(马修斯相关系数, 在正负样本分布十分不均衡的情况下使用的二分类评估指标)</li></ul></li></ul></blockquote><hr><h4 id="SST-2数据集文件样式"><a href="#SST-2数据集文件样式" class="headerlink" title="SST-2数据集文件样式"></a>SST-2数据集文件样式</h4><pre class="line-numbers language-none"><code class="language-none">- SST-2/        - dev.tsv        - original/        - test.tsv        - train.tsv<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>文件样式说明:<ul><li>在使用中常用到的文件是train.tsv, dev.tsv, test.tsv, 分别代表训练集, 验证集和测试集. 其中train.tsv与dev.tsv数据样式相同, 都是带有标签的数据, 其中test.tsv是不带有标签的数据.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">sentence    labelhide new secretions from the parental units     0contains no wit , only labored gags     0that loves its characters and communicates something rather beautiful about human nature    1remains utterly satisfied to remain the same throughout     0on the worst revenge-of-the-nerds clichés the filmmakers could dredge up    0that 's far too tragic to merit such superficial treatment  0demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .    1of saucy    1a depressed fifteen-year-old 's suicidal poetry     0...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>train.tsv数据样式说明:<ul><li>train.tsv中的数据内容共分为2列, 第一列数据代表具有感情色彩的评论文本; 第二列数据, 0或1, 代表每条文本数据是积极或者消极的评论, 0代表消极, 1代表积极.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>test.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   sentence0   uneasy mishmash of styles and genres .1   this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .2   by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .3   director rob marshall went out gunning to make a great one .4   lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new .5   a well-made and often lovely depiction of the mysteries of friendship .6   none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor .7   although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious .8   it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another .9   this is junk food cinema at its greasiest ....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>test.tsv数据样式说明: * test.tsv中的数据内容共分为2列, 第一列数据代表每条文本数据的索引; 第二列数据代表用于测试的句子.</li></ul></blockquote><hr><blockquote><ul><li>SST-2数据集的任务类型:<ul><li>二分类任务</li><li>评估指标为: ACC</li></ul></li></ul></blockquote><hr><h4 id="MRPC数据集文件样式"><a href="#MRPC数据集文件样式" class="headerlink" title="MRPC数据集文件样式"></a>MRPC数据集文件样式</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">- MRPC/        - dev.tsv        - test.tsv        - train.tsv    - dev_ids.tsv    - msr_paraphrase_test.txt    - msr_paraphrase_train.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>文件样式说明:<ul><li>在使用中常用到的文件是train.tsv, dev.tsv, test.tsv, 分别代表训练集, 验证集和测试集. 其中train.tsv与dev.tsv数据样式相同, 都是带有标签的数据, 其中test.tsv是不带有标签的数据.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">Quality #1 ID   #2 ID   #1 String   #2 String1   702876  702977  Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence . Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .0   2108705 2108831 Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .   Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .1   1330381 1330521 They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .   On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .0   3344667 3344648 Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 . Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .1   1236820 1236712 The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .   PG &amp; E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .1   738533  737951  Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier .   With the scandal hanging over Stewart 's company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .0   264589  264502  The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .    The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .1   579975  579810  The DVD-CCA then appealed to the state Supreme Court .  The DVD CCA appealed that decision to the U.S. Supreme Court ....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>train.tsv数据样式说明:<ul><li>train.tsv中的数据内容共分为5列, 第一列数据, 0或1, 代表每对句子是否具有相同的含义, 0代表含义不相同, 1代表含义相同. 第二列和第三列分别代表每对句子的id, 第四列和第五列分别具有相同/不同含义的句子对.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>test.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   #1 ID   #2 ID   #1 String   #2 String0   1089874 1089925 PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So . Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .1   3019446 3019327 The world 's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected . Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .2   1945605 1945824 According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 .   The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 .3   1430402 1430329 A tropical storm rapidly developed in the Gulf of Mexico Sunday and was expected to hit somewhere along the Texas or Louisiana coasts by Monday night . A tropical storm rapidly developed in the Gulf of Mexico on Sunday and could have hurricane-force winds when it hits land somewhere along the Louisiana coast Monday night .4   3354381 3354396 The company didn 't detail the costs of the replacement and repairs .   But company officials expect the costs of the replacement work to run into the millions of dollars .5   1390995 1391183 The settling companies would also assign their possible claims against the underwriters to the investor plaintiffs , he added . Under the agreement , the settling companies will also assign their potential claims against the underwriters to the investors , he added .6   2201401 2201285 Air Commodore Quaife said the Hornets remained on three-minute alert throughout the operation . Air Commodore John Quaife said the security operation was unprecedented .7   2453843 2453998 A Washington County man may have the countys first human case of West Nile virus , the health department said Friday .  The countys first and only human case of West Nile this year was confirmed by health officials on Sept . 8 ....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>test.tsv数据样式说明: * test.tsv中的数据内容共分为5列, 第一列数据代表每条文本数据的索引; 其余列的含义与train.tsv中相同.</li></ul></blockquote><hr><blockquote><ul><li>MRPC数据集的任务类型:<ul><li>句子对二分类任务</li><li>评估指标为: ACC和F1</li></ul></li></ul></blockquote><hr><h4 id="STS-B数据集文件样式"><a href="#STS-B数据集文件样式" class="headerlink" title="STS-B数据集文件样式"></a>STS-B数据集文件样式</h4><pre class="line-numbers language-none"><code class="language-none">- STS-B/        - dev.tsv        - test.tsv        - train.tsv    - LICENSE.txt    - readme.txt    - original/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>文件样式说明:<ul><li>在使用中常用到的文件是train.tsv, dev.tsv, test.tsv, 分别代表训练集, 验证集和测试集. 其中train.tsv与dev.tsv数据样式相同, 都是带有标签的数据, 其中test.tsv是不带有标签的数据.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   genre   filename    year    old_index   source1 source2 sentence1   sentence2   score0   main-captions   MSRvid  2012test    0001    none    none    A plane is taking off.  An air plane is taking off. 5.0001   main-captions   MSRvid  2012test    0004    none    none    A man is playing a large flute. A man is playing a flute.   3.8002   main-captions   MSRvid  2012test    0005    none    none    A man is spreading shreded cheese on a pizza.   A man is spreading shredded cheese on an uncooked pizza.    3.8003   main-captions   MSRvid  2012test    0006    none    none    Three men are playing chess.Two men are playing chess.  2.6004   main-captions   MSRvid  2012test    0009    none    none    A man is playing the cello.A man seated is playing the cello.   4.2505   main-captions   MSRvid  2012test    0011    none    none    Some men are fighting.  Two men are fighting.   4.2506   main-captions   MSRvid  2012test    0012    none    none    A man is smoking.   A man is skating.   0.5007   main-captions   MSRvid  2012test    0013    none    none    The man is playing the piano.   The man is playing the guitar.  1.6008   main-captions   MSRvid  2012test    0014    none    none    A man is playing on a guitar and singing.   A woman is playing an acoustic guitar and singing.  2.2009   main-captions   MSRvid  2012test    0016    none    none    A person is throwing a cat on to the ceiling.   A person throws a cat on the ceiling.   5.000...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>train.tsv数据样式说明:<ul><li>train.tsv中的数据内容共分为10列, 第一列数据是数据索引; 第二列代表每对句子的来源, 如main-captions表示来自字幕; 第三列代表来源的具体保存文件名, 第四列代表出现时间(年); 第五列代表原始数据的索引; 第六列和第七列分别代表句子对原始来源; 第八列和第九列代表相似程度不同的句子对; 第十列代表句子对的相似程度由低到高, 值域范围是[0, 5].</li></ul></li></ul></blockquote><hr><blockquote><ul><li>test.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   genre   filename    year    old_index   source1 source2 sentence1   sentence20   main-captions   MSRvid  2012test    0024    none    none    A girl is styling her hair. A girl is brushing her hair.1   main-captions   MSRvid  2012test    0033    none    none    A group of men play soccer on the beach.    A group of boys are playing soccer on the beach.2   main-captions   MSRvid  2012test    0045    none    none    One woman is measuring another woman's ankle.   A woman measures another woman's ankle.3   main-captions   MSRvid  2012test    0063    none    none    A man is cutting up a cucumber. A man is slicing a cucumber.4   main-captions   MSRvid  2012test    0066    none    none    A man is playing a harp.    A man is playing a keyboard.5   main-captions   MSRvid  2012test    0074    none    none    A woman is cutting onions.  A woman is cutting tofu.6   main-captions   MSRvid  2012test    0076    none    none    A man is riding an electric bicycle.    A man is riding a bicycle.7   main-captions   MSRvid  2012test    0082    none    none    A man is playing the drums. A man is playing the guitar.8   main-captions   MSRvid  2012test    0092    none    none    A man is playing guitar.    A lady is playing the guitar.9   main-captions   MSRvid  2012test    0095    none    none    A man is playing a guitar.  A man is playing a trumpet.10  main-captions   MSRvid  2012test    0096    none    none    A man is playing a guitar.  A man is playing a trumpet....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>test.tsv数据样式说明:<ul><li>test.tsv中的数据内容共分为9列, 含义与train.tsv前9列相同.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>STS-B数据集的任务类型:<ul><li>句子对多分类任务/句子对回归任务</li><li>评估指标为: Pearson-Spearman Corr</li></ul></li></ul></blockquote><hr><h4 id="QQP数据集文件样式"><a href="#QQP数据集文件样式" class="headerlink" title="QQP数据集文件样式"></a>QQP数据集文件样式</h4><pre class="line-numbers language-none"><code class="language-none">- QQP/        - dev.tsv        - original/        - test.tsv        - train.tsv<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>文件样式说明:<ul><li>在使用中常用到的文件是train.tsv, dev.tsv, test.tsv, 分别代表训练集, 验证集和测试集. 其中train.tsv与dev.tsv数据样式相同, 都是带有标签的数据, 其中test.tsv是不带有标签的数据.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">id  qid1    qid2    question1   question2   is_duplicate133273  213221  213222  How is the life of a math student? Could you describe your own experiences?Which level of prepration is enough for the exam jlpt5?  0402555  536040  536041  How do I control my horny emotions? How do you control your horniness?  1360472  364011  490273  What causes stool color to change to yellow?    What can cause stool to come out as little balls?   0150662  155721  7256    What can one do after MBBS? What do i do after my MBBS ?    1183004  279958  279959  Where can I find a power outlet for my laptop at Melbourne Airport? Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney?   0119056  193387  193388  How not to feel guilty since I am Muslim and I'm conscious we won't have sex together?  I don't beleive I am bulimic, but I force throw up atleast once a day after I eat something and feel guilty. Should I tell somebody, and if so who? 0356863  422862  96457   How is air traffic controlled?  How do you become an air traffic controller?0106969  147570  787 What is the best self help book you have read? Why? How did it change your life?    What are the top self help books I should read? 1...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>train.tsv数据样式说明:<ul><li>train.tsv中的数据内容共分为6列, 第一列代表文本数据索引; 第二列和第三列数据分别代表问题1和问题2的id; 第四列和第五列代表需要进行’是否重复’判定的句子对; 第六列代表上述问题是/不是重复性问题的标签, 0代表不重复, 1代表重复.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>test.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">id  question1   question20   Would the idea of Trump and Putin in bed together scare you, given the geopolitical implications?   Do you think that if Donald Trump were elected President, he would be able to restore relations with Putin and Russia as he said he could, based on the rocky relationship Putin had with Obama and Bush?1   What are the top ten Consumer-to-Consumer E-commerce online?    What are the top ten Consumer-to-Business E-commerce online?2   Why don't people simply 'Google' instead of asking questions on Quora?  Why do people ask Quora questions instead of just searching google?3   Is it safe to invest in social trade biz?   Is social trade geniune?4   If the universe is expanding then does matter also expand?  If universe and space is expanding? Does that mean anything that occupies space is also expanding?5   What is the plural of hypothesis?   What is the plural of thesis?6   What is the application form you need for launching a company?  What is the application form you need for launching a company in Austria?7   What is Big Theta? When should I use Big Theta as opposed to big O? Is O(Log n) close to O(n) or O(1)?8   What are the health implications of accidentally eating a small quantity of aluminium foil?What are the implications of not eating vegetables?...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>test.tsv数据样式说明:<ul><li>test.tsv中的数据内容共分为3列, 第一列数据代表每条文本数据的索引; 第二列和第三列数据代表用于测试的问题句子对.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>QQP数据集的任务类型:<ul><li>句子对二分类任务</li><li>评估指标为: ACC/F1</li></ul></li></ul></blockquote><hr><h4 id="MNLI-SNLI-数据集文件样式"><a href="#MNLI-SNLI-数据集文件样式" class="headerlink" title="(MNLI/SNLI)数据集文件样式"></a>(MNLI/SNLI)数据集文件样式</h4><pre class="line-numbers language-none"><code class="language-none">- (MNLI/SNLI)/    - dev_matched.tsv    - dev_mismatched.tsv    - original/    - test_matched.tsv    - test_mismatched.tsv    - train.tsv<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>文件样式说明:<ul><li>在使用中常用到的文件是train.tsv, dev_matched.tsv, dev_mismatched.tsv, test_matched.tsv, test_mismatched.tsv分别代表训练集, 与训练集一同采集的验证集, 与训练集不是一同采集验证集, 与训练集一同采集的测试集, 与训练集不是一同采集测试集. 其中train.tsv与dev_matched.tsv和dev_mismatched.tsv数据样式相同, 都是带有标签的数据, 其中test_matched.tsv与test_mismatched.tsv数据样式相同, 都是不带有标签的数据.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   promptID    pairID  genre   sentence1_binary_parse  sentence2_binary_parse  sentence1_parse sentence2_parse sentence1   sentence2   label1  gold_label0   31193   31193n  government  ( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )  ( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )   (ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))  (ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))   Conceptually cream skimming has two basic dimensions - product and geography.   Product and geography are what make cream skimming work.    neutral neutral1   101457  101457e telephone   ( you ( ( know ( during ( ( ( the season ) and ) ( i guess ) ) ) ) ( at ( at ( ( your level ) ( uh ( you ( ( ( lose them ) ( to ( the ( next level ) ) ) ) ( if ( ( if ( they ( decide ( to ( recall ( the ( the ( parent team ) ) ) ) ) ) ) ) ( ( the Braves ) ( decide ( to ( call ( to ( ( recall ( a guy ) ) ( from ( ( triple A ) ( ( ( then ( ( a ( double ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) and ) ( ( a ( single ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ( You ( ( ( ( lose ( the things ) ) ( to ( the ( following level ) ) ) ) ( if ( ( the people ) recall ) ) ) . ) )   (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN during) (NP (NP (DT the) (NN season)) (CC and) (NP (FW i) (FW guess)))) (PP (IN at) (IN at) (NP (NP (PRP$ your) (NN level)) (SBAR (S (INTJ (UH uh)) (NP (PRP you)) (VP (VBP lose) (NP (PRP them)) (PP (TO to) (NP (DT the) (JJ next) (NN level))) (SBAR (IN if) (S (SBAR (IN if) (S (NP (PRP they)) (VP (VBP decide) (S (VP (TO to) (VP (VB recall) (NP (DT the) (DT the) (NN parent) (NN team)))))))) (NP (DT the) (NNPS Braves)) (VP (VBP decide) (S (VP (TO to) (VP (VB call) (S (VP (TO to) (VP (VB recall) (NP (DT a) (NN guy)) (PP (IN from) (NP (NP (RB triple) (DT A)) (SBAR (S (S (ADVP (RB then)) (NP (DT a) (JJ double) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))) (CC and) (S (NP (DT a) (JJ single) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him)))))))))))))))))))))))))))) (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT the) (NNS things)) (PP (TO to) (NP (DT the) (JJ following) (NN level))) (SBAR (IN if) (S (NP (DT the) (NNS people)) (VP (VBP recall))))) (. .))) you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him You lose the things to the following level if the people recall.    entailment  entailment2   134793  134793e fiction ( ( One ( of ( our number ) ) ) ( ( will ( ( ( carry out ) ( your instructions ) ) minutely ) ) . ) )   ( ( ( A member ) ( of ( my team ) ) ) ( ( will ( ( execute ( your orders ) ) ( with ( immense precision ) ) ) ) . ) )   (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PRP$ our) (NN number)))) (VP (MD will) (VP (VB carry) (PRT (RP out)) (NP (PRP$ your) (NNS instructions)) (ADVP (RB minutely)))) (. .))) (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN of) (NP (PRP$ my) (NN team)))) (VP (MD will) (VP (VB execute) (NP (PRP$ your) (NNS orders)) (PP (IN with) (NP (JJ immense) (NN precision))))) (. .)))  One of our number will carry out your instructions minutely.    A member of my team will execute your orders with immense precision.    entailment  entailment3   37397   37397e  fiction ( ( How ( ( ( do you ) know ) ? ) ) ( ( All this ) ( ( ( is ( their information ) ) again ) . ) ) ) ( ( This information ) ( ( belongs ( to them ) ) . ) )  (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do) (NP (PRP you)) (VP (VB know))) (. ?)) (NP (PDT All) (DT this)) (VP (VBZ is) (NP (PRP$ their) (NN information)) (ADVP (RB again))) (. .)))   (ROOT (S (NP (DT This) (NN information)) (VP (VBZ belongs) (PP (TO to) (NP (PRP them)))) (. .)))    How do you know? All this is their information again.   This information belongs to them.   entailment  entailment...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>train.tsv数据样式说明:<ul><li>train.tsv中的数据内容共分为12列, 第一列代表文本数据索引; 第二列和第三列数据分别代表句子对的不同类型id; 第四列代表句子对的来源; 第五列和第六列代表具有句法结构分析的句子对表示; 第七列和第八列代表具有句法结构和词性标注的句子对表示, 第九列和第十列代表原始的句子对, 第十一和第十二列代表不同标准的标注方法产生的标签, 在这里，他们始终相同, 一共有三种类型的标签, neutral代表两个句子既不矛盾也不蕴含, entailment代表两个句子具有蕴含关系, contradiction代表两个句子观点矛盾.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>test_matched.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   promptID    pairID  genre   sentence1_binary_parse  sentence2_binary_parse  sentence1_parse sentence2_parse sentence1   sentence20   31493   31493   travel  ( ( ( ( ( ( ( ( Hierbas , ) ( ans seco ) ) , ) ( ans dulce ) ) , ) and ) frigola ) ( ( ( are just ) ( ( a ( few names ) ) ( worth ( ( keeping ( a look-out ) ) for ) ) ) ) . ) )    ( Hierbas ( ( is ( ( a name ) ( worth ( ( looking out ) for ) ) ) ) . ) )   (ROOT (S (NP (NP (NNS Hierbas)) (, ,) (NP (NN ans) (NN seco)) (, ,) (NP (NN ans) (NN dulce)) (, ,) (CC and) (NP (NN frigola))) (VP (VBP are) (ADVP (RB just)) (NP (NP (DT a) (JJ few) (NNS names)) (PP (JJ worth) (S (VP (VBG keeping) (NP (DT a) (NN look-out)) (PP (IN for))))))) (. .))) (ROOT (S (NP (NNS Hierbas)) (VP (VBZ is) (NP (NP (DT a) (NN name)) (PP (JJ worth) (S (VP (VBG looking) (PRT (RP out)) (PP (IN for))))))) (. .)))    Hierbas, ans seco, ans dulce, and frigola are just a few names worth keeping a look-out for.    Hierbas is a name worth looking out for.1   92164   92164   government  ( ( ( The extent ) ( of ( the ( behavioral effects ) ) ) ) ( ( would ( ( depend ( in ( part ( on ( ( the structure ) ( of ( ( ( the ( individual ( account program ) ) ) and ) ( any limits ) ) ) ) ) ) ) ) ( on ( accessing ( the funds ) ) ) ) ) . ) )    ( ( Many people ) ( ( would ( be ( very ( unhappy ( to ( ( loose control ) ( over ( their ( own money ) ) ) ) ) ) ) ) ) . ) )   (ROOT (S (NP (NP (DT The) (NN extent)) (PP (IN of) (NP (DT the) (JJ behavioral) (NNS effects)))) (VP (MD would) (VP (VB depend) (PP (IN in) (NP (NP (NN part)) (PP (IN on) (NP (NP (DT the) (NN structure)) (PP (IN of) (NP (NP (DT the) (JJ individual) (NN account) (NN program)) (CC and) (NP (DT any) (NNS limits)))))))) (PP (IN on) (S (VP (VBG accessing) (NP (DT the) (NNS funds))))))) (. .))) (ROOT (S (NP (JJ Many) (NNS people)) (VP (MD would) (VP (VB be) (ADJP (RB very) (JJ unhappy) (PP (TO to) (NP (NP (JJ loose) (NN control)) (PP (IN over) (NP (PRP$ their) (JJ own) (NN money)))))))) (. .))) The extent of the behavioral effects would depend in part on the structure of the individual account program and any limits on accessing the funds. Many people would be very unhappy to loose control over their own money.2   9662    9662    government  ( ( ( Timely access ) ( to information ) ) ( ( is ( in ( ( the ( best interests ) ) ( of ( ( ( both GAO ) and ) ( the agencies ) ) ) ) ) ) . ) )    ( It ( ( ( is ( in ( ( everyone 's ) ( best interest ) ) ) ) ( to ( ( have access ) ( to ( information ( in ( a ( timely manner ) ) ) ) ) ) ) ) . ) )   (ROOT (S (NP (NP (JJ Timely) (NN access)) (PP (TO to) (NP (NN information)))) (VP (VBZ is) (PP (IN in) (NP (NP (DT the) (JJS best) (NNS interests)) (PP (IN of) (NP (NP (DT both) (NNP GAO)) (CC and) (NP (DT the) (NNS agencies))))))) (. .))) (ROOT (S (NP (PRP It)) (VP (VBZ is) (PP (IN in) (NP (NP (NN everyone) (POS 's)) (JJS best) (NN interest))) (S (VP (TO to) (VP (VB have) (NP (NN access)) (PP (TO to) (NP (NP (NN information)) (PP (IN in) (NP (DT a) (JJ timely) (NN manner))))))))) (. .)))   Timely access to information is in the best interests of both GAO and the agencies. It is in everyone's best interest to have access to information in a timely manner.3   5991    5991    travel  ( ( Based ( in ( ( the ( Auvergnat ( spa town ) ) ) ( of Vichy ) ) ) ) ( , ( ( the ( French government ) ) ( often ( ( ( ( proved ( more zealous ) ) ( than ( its masters ) ) ) ( in ( ( ( suppressing ( civil liberties ) ) and ) ( ( drawing up ) ( anti-Jewish legislation ) ) ) ) ) . ) ) ) ) ) ( ( The ( French government ) ) ( ( passed ( ( anti-Jewish laws ) ( aimed ( at ( helping ( the Nazi ) ) ) ) ) ) . ) )   (ROOT (S (PP (VBN Based) (PP (IN in) (NP (NP (DT the) (NNP Auvergnat) (NN spa) (NN town)) (PP (IN of) (NP (NNP Vichy)))))) (, ,) (NP (DT the) (JJ French) (NN government)) (ADVP (RB often)) (VP (VBD proved) (NP (JJR more) (NNS zealous)) (PP (IN than) (NP (PRP$ its) (NNS masters))) (PP (IN in) (S (VP (VP (VBG suppressing) (NP (JJ civil) (NNS liberties))) (CC and) (VP (VBG drawing) (PRT (RP up)) (NP (JJ anti-Jewish) (NN legislation))))))) (. .))) (ROOT (S (NP (DT The) (JJ French) (NN government)) (VP (VBD passed) (NP (NP (JJ anti-Jewish) (NNS laws)) (VP (VBN aimed) (PP (IN at) (S (VP (VBG helping) (NP (DT the) (JJ Nazi)))))))) (. .))) Based in the Auvergnat spa town of Vichy, the French government often proved more zealous than its masters in suppressing civil liberties and drawing up anti-Jewish legislation.   The French government passed anti-Jewish laws aimed at helping the Nazi....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>test_matched.tsv数据样式说明:<ul><li>test_matched.tsv中的数据内容共分为10列, 与train.tsv的前10列含义相同.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>(MNLI/SNLI)数据集的任务类型:<ul><li>句子对多分类任务</li><li>评估指标为: ACC</li></ul></li></ul></blockquote><hr><h4 id="QNLI-RTE-WNLI-数据集文件样式"><a href="#QNLI-RTE-WNLI-数据集文件样式" class="headerlink" title="(QNLI/RTE/WNLI)数据集文件样式"></a>(QNLI/RTE/WNLI)数据集文件样式</h4><pre class="line-numbers language-none"><code class="language-none">* QNLI, RTE, WNLI三个数据集的样式基本相同.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">- (QNLI/RTE/WNLI)/        - dev.tsv        - test.tsv        - train.tsv<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>文件样式说明:<ul><li>在使用中常用到的文件是train.tsv, dev.tsv, test.tsv, 分别代表训练集, 验证集和测试集. 其中train.tsv与dev.tsv数据样式相同, 都是带有标签的数据, 其中test.tsv是不带有标签的数据.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>QNLI中的train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   question    sentence    label0   When did the third Digimon series begin?    Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese. not_entailment1   Which missile batteries often have individual launchers several kilometres from one another?    When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.    not_entailment2   What two things does Popper argue Tarski's theory involves in an evaluation of truth?   He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.   entailment3   What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians?    On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854.  entailment4   What famous palace is located in London?    London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0° longitude, and GMT).  not_entailment5   When is the term 'German dialects' used in regard to the German language?   When talking about the German language, the term German dialects is only used for the traditional regional varieties.   entailment6   What was the name of the island the English traded to the Dutch in return for New Amsterdam?    At the end of the Second Anglo-Dutch War, the English gained New Amsterdam (New York) in North America in exchange for Dutch control of Run, an Indonesian island.  entailment7   How were the Portuguese expelled from Myanmar?  From the 1720s onward, the kingdom was beset with repeated Meithei raids into Upper Myanmar and a nagging rebellion in Lan Na.  not_entailment8   What does the word 'customer' properly apply to?    The bill also required rotation of principal maintenance inspectors and stipulated that the word "customer" properly applies to the flying public, not those entities regulated by the FAA. entailment...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>RTE中的train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   sentence1   sentence2   label0   No Weapons of Mass Destruction Found in Iraq Yet.   Weapons of Mass Destruction Found in Iraq.  not_entailment1   A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.Pope Benedict XVI is the new leader of the Roman Catholic Church. entailment2   Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.  Herceptin can be used to treat breast cancer.   entailment3   Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment.   The previous name of Ho Chi Minh City was Saigon.entailment4   A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later.  Paul Stewart Hutchinson is accused of having stabbed a girl.    not_entailment5   Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, where he was released by police after being detained for 24 hours. Bakri was briefly detained, but was released.   entailment6   Nearly 4 million children who have at least one parent who entered the U.S. illegally were born in the United States and are U.S. citizens as a result, according to the study conducted by the Pew Hispanic Center. That's about three quarters of the estimated 5.5 million children of illegal immigrants inside the United States, according to the study. About 1.8 million children of undocumented immigrants live in poverty, the study found.  Three quarters of U.S. illegal immigrants have children.    not_entailment7   Like the United States, U.N. officials are also dismayed that Aristide killed a conference called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together.  Aristide had Prime Minister Robert Malval  murdered in Port-au-Prince.  not_entailment8   WASHINGTON --  A newly declassified narrative of the Bush administration's advice to the CIA on harsh interrogations shows that the small group of Justice Department lawyers who wrote memos authorizing controversial interrogation techniques were operating not on their own but with direction from top administration officials, including then-Vice President Dick Cheney and national security adviser Condoleezza Rice. At the same time, the narrative suggests that then-Defense Secretary Donald H. Rumsfeld and then-Secretary of State Colin Powell were largely left out of the decision-making process. Dick Cheney was the Vice President of Bush. entailment<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>WNLI中的train.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   sentence1   sentence2   label0   I stuck a pin through a carrot. When I pulled the pin out, it had a hole.   The carrot had a hole.  11   John couldn't see the stage with Billy in front of him because he is so short.  John is so short.   12   The police arrested all of the gang members. They were trying to stop the drug trade in the neighborhood.   The police were trying to stop the drug trade in the neighborhood.  13   Steve follows Fred's example in everything. He influences him hugely.   Steve influences him hugely.    04   When Tatyana reached the cabin, her mother was sleeping. She was careful not to disturb her, undressing and climbing back into her berth.   mother was careful not to disturb her, undressing and climbing back into her berth. 05   George got free tickets to the play, but he gave them to Eric, because he was particularly eager to see it. George was particularly eager to see it.    06   John was jogging through the park when he saw a man juggling watermelons. He was very impressive.   John was very impressive.   07   I couldn't put the pot on the shelf because it was too tall.    The pot was too tall.   18   We had hoped to place copies of our newsletter on all the chairs in the auditorium, but there were simply not enough of them.   There were simply not enough copies of the newsletter.  1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>(QNLI/RTE/WNLI)中的train.tsv数据样式说明:<ul><li>train.tsv中的数据内容共分为4列, 第一列代表文本数据索引; 第二列和第三列数据代表需要进行’是否蕴含’判定的句子对; 第四列数据代表两个句子是否具有蕴含关系, 0/not_entailment代表不是蕴含关系, 1/entailment代表蕴含关系.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>QNLI中的test.tsv数据样式:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">index   question    sentence0   What organization is devoted to Jihad against Israel?   For some decades prior to the First Palestine Intifada in 1987, the Muslim Brotherhood in Palestine took a "quiescent" stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel's "indulgence" to build up a network of mosques and charitable organizations.1   In what century was the Yarrow-Schlick-Tweedy balancing system used?    In the late 19th century, the Yarrow-Schlick-Tweedy balancing 'system' was used on some marine triple expansion engines.2   The largest brand of what store in the UK is located in Kingston Park?  Close to Newcastle, the largest indoor shopping centre in Europe, the MetroCentre, is located in Gateshead.3   What does the IPCC rely on for research?    In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included.4   What is the principle about relating spin and space variables?  Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.5   Which network broadcasted Super Bowl 50 in the U.S.?    CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game.6   What did the museum acquire from the Royal College of Science?  To link this to the rest of the museum, a new entrance building was constructed on the site of the former boiler house, the intended site of the Spiral, between 1978 and 1982.7   What is the name of the old north branch of the Rhine?  From Wijk bij Duurstede, the old north branch of the Rhine is called Kromme Rijn ("Bent Rhine") past Utrecht, first Leidse Rijn ("Rhine of Leiden") and then, Oude Rijn ("Old Rhine").8   What was one of Luther's most personal writings?    It remains in use today, along with Luther's hymns and his translation of the Bible....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>(RTE/WNLI)中的test.tsv数据样式:</li></ul><pre class="line-numbers language-none"><code class="language-none">index   sentence1   sentence20   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight.    Horses ran away when Maude and Dora came in sight.1   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight.    Horses ran away when the trains came in sight.2   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight.    Horses ran away when the puffs came in sight.3   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight.    Horses ran away when the roars came in sight.4   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight.    Horses ran away when the whistles came in sight.5   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight.    Horses ran away when the horses came in sight.6   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming.   Maude and Dora saw a train coming.7   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming.   The trains saw a train coming.8   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming.   The puffs saw a train coming....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>(QNLI/RTE/WNLI)中的test.tsv数据样式说明:<ul><li>test.tsv中的数据内容共分为3列, 第一列数据代表每条文本数据的索引; 第二列和第三列数据代表需要进行’是否蕴含’判定的句子对.</li></ul></li></ul></blockquote><hr><blockquote><ul><li>(QNLI/RTE/WNLI)数据集的任务类型:<ul><li>句子对二分类任务</li><li>评估指标为: ACC</li></ul></li></ul></blockquote><hr><h3 id="小节总结"><a href="#小节总结" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了GLUE数据集合的介绍:</p><ul><li>GLUE由纽约大学, 华盛顿大学, Google联合推出, 涵盖不同NLP任务类型, 截止至2020年1月其中包括11个子任务数据集, 成为衡量NLP研究发展的衡量标准.</li></ul><hr></li><li><p>GLUE数据集合包含以下数据集:</p><ul><li>CoLA 数据集</li><li>SST-2 数据集</li><li>MRPC 数据集</li><li>STS-B 数据集</li><li>QQP 数据集</li><li>MNLI 数据集</li><li>SNLI 数据集</li><li>QNLI 数据集</li><li>RTE 数据集</li><li>WNLI 数据集</li></ul></li></ul><hr><h2 id="2-3-NLP中的常用预训练模型"><a href="#2-3-NLP中的常用预训练模型" class="headerlink" title="2.3 NLP中的常用预训练模型"></a>2.3 NLP中的常用预训练模型</h2><hr><h3 id="学习目标-2"><a href="#学习目标-2" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解当下NLP中流行的预训练模型.</li><li>掌握如何加载和使用预训练模型.</li></ul><hr><h3 id="当下NLP中流行的预训练模型"><a href="#当下NLP中流行的预训练模型" class="headerlink" title="当下NLP中流行的预训练模型"></a>当下NLP中流行的预训练模型</h3><ul><li>BERT</li><li>GPT</li><li>GPT-2</li><li>Transformer-XL</li><li>XLNet</li><li>XLM</li><li>RoBERTa</li><li>DistilBERT</li><li>ALBERT</li><li>T5</li><li>XLM-RoBERTa</li></ul><hr><ul><li>BERT及其变体:<ul><li>bert-base-uncased: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在小写的英文文本上进行训练而得到.</li><li>bert-large-uncased: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共340M参数量, 在小写的英文文本上进行训练而得到.</li><li>bert-base-cased: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在不区分大小写的英文文本上进行训练而得到.</li><li>bert-large-cased: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共340M参数量, 在不区分大小写的英文文本上进行训练而得到.</li><li>bert-base-multilingual-uncased: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在小写的102种语言文本上进行训练而得到.</li><li>bert-large-multilingual-uncased: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共340M参数量, 在小写的102种语言文本上进行训练而得到.</li><li>bert-base-chinese: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在简体和繁体中文文本上进行训练而得到.</li></ul></li></ul><hr><ul><li>GPT:<ul><li>openai-gpt: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 由OpenAI在英文语料上进行训练而得到.</li></ul></li></ul><hr><ul><li>GPT-2及其变体:<ul><li>gpt2: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共117M参数量, 在OpenAI GPT-2英文语料上进行训练而得到.</li><li>gpt2-xl: 编码器具有48个隐层, 输出1600维张量, 25个自注意力头, 共1558M参数量, 在大型的OpenAI GPT-2英文语料上进行训练而得到.</li></ul></li></ul><hr><ul><li>Transformer-XL:<ul><li>transfo-xl-wt103: 编码器具有18个隐层, 输出1024维张量, 16个自注意力头, 共257M参数量, 在wikitext-103英文语料进行训练而得到.</li></ul></li></ul><hr><ul><li>XLNet及其变体:<ul><li>xlnet-base-cased: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共110M参数量, 在英文语料上进行训练而得到.</li><li>xlnet-large-cased: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共240参数量, 在英文语料上进行训练而得到.</li></ul></li></ul><hr><ul><li>XLM:<ul><li>xlm-mlm-en-2048: 编码器具有12个隐层, 输出2048维张量, 16个自注意力头, 在英文文本上进行训练而得到.</li></ul></li></ul><hr><ul><li>RoBERTa及其变体:<ul><li>roberta-base: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共125M参数量, 在英文文本上进行训练而得到.</li><li>roberta-large: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共355M参数量, 在英文文本上进行训练而得到.</li></ul></li></ul><hr><ul><li>DistilBERT及其变体:<ul><li>distilbert-base-uncased: 基于bert-base-uncased的蒸馏(压缩)模型, 编码器具有6个隐层, 输出768维张量, 12个自注意力头, 共66M参数量.</li><li>distilbert-base-multilingual-cased: 基于bert-base-multilingual-uncased的蒸馏(压缩)模型, 编码器具有6个隐层, 输出768维张量, 12个自注意力头, 共66M参数量.</li></ul></li></ul><hr><ul><li>ALBERT:<ul><li>albert-base-v1: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共125M参数量, 在英文文本上进行训练而得到.</li><li>albert-base-v2: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共125M参数量, 在英文文本上进行训练而得到, 相比v1使用了更多的数据量, 花费更长的训练时间.</li></ul></li></ul><hr><ul><li>T5及其变体:<ul><li>t5-small: 编码器具有6个隐层, 输出512维张量, 8个自注意力头, 共60M参数量, 在C4语料上进行训练而得到.</li><li>t5-base: 编码器具有12个隐层, 输出768维张量, 12个自注意力头, 共220M参数量, 在C4语料上进行训练而得到.</li><li>t5-large: 编码器具有24个隐层, 输出1024维张量, 16个自注意力头, 共770M参数量, 在C4语料上进行训练而得到.</li></ul></li></ul><hr><ul><li>XLM-RoBERTa及其变体:<ul><li>xlm-roberta-base: 编码器具有12个隐层, 输出768维张量, 8个自注意力头, 共125M参数量, 在2.5TB的100种语言文本上进行训练而得到.</li><li>xlm-roberta-large: 编码器具有24个隐层, 输出1027维张量, 16个自注意力头, 共355M参数量, 在2.5TB的100种语言文本上进行训练而得到.</li></ul></li></ul><hr><ul><li>预训练模型说明:<ul><li>所有上述预训练模型及其变体都是以transformer为基础，只是在模型结构如神经元连接方式，编码器隐层数，多头注意力的头数等发生改变，这些改变方式的大部分依据都是由在标准数据集上的表现而定，因此，对于我们使用者而言，不需要从理论上深度探究这些预训练模型的结构设计的优劣，只需要在自己处理的目标数据上，尽量遍历所有可用的模型对比得到最优效果即可.</li></ul></li></ul><hr><h3 id="小节总结-1"><a href="#小节总结-1" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li>当下NLP中流行的预训练模型:<ul><li>BERT</li><li>GPT</li><li>GPT-2</li><li>Transformer-XL</li><li>XLNet</li><li>XLM</li><li>RoBERTa</li><li>DistilBERT</li><li>ALBERT</li><li>T5</li><li>XLM-RoBERTa</li></ul></li></ul><hr><h2 id="2-4-加载和使用预训练模型"><a href="#2-4-加载和使用预训练模型" class="headerlink" title="2.4 加载和使用预训练模型"></a>2.4 加载和使用预训练模型</h2><hr><h3 id="学习目标-3"><a href="#学习目标-3" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解加载和使用预训练模型的工具.</li><li>掌握加载和使用预训练模型的过程.</li></ul><hr><h3 id="加载和使用预训练模型的工具"><a href="#加载和使用预训练模型的工具" class="headerlink" title="加载和使用预训练模型的工具"></a>加载和使用预训练模型的工具</h3><ul><li>在这里我们使用torch.hub工具进行模型的加载和使用.</li><li>这些预训练模型由世界先进的NLP研发团队huggingface提供.</li></ul><hr><h3 id="加载和使用预训练模型的步骤"><a href="#加载和使用预训练模型的步骤" class="headerlink" title="加载和使用预训练模型的步骤"></a>加载和使用预训练模型的步骤</h3><ul><li>第一步: 确定需要加载的预训练模型并安装依赖包.</li><li>第二步: 加载预训练模型的映射器tokenizer.</li><li>第三步: 加载带/不带头的预训练模型.</li><li>第四步: 使用模型获得输出结果.</li></ul><hr><h4 id="第一步-确定需要加载的预训练模型并安装依赖包"><a href="#第一步-确定需要加载的预训练模型并安装依赖包" class="headerlink" title="第一步: 确定需要加载的预训练模型并安装依赖包"></a>第一步: 确定需要加载的预训练模型并安装依赖包</h4><ul><li>能够加载哪些模型可以参考<a href="http://47.92.175.143:8007/2/#23-nlp">2.3 NLP中的常用预训练模型</a></li><li>这里假设我们处理的是中文文本任务, 需要加载的模型是BERT的中文模型: bert-base-chinese</li><li>在使用工具加载模型前需要安装必备的依赖包:</li></ul><pre class="line-numbers language-none"><code class="language-none">pip install tqdm boto3 requests regex sentencepiece sacremoses<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><h4 id="第二步-加载预训练模型的映射器tokenizer"><a href="#第二步-加载预训练模型的映射器tokenizer" class="headerlink" title="第二步: 加载预训练模型的映射器tokenizer"></a>第二步: 加载预训练模型的映射器tokenizer</h4><pre class="line-numbers language-none"><code class="language-none">import torch# 预训练模型来源source = 'huggingface/pytorch-transformers'# 选定加载模型的哪一部分, 这里是模型的映射器part = 'tokenizer'# 加载的预训练模型的名字model_name = 'bert-base-chinese'tokenizer = torch.hub.load(source, part, model_name)  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第三步-加载带-不带头的预训练模型"><a href="#第三步-加载带-不带头的预训练模型" class="headerlink" title="第三步: 加载带/不带头的预训练模型"></a>第三步: 加载带/不带头的预训练模型</h4><ul><li>加载预训练模型时我们可以选择带头或者不带头的模型</li><li>这里的’头’是指模型的任务输出层, 选择加载不带头的模型, 相当于使用模型对输入文本进行特征表示.</li><li>选择加载带头的模型时, 有三种类型的’头’可供选择, modelWithLMHead(语言模型头), modelForSequenceClassification(分类模型头), modelForQuestionAnswering(问答模型头)</li><li>不同类型的’头’, 可以使预训练模型输出指定的张量维度. 如使用’分类模型头’, 则输出尺寸为(1,2)的张量, 用于进行分类任务判定结果.</li></ul><pre class="line-numbers language-none"><code class="language-none"># 加载不带头的预训练模型part = 'model'model = torch.hub.load(source, part, model_name)# 加载带有语言模型头的预训练模型part = 'modelWithLMHead'lm_model = torch.hub.load(source, part, model_name)# 加载带有类模型头的预训练模型part = 'modelForSequenceClassification'classification_model = torch.hub.load(source, part, model_name)# 加载带有问答模型头的预训练模型part = 'modelForQuestionAnswering'qa_model = torch.hub.load(source, part, model_name)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第四步-使用模型获得输出结果"><a href="#第四步-使用模型获得输出结果" class="headerlink" title="第四步: 使用模型获得输出结果"></a>第四步: 使用模型获得输出结果</h4><ul><li>使用不带头的模型进行输出:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 输入的中文文本input_text = "人生该如何起头"# 使用tokenizer进行数值映射indexed_tokens = tokenizer.encode(input_text)# 打印映射后的结构print("indexed_tokens:", indexed_tokens)# 将映射结构转化为张量输送给不带头的预训练模型tokens_tensor = torch.tensor([indexed_tokens])# 使用不带头的预训练模型获得结果with torch.no_grad():    encoded_layers, _ = model(tokens_tensor)print("不带头的模型输出结果:", encoded_layers)print("不带头的模型输出结果的尺寸:", encoded_layers.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># tokenizer映射后的结果, 101和102是起止符, # 中间的每个数字对应"人生该如何起头"的每个字.indexed_tokens: [101, 782, 4495, 6421, 1963, 862, 6629, 1928, 102]不带头的模型输出结果: tensor([[[ 0.5421,  0.4526, -0.0179,  ...,  1.0447, -0.1140,  0.0068],         [-0.1343,  0.2785,  0.1602,  ..., -0.0345, -0.1646, -0.2186],         [ 0.9960, -0.5121, -0.6229,  ...,  1.4173,  0.5533, -0.2681],         ...,         [ 0.0115,  0.2150, -0.0163,  ...,  0.6445,  0.2452, -0.3749],         [ 0.8649,  0.4337, -0.1867,  ...,  0.7397, -0.2636,  0.2144],         [-0.6207,  0.1668,  0.1561,  ...,  1.1218, -0.0985, -0.0937]]])# 输出尺寸为1x9x768, 即每个字已经使用768维的向量进行了表示,# 我们可以基于此编码结果进行接下来的自定义操作, 如: 编写自己的微调网络进行最终输出.不带头的模型输出结果的尺寸: torch.Size([1, 9, 768])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>使用带有语言模型头的模型进行输出:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 使用带有语言模型头的预训练模型获得结果with torch.no_grad():    lm_output = lm_model(tokens_tensor)print("带语言模型头的模型输出结果:", lm_output)print("带语言模型头的模型输出结果的尺寸:", lm_output[0].shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">带语言模型头的模型输出结果: (tensor([[[ -7.9706,  -7.9119,  -7.9317,  ...,  -7.2174,  -7.0263,  -7.3746],         [ -8.2097,  -8.1810,  -8.0645,  ...,  -7.2349,  -6.9283,  -6.9856],         [-13.7458, -13.5978, -12.6076,  ...,  -7.6817,  -9.5642, -11.9928],         ...,         [ -9.0928,  -8.6857,  -8.4648,  ...,  -8.2368,  -7.5684, -10.2419],         [ -8.9458,  -8.5784,  -8.6325,  ...,  -7.0547,  -5.3288,  -7.8077],         [ -8.4154,  -8.5217,  -8.5379,  ...,  -6.7102,  -5.9782,  -7.6909]]]),)# 输出尺寸为1x9x21128, 即每个字已经使用21128维的向量进行了表示, # 同不带头的模型一样, 我们可以基于此编码结果进行接下来的自定义操作, 如: 编写自己的微调网络进行最终输出.带语言模型头的模型输出结果的尺寸: torch.Size([1, 9, 21128])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>使用带有分类模型头的模型进行输出:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 使用带有分类模型头的预训练模型获得结果with torch.no_grad():    classification_output = classification_model(tokens_tensor)print("带分类模型头的模型输出结果:", classification_output)print("带分类模型头的模型输出结果的尺寸:", classification_output[0].shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">带分类模型头的模型输出结果: (tensor([[-0.0649, -0.1593]]),)# 输出尺寸为1x2, 可直接用于文本二分问题的输出带分类模型头的模型输出结果的尺寸: torch.Size([1, 2])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>使用带有问答模型头的模型进行输出:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 使用带有问答模型头的模型进行输出时, 需要使输入的形式为句子对# 第一条句子是对客观事物的陈述# 第二条句子是针对第一条句子提出的问题# 问答模型最终将得到两个张量, # 每个张量中最大值对应索引的分别代表答案的在文本中的起始位置和终止位置.input_text1 = "我家的小狗是黑色的"input_text2 = "我家的小狗是什么颜色的呢?"# 映射两个句子indexed_tokens = tokenizer.encode(input_text1, input_text2)print("句子对的indexed_tokens:", indexed_tokens)# 输出结果: [101, 2769, 2157, 4638, 2207, 4318, 3221, 7946, 5682, 4638, 102, 2769, 2157, 4638, 2207, 4318, 3221, 784, 720, 7582, 5682, 4638, 1450, 136, 102]# 用0，1来区分第一条和第二条句子segments_ids = [0]*11 + [1]*14# 转化张量形式segments_tensors = torch.tensor([segments_ids])tokens_tensor = torch.tensor([indexed_tokens])# 使用带有问答模型头的预训练模型获得结果with torch.no_grad():    start_logits, end_logits = qa_model(tokens_tensor, token_type_ids=segments_tensors)print("带问答模型头的模型输出结果:", (start_logits, end_logits))print("带问答模型头的模型输出结果的尺寸:", (start_logits.shape, end_logits.shape))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">句子对的indexed_tokens: [101, 2769, 2157, 4638, 2207, 4318, 3221, 7946, 5682, 4638, 102, 2769, 2157, 4638, 2207, 4318, 3221, 784, 720, 7582, 5682, 4638, 1450, 136, 102]带问答模型头的模型输出结果: (tensor([[ 0.2574, -0.0293, -0.8337, -0.5135, -0.3645, -0.2216, -0.1625, -0.2768,         -0.8368, -0.2581,  0.0131, -0.1736, -0.5908, -0.4104, -0.2155, -0.0307,         -0.1639, -0.2691, -0.4640, -0.1696, -0.4943, -0.0976, -0.6693,  0.2426,          0.0131]]), tensor([[-0.3788, -0.2393, -0.5264, -0.4911, -0.7277, -0.5425, -0.6280, -0.9800,         -0.6109, -0.2379, -0.0042, -0.2309, -0.4894, -0.5438, -0.6717, -0.5371,         -0.1701,  0.0826,  0.1411, -0.1180, -0.4732, -0.1541,  0.2543,  0.2163,         -0.0042]]))# 输出为两个形状1x25的张量, 他们是两条句子合并长度的概率分布,# 第一个张量中最大值所在的索引代表答案出现的起始索引, # 第二个张量中最大值所在的索引代表答案出现的终止索引.带问答模型头的模型输出结果的尺寸: (torch.Size([1, 25]), torch.Size([1, 25]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="小节总结-2"><a href="#小节总结-2" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>加载和使用预训练模型的工具:</p><ul><li>在这里我们使用torch.hub工具进行模型的加载和使用. 这些预训练模型由世界先进的NLP研发团队huggingface提供.</li></ul><hr></li><li><p>加载和使用预训练模型的步骤:</p><ul><li>第一步: 确定需要加载的预训练模型并安装依赖包.</li><li>第二步: 加载预训练模型的映射器tokenizer.</li><li>第三步: 加载带/不带头的预训练模型.</li><li>第四步: 使用模型获得输出结果.</li></ul></li></ul><hr><h2 id="2-5-迁移学习实践"><a href="#2-5-迁移学习实践" class="headerlink" title="2.5 迁移学习实践"></a>2.5 迁移学习实践</h2><hr><h3 id="学习目标-4"><a href="#学习目标-4" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解并掌握指定任务类型的微调脚本使用方法.</li><li>了解并掌握通过微调脚本微调后模型的使用方法.</li><li>掌握通过微调方式进行迁移学习的两种类型实现过程.</li></ul><hr><ul><li>指定任务类型的微调脚本:<ul><li>huggingface研究机构向我们提供了针对GLUE数据集合任务类型的微调脚本, 这些微调脚本的核心都是微调模型的最后一个全连接层.</li><li>通过简单的参数配置来指定GLUE中存在任务类型(如: CoLA对应文本二分类, MRPC对应句子对文本二分类, STS-B对应句子对文本多分类), 以及指定需要微调的预训练模型.</li></ul></li></ul><hr><h3 id="指定任务类型的微调脚本使用步骤"><a href="#指定任务类型的微调脚本使用步骤" class="headerlink" title="指定任务类型的微调脚本使用步骤"></a>指定任务类型的微调脚本使用步骤</h3><ul><li>第一步: 下载微调脚本文件</li><li>第二步: 配置微调脚本参数</li><li>第三步: 运行并检验效果</li></ul><hr><h4 id="第一步-下载微调脚本文件"><a href="#第一步-下载微调脚本文件" class="headerlink" title="第一步: 下载微调脚本文件"></a>第一步: 下载微调脚本文件</h4><pre class="line-numbers language-none"><code class="language-none"># 克隆huggingface的transfomers文件git clone https://github.com/huggingface/transformers.git# 进行transformers文件夹cd transformers# 安装python的transformer工具包, 因为微调脚本是py文件.pip install .# 当前的版本可能跟我们教学的版本并不相同，你还需要执行：pip install transformers==2.3.0# 进入微调脚本所在路径并查看cd examplesls# 其中run_glue.py就是针对GLUE数据集合任务类型的微调脚本<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>注意：<ul><li>对于run_glue.py，由于版本变更导致，请通过该地址<a href="http://git.itcast.cn/Stephen/AI-key-file/blob/master/run_glue.py%E5%A4%8D%E5%88%B6%E9%87%8C%E9%9D%A2%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%8C%E8%A6%86%E7%9B%96%E5%8E%9F%E6%9C%89%E5%86%85%E5%AE%B9%E3%80%82">http://git.itcast.cn/Stephen/AI-key-file/blob/master/run_glue.py复制里面的代码，覆盖原有内容。</a></li></ul></li></ul><hr><h4 id="第二步-配置微调脚本参数"><a href="#第二步-配置微调脚本参数" class="headerlink" title="第二步: 配置微调脚本参数"></a>第二步: 配置微调脚本参数</h4><ul><li>在run_glue.py同级目录下创建run_glue.sh文件, 写入内容如下:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 定义DATA_DIR: 微调数据所在路径, 这里我们使用glue_data中的数据作为微调数据export DATA_DIR="../../glue_data"# 定义SAVE_DIR: 模型的保存路径, 我们将模型保存在当前目录的bert_finetuning_test文件中export SAVE_DIR="./bert_finetuning_test/"# 使用python运行微调脚本# --model_type: 选择需要微调的模型类型, 这里可以选择BERT, XLNET, XLM, roBERTa, distilBERT, ALBERT# --model_name_or_path: 选择具体的模型或者变体, 这里是在英文语料上微调, 因此选择bert-base-uncased# --task_name: 它将代表对应的任务类型, 如MRPC代表句子对二分类任务# --do_train: 使用微调脚本进行训练# --do_eval: 使用微调脚本进行验证# --data_dir: 训练集及其验证集所在路径, 将自动寻找该路径下的train.tsv和dev.tsv作为训练集和验证集# --max_seq_length: 输入句子的最大长度, 超过则截断, 不足则补齐# --learning_rate: 学习率# --num_train_epochs: 训练轮数# --output_dir $SAVE_DIR: 训练后的模型保存路径# --overwrite_output_dir: 再次训练时将清空之前的保存路径内容重新写入python run_glue.py \  --model_type BERT \  --model_name_or_path bert-base-uncased \  --task_name MRPC \  --do_train \  --do_eval \  --data_dir $DATA_DIR/MRPC/ \  --max_seq_length 128 \  --learning_rate 2e-5 \  --num_train_epochs 1.0 \  --output_dir $SAVE_DIR \  --overwrite_output_dir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第三步-运行并检验效果"><a href="#第三步-运行并检验效果" class="headerlink" title="第三步: 运行并检验效果"></a>第三步: 运行并检验效果</h4><pre class="line-numbers language-none"><code class="language-none"># 使用sh命令运行sh run_glue.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 最终打印模型的验证结果:01/05/2020 23:59:53 - INFO - __main__ -   Saving features into cached file ../../glue_data/MRPC/cached_dev_bert-base-uncased_128_mrpc01/05/2020 23:59:53 - INFO - __main__ -   ***** Running evaluation  *****01/05/2020 23:59:53 - INFO - __main__ -     Num examples = 40801/05/2020 23:59:53 - INFO - __main__ -     Batch size = 8Evaluating: 100%|█| 51/51 [00:23&lt;00:00,  2.20it/s]01/06/2020 00:00:16 - INFO - __main__ -   ***** Eval results  *****01/06/2020 00:00:16 - INFO - __main__ -     acc = 0.767156862745098101/06/2020 00:00:16 - INFO - __main__ -     acc_and_f1 = 0.807334450634186301/06/2020 00:00:16 - INFO - __main__ -     f1 = 0.8475120385232745<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>查看$SAVE_DIR的文件内容:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">added_tokens.json  checkpoint-450  checkpoint-400  checkpoint-350  checkpoint-200  checkpoint-300  checkpoint-250  checkpoint-200  checkpoint-150  checkpoint-100  checkpoint-50     pytorch_model.bin        training_args.binconfig.json       special_tokens_map.json  vocab.txteval_results.txt  tokenizer_config.json<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>文件解释:<ul><li>pytorch_model.bin代表模型参数，可以使用torch.load加载查看；</li><li>traning_args.bin代表模型训练时的超参，如batch_size，epoch等，仍可使用torch.load查看；</li><li>config.json是模型配置文件，如多头注意力的头数，编码器的层数等，代表典型的模型结构，如bert，xlnet，一般不更改；</li><li>added_token.json记录在训练时通过代码添加的自定义token对应的数值，即在代码中使用add_token方法添加的自定义词汇；</li><li>special_token_map.json当添加的token具有特殊含义时，如分隔符，该文件存储特殊字符的及其对应的含义，使文本中出现的特殊字符先映射成其含义，之后特殊字符的含义仍然使用add_token方法映射。</li><li>checkpoint: 若干步骤保存的模型参数文件(也叫检测点文件)。</li></ul></li></ul><hr><h3 id="通过微调脚本微调后模型的使用步骤"><a href="#通过微调脚本微调后模型的使用步骤" class="headerlink" title="通过微调脚本微调后模型的使用步骤"></a>通过微调脚本微调后模型的使用步骤</h3><ul><li>第一步: 在<a href="https://huggingface.co/join%E4%B8%8A%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B8%90%E6%88%B7">https://huggingface.co/join上创建一个帐户</a></li><li>第二步: 在服务器终端使用transformers-cli登陆</li><li>第三步: 使用transformers-cli上传模型并查看</li><li>第四步: 使用pytorch.hub加载模型进行使用</li></ul><hr><h4 id="第一步-在https-huggingface-co-join上创建一个帐户"><a href="#第一步-在https-huggingface-co-join上创建一个帐户" class="headerlink" title="第一步: 在https://huggingface.co/join上创建一个帐户"></a>第一步: 在<a href="https://huggingface.co/join%E4%B8%8A%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B8%90%E6%88%B7">https://huggingface.co/join上创建一个帐户</a></h4><pre class="line-numbers language-none"><code class="language-none"># 如果由于网络原因无法访问, 我们已经为你提供了默认账户username: ItcastAIpassword: ItcastAI<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><img src="/2021/04/23/fasttext-qian-yi-xue-xi/huggingface.png" alt="avatar"></p><hr><h4 id="第二步-在服务器终端使用transformers-cli登陆"><a href="#第二步-在服务器终端使用transformers-cli登陆" class="headerlink" title="第二步: 在服务器终端使用transformers-cli登陆"></a>第二步: 在服务器终端使用transformers-cli登陆</h4><pre class="line-numbers language-none"><code class="language-none"># 在微调模型的服务器上登陆# 使用刚刚注册的用户名和密码# 默认username: ItcastAI# 默认password: ItcastAI$ transformers-cli login<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第三步-使用transformers-cli上传模型并查看"><a href="#第三步-使用transformers-cli上传模型并查看" class="headerlink" title="第三步: 使用transformers-cli上传模型并查看"></a>第三步: 使用transformers-cli上传模型并查看</h4><pre class="line-numbers language-none"><code class="language-none"># 使用transformers-cli upload命令上传模型# 选择正确的微调模型路径$ transformers-cli upload ./bert_finetuning_test/# 查看上传结果$ transformers-cli lsFilename                                              LastModified             ETag                               Size----------------------------------------------------- ------------------------ ---------------------------------- ---------bert_finetuning_test/added_tokens.json                2020-01-05T17:39:57.000Z "99914b932bd37a50b983c5e7c90ae93b"         2bert_finetuning_test/checkpoint-400/config.json       2020-01-05T17:26:49.000Z "74d53ea41e5acb6d60496bc195d82a42"       684bert_finetuning_test/checkpoint-400/training_args.bin 2020-01-05T17:26:47.000Z "b3273519c2b2b1cb2349937279880f50"      1207bert_finetuning_test/checkpoint-450/config.json       2020-01-05T17:15:42.000Z "74d53ea41e5acb6d60496bc195d82a42"       684bert_finetuning_test/checkpoint-450/pytorch_model.bin 2020-01-05T17:15:58.000Z "077cc0289c90b90d6b662cce104fe4ef" 437982584bert_finetuning_test/checkpoint-450/training_args.bin 2020-01-05T17:15:40.000Z "b3273519c2b2b1cb2349937279880f50"      1207bert_finetuning_test/config.json                      2020-01-05T17:28:50.000Z "74d53ea41e5acb6d60496bc195d82a42"       684bert_finetuning_test/eval_results.txt                 2020-01-05T17:28:56.000Z "67d2d49a96afc4308d33bfcddda8a7c5"        81bert_finetuning_test/pytorch_model.bin                2020-01-05T17:28:59.000Z "d46a8ccfb8f5ba9ecee70cef8306679e" 437982584bert_finetuning_test/special_tokens_map.json          2020-01-05T17:28:54.000Z "8b3fb1023167bb4ab9d70708eb05f6ec"       112bert_finetuning_test/tokenizer_config.json            2020-01-05T17:28:52.000Z "0d7f03e00ecb582be52818743b50e6af"        59bert_finetuning_test/training_args.bin                2020-01-05T17:28:48.000Z "b3273519c2b2b1cb2349937279880f50"      1207bert_finetuning_test/vocab.txt                        2020-01-05T17:39:55.000Z "64800d5d8528ce344256daf115d4965e"    231508<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="第四步-使用pytorch-hub加载模型进行使用-更多信息请参考2-4-加载和使用预训练模型"><a href="#第四步-使用pytorch-hub加载模型进行使用-更多信息请参考2-4-加载和使用预训练模型" class="headerlink" title="第四步: 使用pytorch.hub加载模型进行使用, 更多信息请参考2.4 加载和使用预训练模型"></a>第四步: 使用pytorch.hub加载模型进行使用, 更多信息请参考<a href="http://47.92.175.143:8007/2/#24">2.4 加载和使用预训练模型</a></h4><pre class="line-numbers language-none"><code class="language-none"># 若之前使用过huggingface的transformers, 请清除~/.cacheimport torch# 如： ItcastAI/bert_finetuning_testsource = 'huggingface/pytorch-transformers'# 选定加载模型的哪一部分, 这里是模型的映射器part = 'tokenizer'############################################## 加载的预训练模型的名字# 使用自己的模型名字"username/model_name"# 如：'ItcastAI/bert_finetuning_test'model_name = 'ItcastAI/bert_finetuning_test'#############################################tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', model_name)model =  torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', model_name)index = tokenizer.encode("Talk is cheap", "Please show me your code!")# 102是bert模型中的间隔(结束)符号的数值映射mark = 102# 找到第一个102的索引, 即句子对的间隔符号k = index.index(mark)# 句子对分割id列表, 由0，1组成, 0的位置代表第一个句子, 1的位置代表第二个句子segments_ids = [0]*(k + 1) + [1]*(len(index) - k - 1)# 转化为tensortokens_tensor = torch.tensor([index])segments_tensors = torch.tensor([segments_ids])# 使用评估模式with torch.no_grad():    # 使用模型预测获得结果    result = model(tokens_tensor, token_type_ids=segments_tensors)    # 打印预测结果以及张量尺寸    print(result)    print(result[0].shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">(tensor([[-0.0181,  0.0263]]),)torch.Size([1, 2])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><h3 id="通过微调方式进行迁移学习的两种类型"><a href="#通过微调方式进行迁移学习的两种类型" class="headerlink" title="通过微调方式进行迁移学习的两种类型"></a>通过微调方式进行迁移学习的两种类型</h3><ul><li>类型一: 使用指定任务类型的微调脚本微调预训练模型, 后接带有输出头的预定义网络输出结果.</li><li>类型二: 直接加载预训练模型进行输入文本的特征表示, 后接自定义网络进行微调输出结果.</li></ul><hr><ul><li>说明: 所有类型的实战演示, 都将针对中文文本进行.</li></ul><hr><h4 id="类型一实战演示"><a href="#类型一实战演示" class="headerlink" title="类型一实战演示"></a>类型一实战演示</h4><ul><li>使用文本二分类的任务类型SST-2的微调脚本微调中文预训练模型, 后接带有分类输出头的预定义网络输出结果. 目标是判断句子的情感倾向.</li><li>准备中文酒店评论的情感分析语料, 语料样式与SST-2数据集相同, 标签0代表差评, 标签1好评.</li><li>语料存放在与glue_data/同级目录cn_data/下, 其中的SST-2目录包含train.tsv和dev.tsv</li></ul><hr><ul><li>train.tsv</li></ul><pre class="line-numbers language-none"><code class="language-none">sentence    label早餐不好,服务不到位,晚餐无西餐,早餐晚餐相同,房间条件不好,餐厅不分吸烟区.房间不分有无烟房.    0去的时候 ,酒店大厅和餐厅在装修,感觉大厅有点挤.由于餐厅装修本来该享受的早饭,也没有享受(他们是8点开始每个房间送,但是我时间来不及了)不过前台服务员态度好!    1有很长时间没有在西藏大厦住了，以前去北京在这里住的较多。这次住进来发现换了液晶电视，但网络不是很好，他们自己说是收费的原因造成的。其它还好。  1非常好的地理位置，住的是豪华海景房，打开窗户就可以看见栈桥和海景。记得很早以前也住过，现在重新装修了。总的来说比较满意，以后还会住   1交通很方便，房间小了一点，但是干净整洁，很有香港的特色，性价比较高，推荐一下哦 1酒店的装修比较陈旧，房间的隔音，主要是卫生间的隔音非常差，只能算是一般的    0酒店有点旧，房间比较小，但酒店的位子不错，就在海边，可以直接去游泳。8楼的海景打开窗户就是海。如果想住在热闹的地带，这里不是一个很好的选择，不过威海城市真的比较小，打车还是相当便宜的。晚上酒店门口出租车比较少。   1位置很好，走路到文庙、清凉寺5分钟都用不了，周边公交车很多很方便，就是出租车不太爱去（老城区路窄爱堵车），因为是老宾馆所以设施要陈旧些，    1酒店设备一般，套房里卧室的不能上网，要到客厅去。    0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>dev.tsv</li></ul><pre class="line-numbers language-none"><code class="language-none">sentence    label房间里有电脑，虽然房间的条件略显简陋，但环境、服务还有饭菜都还是很不错的。如果下次去无锡，我还是会选择这里的。 1我们是5月1日通过携程网入住的，条件是太差了，根本达不到四星级的标准，所有的东西都很陈旧，卫生间水龙头用完竟关不上，浴缸的漆面都掉了，估计是十年前的四星级吧，总之下次是不会入住了。  0离火车站很近很方便。住在东楼标间，相比较在九江住的另一家酒店，房间比较大。卫生间设施略旧。服务还好。10元中式早餐也不错，很丰富，居然还有青菜肉片汤。 1坐落在香港的老城区，可以体验香港居民生活，门口交通很方便，如果时间不紧，坐叮当车很好呀！周围有很多小餐馆，早餐就在中远后面的南北嚼吃的，东西很不错。我们定的大床房，挺安静的，总体来说不错。前台结账没有银联！ 1酒店前台服务差，对待客人不热情。号称携程没有预定。感觉是客人在求他们，我们一定得住。这样的宾馆下次不会入住！  0价格确实比较高，而且还没有早餐提供。  1是一家很实惠的酒店，交通方便，房间也宽敞，晚上没有电话骚扰，住了两次，有一次住５０１房间，洗澡间排水不畅通，也许是个别问题．服务质量很好，刚入住时没有调好宽带，服务员很快就帮忙解决了．    1位置非常好，就在西街的街口，但是却闹中取静，环境很清新优雅。  1房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错. 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>在run_glue.py同级目录下创建run_cn.sh文件, 写入内容如下:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 定义DATA_DIR: 微调数据所在路径export DATA_DIR="../../cn_data"# 定义SAVE_DIR: 模型的保存路径, 我们将模型保存在当前目录的bert_finetuning文件中export SAVE_DIR="./bert_cn_finetuning/"# 使用python运行微调脚本# --model_type: 选择BERT# --model_name_or_path: 选择bert-base-chinese# --task_name: 句子二分类任务SST-2# --do_train: 使用微调脚本进行训练# --do_eval: 使用微调脚本进行验证# --data_dir: "./cn_data/SST-2/", 将自动寻找该路径下的train.tsv和dev.tsv作为训练集和验证集# --max_seq_length: 128，输入句子的最大长度# --output_dir $SAVE_DIR: "./bert_finetuning/", 训练后的模型保存路径python run_glue.py \  --model_type BERT \  --model_name_or_path bert-base-chinese \  --task_name SST-2 \  --do_train \  --do_eval \  --data_dir $DATA_DIR/SST-2/ \  --max_seq_length 128 \  --learning_rate 2e-5 \  --num_train_epochs 1.0 \  --output_dir $SAVE_DIR \<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>运行并检验效果</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 使用sh命令运行sh run_cn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none"># 最终打印模型的验证结果, 准确率高达0.88.01/06/2020 14:22:36 - INFO - __main__ -   Saving features into cached file ../../cn_data/SST-2/cached_dev_bert-base-chinese_128_sst-201/06/2020 14:22:36 - INFO - __main__ -   ***** Running evaluation  *****01/06/2020 14:22:36 - INFO - __main__ -     Num examples = 100001/06/2020 14:22:36 - INFO - __main__ -     Batch size = 8Evaluating: 100%|████████████| 125/125 [00:56&lt;00:00,  2.20it/s]01/06/2020 14:23:33 - INFO - __main__ -   ***** Eval results  *****01/06/2020 14:23:33 - INFO - __main__ -     acc = 0.88<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>查看$SAVE_DIR的文件内容:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">added_tokens.jsoncheckpoint-350checkpoint-200checkpoint-300checkpoint-250checkpoint-200checkpoint-150checkpoint-100checkpoint-50pytorch_model.bintraining_args.binconfig.jsonspecial_tokens_map.jsonvocab.txteval_results.txttokenizer_config.json<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>使用transformers-cli上传模型:</li></ul><pre class="line-numbers language-none"><code class="language-none"># 默认username: ItcastAI# 默认password: ItcastAI$ transformers-cli login# 使用transformers-cli upload命令上传模型# 选择正确的微调模型路径$ transformers-cli upload ./bert_cn_finetuning/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>通过pytorch.hub加载模型进行使用:</li></ul><pre class="line-numbers language-none"><code class="language-none">import torchsource = 'huggingface/pytorch-transformers'# 模型名字为'ItcastAI/bert_cn_finetuning'model_name = 'ItcastAI/bert_cn_finetuning'tokenizer = torch.hub.load(source, 'tokenizer', model_name)model =  torch.hub.load(source, 'modelForSequenceClassification', model_name)def get_label(text):    index = tokenizer.encode(text)    tokens_tensor = torch.tensor([index])    # 使用评估模式    with torch.no_grad():        # 使用模型预测获得结果        result = model(tokens_tensor)    predicted_label = torch.argmax(result[0]).item()    return predicted_labelif __name__ == "__main__":    # text = "早餐不好,服务不到位,晚餐无西餐,早餐晚餐相同,房间条件不好"    text = "房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错."    print("输入文本为:", text)    print("预测标签为:", get_label(text))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">输入文本为: 早餐不好,服务不到位,晚餐无西餐,早餐晚餐相同,房间条件不好预测标签为: 0输入文本为: 房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错.预测标签为: 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h4 id="类型二实战演示"><a href="#类型二实战演示" class="headerlink" title="类型二实战演示"></a>类型二实战演示</h4><ul><li>直接加载预训练模型进行输入文本的特征表示, 后接自定义网络进行微调输出结果.</li><li>使用语料和完成的目标与类型一实战相同.</li></ul><hr><ul><li>直接加载预训练模型进行输入文本的特征表示:</li></ul><pre class="line-numbers language-none"><code class="language-none">import torch# 进行句子的截断补齐(规范长度)from keras.preprocessing import sequencesource = 'huggingface/pytorch-transformers'# 直接使用预训练的bert中文模型model_name = 'bert-base-chinese'# 通过torch.hub获得已经训练好的bert-base-chinese模型model =  torch.hub.load(source, 'model', model_name)# 获得对应的字符映射器, 它将把中文的每个字映射成一个数字tokenizer = torch.hub.load(source, 'tokenizer', model_name)# 句子规范长度cutlen = 32def get_bert_encode(text):    """    description: 使用bert-chinese编码中文文本    :param text: 要进行编码的文本    :return: 使用bert编码后的文本张量表示    """    # 首先使用字符映射器对每个汉字进行映射    # 这里需要注意, bert的tokenizer映射后会为结果前后添加开始和结束标记即101和102     # 这对于多段文本的编码是有意义的, 但在我们这里没有意义, 因此使用[1:-1]对头和尾进行切片    indexed_tokens = tokenizer.encode(text[:cutlen])[1:-1]    # 对映射后的句子进行截断补齐    indexed_tokens = sequence.pad_sequences([indexed_tokens], cutlen)     # 之后将列表结构转化为tensor    tokens_tensor = torch.LongTensor(indexed_tokens)    # 使模型不自动计算梯度    with torch.no_grad():        # 调用模型获得隐层输出        encoded_layers, _ = model(tokens_tensor)    # 输出的隐层是一个三维张量, 最外层一维是1, 我们使用[0]降去它.    encoded_layers = encoded_layers[0]    return encoded_layers<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">if __name__ == "__main__":    text = "早餐不好,服务不到位,晚餐无西餐,早餐晚餐相同,房间条件不好"    encoded_layers = get_bert_encode(text)    print(encoded_layers)    print(encoded_layers.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[-1.2282,  1.0551, -0.7953,  ...,  2.3363, -0.6413,  0.4174],        [-0.9769,  0.8361, -0.4328,  ...,  2.1668, -0.5845,  0.4836],        [-0.7990,  0.6181, -0.1424,  ...,  2.2845, -0.6079,  0.5288],        ...,        [ 0.9514,  0.5972,  0.3120,  ...,  1.8408, -0.1362, -0.1206],        [ 0.1250,  0.1984,  0.0484,  ...,  1.2302, -0.1905,  0.3205],        [ 0.2651,  0.0228,  0.1534,  ...,  1.0159, -0.3544,  0.1479]])torch.Size([32, 768])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>自定义单层的全连接网络作为微调网络:<ul><li>根据实际经验, 自定义的微调网络参数总数应大于0.5倍的训练数据量, 小于10倍的训练数据量, 这样有助于模型在合理的时间范围内收敛.</li></ul></li></ul><pre class="line-numbers language-none"><code class="language-none">import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module):    """定义微调网络的类"""    def __init__(self, char_size=32, embedding_size=768):        """        :param char_size: 输入句子中的字符数量, 即输入句子规范后的长度128.        :param embedding_size: 字嵌入的维度, 因为使用的bert中文模型嵌入维度是768, 因此embedding_size为768        """        super(Net, self).__init__()        # 将char_size和embedding_size传入其中        self.char_size = char_size        self.embedding_size = embedding_size        # 实例化一个全连接层        self.fc1 = nn.Linear(char_size*embedding_size, 2)    def forward(self, x):        # 对输入的张量形状进行变换, 以满足接下来层的输入要求        x = x.view(-1, self.char_size*self.embedding_size)        # 使用一个全连接层        x = self.fc1(x)        return x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">if __name__ == "__main__":    # 随机初始化一个输入参数    x = torch.randn(1, 32, 768)    # 实例化网络结构, 所有参数使用默认值    net = Net()    nr = net(x)    print(nr)    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">tensor([[0.3279, 0.2519]], grad_fn=&lt;ReluBackward0&gt;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><ul><li>构建训练与验证数据批次生成器:</li></ul><pre class="line-numbers language-none"><code class="language-none">import pandas as pdfrom collections import Counterfrom functools import reducefrom sklearn.utils import shuffledef data_loader(train_data_path, valid_data_path, batch_size):    """    description: 从持久化文件中加载数据    :param train_data_path: 训练数据路径    :param valid_data_path: 验证数据路径    :param batch_size: 训练和验证数据集的批次大小    :return: 训练数据生成器, 验证数据生成器, 训练数据数量, 验证数据数量    """    # 使用pd进行csv数据的读取, 并去除第一行的列名    train_data = pd.read_csv(train_data_path, header=None, sep="\t").drop([0])    valid_data = pd.read_csv(valid_data_path, header=None, sep="\t").drop([0])    # 打印训练集和验证集上的正负样本数量    print("训练数据集的正负样本数量:")    print(dict(Counter(train_data[1].values)))    print("验证数据集的正负样本数量:")    print(dict(Counter(valid_data[1].values)))    # 验证数据集中的数据总数至少能够满足一个批次    if len(valid_data) &lt; batch_size:        raise("Batch size or split not match!")    def _loader_generator(data):        """        description: 获得训练集/验证集的每个批次数据的生成器        :param data: 训练数据或验证数据        :return: 一个批次的训练数据或验证数据的生成器        """        # 以每个批次的间隔遍历数据集        for batch in range(0, len(data), batch_size):            # 定义batch数据的张量列表            batch_encoded = []            batch_labels = []            # 将一个bitch_size大小的数据转换成列表形式, 并进行逐条遍历            for item in shuffle(data.values.tolist())[batch: batch+batch_size]:                # 使用bert中文模型进行编码                encoded = get_bert_encode(item[0])                # 将编码后的每条数据装进预先定义好的列表中                batch_encoded.append(encoded)                # 同样将对应的该batch的标签装进labels列表中                batch_labels.append([int(item[1])])            # 使用reduce高阶函数将列表中的数据转换成模型需要的张量形式            # encoded的形状是(batch_size*max_len, embedding_size)            encoded = reduce(lambda x, y: torch.cat((x, y), dim=0), batch_encoded)            labels = torch.tensor(reduce(lambda x, y: x + y, batch_labels))            # 以生成器的方式返回数据和标签            yield (encoded, labels)    # 对训练集和验证集分别使用_loader_generator函数, 返回对应的生成器    # 最后还要返回训练集和验证集的样本数量    return _loader_generator(train_data), _loader_generator(valid_data), len(train_data), len(valid_data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">if __name__ == "__main__":    train_data_path = "./cn_data/SST-2/train.tsv"    valid_data_path = "./cn_data/SST-2/dev.tsv"    batch_size = 16    train_data_labels, valid_data_labels, \    train_data_len, valid_data_len = data_loader(train_data_path, valid_data_path, batch_size)    print(next(train_data_labels))    print(next(valid_data_labels))    print("train_data_len:", train_data_len)    print("valid_data_len:", valid_data_len)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">训练数据集的正负样本数量:{'0': 1518, '1': 1442}验证数据集的正负样本数量:{'1': 518, '0': 482}(tensor([[[-0.8328,  0.9376, -1.2489,  ...,  1.8594, -0.4636, -0.1682],        [-0.9798,  0.5113, -0.9868,  ...,  1.5500, -0.1934,  0.2521],        [-0.7574,  0.3086, -0.6031,  ...,  1.8467, -0.2507,  0.3916],        ...,        [ 0.0064,  0.2321,  0.3785,  ...,  0.3376,  0.4748, -0.1272],        [-0.3175,  0.4018, -0.0377,  ...,  0.6030,  0.2916, -0.4172],        [-0.6154,  1.0439,  0.2921,  ...,  0.5048, -0.0983,  0.0061]]]), tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,        1, 0, 1, 1, 1, 1, 0, 0]))(tensor([[[-0.1611,  0.9182, -0.3419,  ...,  0.6323, -0.2013,  0.0184],        [-0.1224,  0.7706, -0.2386,  ...,  0.7925,  0.0444,  0.2160],        [-0.0301,  0.6867, -0.1510,  ...,  0.9140,  0.0308,  0.2611],        ...,        [ 0.3662, -0.4925,  1.2332,  ...,  0.7741, -0.1007, -0.3099],        [-0.0932, -0.8494,  0.6586,  ...,  0.1235, -0.3152, -0.1635],        [ 0.5306, -0.5510,  0.3105,  ...,  1.2631, -0.5882, -0.1133]]]), tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,        1, 0, 0, 1, 1, 1, 0, 0]))train_data_len: 2960valid_data_len: 1000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>编写训练和验证函数:</li></ul><pre class="line-numbers language-none"><code class="language-none">import torch.optim as optimdef train(train_data_labels):    """    description: 训练函数, 在这个过程中将更新模型参数, 并收集准确率和损失    :param train_data_labels: 训练数据和标签的生成器对象    :return: 整个训练过程的平均损失之和以及正确标签的累加数    """    # 定义训练过程的初始损失和准确率累加数    train_running_loss = 0.0    train_running_acc = 0.0    # 循环遍历训练数据和标签生成器, 每个批次更新一次模型参数    for train_tensor, train_labels in train_data_labels:        # 初始化该批次的优化器        optimizer.zero_grad()        # 使用微调网络获得输出        train_outputs = net(train_tensor)        # 得到该批次下的平均损失        train_loss = criterion(train_outputs, train_labels)        # 将该批次的平均损失加到train_running_loss中        train_running_loss += train_loss.item()        # 损失反向传播        train_loss.backward()        # 优化器更新模型参数        optimizer.step()        # 将该批次中正确的标签数量进行累加, 以便之后计算准确率        train_running_acc += (train_outputs.argmax(1) == train_labels).sum().item()    return train_running_loss, train_running_accdef valid(valid_data_labels):    """    description: 验证函数, 在这个过程中将验证模型的在新数据集上的标签, 收集损失和准确率    :param valid_data_labels: 验证数据和标签的生成器对象    :return: 整个验证过程的平均损失之和以及正确标签的累加数    """    # 定义训练过程的初始损失和准确率累加数    valid_running_loss = 0.0    valid_running_acc = 0.0    # 循环遍历验证数据和标签生成器    for valid_tensor, valid_labels in valid_data_labels:        # 不自动更新梯度        with torch.no_grad():            # 使用微调网络获得输出            valid_outputs = net(valid_tensor)            # 得到该批次下的平均损失            valid_loss = criterion(valid_outputs, valid_labels)            # 将该批次的平均损失加到valid_running_loss中            valid_running_loss += valid_loss.item()            # 将该批次中正确的标签数量进行累加, 以便之后计算准确率            valid_running_acc += (valid_outputs.argmax(1) == valid_labels).sum().item()    return valid_running_loss,  valid_running_acc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>调用并保存模型:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">if __name__ == "__main__":    # 设定数据路径    train_data_path = "./cn_data/SST-2/train.tsv"    valid_data_path = "./cn_data/SST-2/dev.tsv"    # 定义交叉熵损失函数    criterion = nn.CrossEntropyLoss()    # 定义SGD优化方法    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)    # 定义训练轮数    epochs = 4    # 定义批次样本数量    batch_size = 16    # 进行指定轮次的训练    for epoch in range(epochs):        # 打印轮次        print("Epoch:", epoch + 1)        # 通过数据加载器获得训练数据和验证数据生成器, 以及对应的样本数量        train_data_labels, valid_data_labels, train_data_len, \        valid_data_len = data_loader(train_data_path, valid_data_path, batch_size)        # 调用训练函数进行训练        train_running_loss, train_running_acc = train(train_data_labels)        # 调用验证函数进行验证        valid_running_loss, valid_running_acc = valid(valid_data_labels)        # 计算每一轮的平均损失, train_running_loss和valid_running_loss是每个批次的平均损失之和        # 因此将它们乘以batch_size就得到了该轮的总损失, 除以样本数即该轮次的平均损失        train_average_loss = train_running_loss * batch_size / train_data_len        valid_average_loss = valid_running_loss * batch_size / valid_data_len        # train_running_acc和valid_running_acc是每个批次的正确标签累加和,        # 因此只需除以对应样本总数即是该轮次的准确率        train_average_acc = train_running_acc /  train_data_len        valid_average_acc = valid_running_acc / valid_data_len        # 打印该轮次下的训练损失和准确率以及验证损失和准确率        print("Train Loss:", train_average_loss, "|", "Train Acc:", train_average_acc)        print("Valid Loss:", valid_average_loss, "|", "Valid Acc:", valid_average_acc)    print('Finished Training')    # 保存路径    MODEL_PATH = './BERT_net.pth'    # 保存模型参数    torch.save(net.state_dict(), MODEL_PATH)     print('Finished Saving')    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">Epoch: 1Train Loss: 2.144986984236597 | Train Acc: 0.7347972972972973Valid Loss: 2.1898122818128902 | Valid Acc: 0.704Epoch: 2Train Loss: 1.3592962406135032 | Train Acc: 0.8435810810810811Valid Loss: 1.8816152956699324 | Valid Acc: 0.784Epoch: 3Train Loss: 1.5507876996199943 | Train Acc: 0.8439189189189189Valid Loss: 1.8626576719331536 | Valid Acc: 0.795Epoch: 4Train Loss: 0.7825378059198299 | Train Acc: 0.9081081081081082Valid Loss: 2.121698483480899 | Valid Acc: 0.803Finished TrainingFinished Saving<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><blockquote><ul><li>加载模型进行使用:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">if __name__ == "__main__":    MODEL_PATH = './BERT_net.pth'    # 加载模型参数    net.load_state_dict(torch.load(MODEL_PATH))    # text = "酒店设备一般，套房里卧室的不能上网，要到客厅去。"    text = "房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错."    print("输入文本为:", text)    with torch.no_grad():        output = net(get_bert_encode(text))        # 从output中取出最大值对应的索引        print("预测标签为:", torch.argmax(output).item())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><ul><li>输出效果:</li></ul></blockquote><pre class="line-numbers language-none"><code class="language-none">输入文本为: 房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错.预测标签为: 1输入文本为: 酒店设备一般，套房里卧室的不能上网，要到客厅去。预测标签为: 0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><h3 id="小节总结-3"><a href="#小节总结-3" class="headerlink" title="小节总结"></a>小节总结</h3><ul><li><p>学习了指定任务类型的微调脚本:</p><ul><li>huggingface研究机构向我们提供了针对GLUE数据集合任务类型的微调脚本, 这些微调脚本的核心都是微调模型的最后一个全连接层.</li><li>通过简单的参数配置来指定GLUE中存在任务类型(如: CoLA对应文本二分类, MRPC对应句子对文本二分类, STS-B对应句子对文本多分类), 以及指定需要微调的预训练模型.</li></ul><hr></li><li><p>学习了指定任务类型的微调脚本使用步骤:</p><ul><li>第一步: 下载微调脚本文件</li><li>第二步: 配置微调脚本参数</li><li>第三步: 运行并检验效果</li></ul><hr></li><li><p>学习了通过微调脚本微调后模型的使用步骤:</p><ul><li>第一步: 在<a href="https://huggingface.co/join%E4%B8%8A%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B8%90%E6%88%B7">https://huggingface.co/join上创建一个帐户</a></li><li>第二步: 在服务器终端使用transformers-cli登陆</li><li>第三步: 使用transformers-cli上传模型并查看</li><li>第四步: 使用pytorch.hub加载模型进行使用</li></ul><hr></li><li><p>学习了通过微调方式进行迁移学习的两种类型:</p><ul><li>类型一: 使用指定任务类型的微调脚本微调预训练模型, 后接带有输出头的预定义网络输出结果.</li><li>类型二: 直接加载预训练模型进行输入文本的特征表示, 后接自定义网络进行微调输出结果.</li></ul><hr></li><li><p>学习了类型一实战演示:</p><ul><li>使用文本二分类的任务类型SST-2的微调脚本微调中文预训练模型, 后接带有分类输出头的预定义网络输出结果. 目标是判断句子的情感倾向.</li><li>准备中文酒店评论的情感分析语料, 语料样式与SST-2数据集相同, 标签0代表差评, 标签1好评.</li><li>语料存放在与glue_data/同级目录cn_data/下, 其中的SST-2目录包含train.tsv和dev.tsv</li></ul><hr></li><li><p>学习了类型二实战演示:</p><ul><li>直接加载预训练模型进行输入文本的特征表示, 后接自定义网络进行微调输出结果.</li></ul></li></ul><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> fasttext </tag>
            
            <tag> 迁移学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo+matery搭建个人博客踩坑记录</title>
      <link href="2021/04/22/hexo-matery-da-jian-ge-ren-bo-ke-cai-keng-ji-lu/"/>
      <url>2021/04/22/hexo-matery-da-jian-ge-ren-bo-ke-cai-keng-ji-lu/</url>
      
        <content type="html"><![CDATA[<p>​        本次hexo个人博客的搭建参考了大佬洪卫的<a href="https://sunhwee.com/posts/6e8839eb.html#toc-heading-24">博客</a>，内容很全，但是在实际搭建中还是遇到了一些坑，这里对这些问题与解决方法进行记录。</p><ul><li><h1 id="一，代码块中行号与代码分离问题"><a href="#一，代码块中行号与代码分离问题" class="headerlink" title="一，代码块中行号与代码分离问题"></a>一，代码块中行号与代码分离问题</h1></li></ul><p>​        被这个问题困住了好久，始终不知道问题出现在哪里，如下图所示</p><p>​       <img src="/2021/04/22/hexo-matery-da-jian-ge-ren-bo-ke-cai-keng-ji-lu/93a1674370c018548a3f2ffdccc2c820.png" alt="行号分离"></p><p>​       问题解决方法：</p><p>​       网上搜索无果后，打开matery主题的阅读文档看了一遍，发现在网上搜索到的一些解决方法是错误的，真后悔没有早点看文档啊。如下：</p><p><img src="/2021/04/22/hexo-matery-da-jian-ge-ren-bo-ke-cai-keng-ji-lu/image-20210422192958983.png" alt="image-20210422192958983"></p><p>​     看到这里感觉应该是prism.css的适配问题，所以到了<a href="https://prismjs.com/download.html#themes=prism-tomorrow&amp;languages=markup+css+clike+javascript+css-extras+http+java+json+json5+log+makefile+markdown+python+jsx+sql&amp;plugins=line-highlight+line-numbers+show-invisibles+file-highlight+show-language+highlight-keywords+inline-color+toolbar+copy-to-clipboard+download-button">prismjs官网</a>一探究竟，果然，想要适配不同的语言进行高亮显示，必须在官网自主对语言进行选择，然后下载定制的css文件以及js文件对项目中的原文件进行替换既可以了，到此问题解决。</p><ul><li><h1 id="二，Markdown文档部署之后图片不显示问题"><a href="#二，Markdown文档部署之后图片不显示问题" class="headerlink" title="二，Markdown文档部署之后图片不显示问题"></a>二，Markdown文档部署之后图片不显示问题</h1></li></ul><p>这里参考了csdn中的博文<a href="https://blog.csdn.net/m0_43401436/article/details/107191688?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&amp;spm=1001.2101.3001.4242">《hexo博客中插入图片失败——解决思路及个人最终解决办法》</a></p><p>这里对解决步骤进行总结：</p><ul><li>将根目录下的_config.yml配置文件中的post_asset_folder中的选项设为true。（该设置可以使hexo new “文件名”创建文章时可以自动生成与文件名相同的文件夹）</li><li>设置Typora，如图所示选择复制到指定路径<br><img src="/2021/04/22/hexo-matery-da-jian-ge-ren-bo-ke-cai-keng-ji-lu/20200707222506636.png" alt="设置1"><br><img src="/2021/04/22/hexo-matery-da-jian-ge-ren-bo-ke-cai-keng-ji-lu/20200707224416296.png" alt="设置2"></li></ul><p>框里的路径是：<code>./${filename}</code>。<code>./</code>表示当前文件夹，<code>${filename}</code>表示当前文件名。（当往md文件中复制图片时自动将该图片保存在同名目录下）</p><ul><li>为项目安装插件（该插件可以从同名文件夹中自动匹配图片的相对路径）</li></ul> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> https://github.com/CodeFalling/hexo-asset-image --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>在md文件中直接使用以下语法，直接上图片名称，会自动匹配到同名文件夹下的图片</li></ul><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown"><span class="token url"><span class="token operator">!</span>[<span class="token content">img</span>](<span class="token url">20200707222506636.png</span>)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在md中图片是无法显示的，只有在项目运行时博客中的文章图片才显示正常，所以本地编辑md文件时，需要将图片的文件夹加入到路径中才可以显示。至此该问题基本解决。</p><ul><li><h1 id="三，matery主题文件中的图标不显示问题"><a href="#三，matery主题文件中的图标不显示问题" class="headerlink" title="三，matery主题文件中的图标不显示问题"></a>三，matery主题文件中的图标不显示问题</h1></li></ul><p>​       该主题是用的是<a href="http://www.fontawesome.com.cn/">fontawesome</a>的矢量图，可能有些图标是付费的原因，导致博客中该样式无法加载，从官网下载免费的css样式对以前的样式进行替换，同时使用免费的图标对代码中的图标进行替换，运行之后，问题完美解决。</p><p><img src="/2021/04/22/hexo-matery-da-jian-ge-ren-bo-ke-cai-keng-ji-lu/image-20210422200535286.png" alt="fontawesome官网"></p><p><img src="/2021/04/22/hexo-matery-da-jian-ge-ren-bo-ke-cai-keng-ji-lu/image-20210422200619699.png" alt="问题解决后的正常显示"></p><ul><li><h1 id="四，行号不显示与行号显示错位问题"><a href="#四，行号不显示与行号显示错位问题" class="headerlink" title="四，行号不显示与行号显示错位问题"></a>四，行号不显示与行号显示错位问题</h1><p>matery.css文件中加入以下代码可以解决行号不显示问题</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">pre<span class="token attribute"><span class="token punctuation">[</span><span class="token attr-name">class</span><span class="token operator">*=</span><span class="token attr-value">"language-"</span><span class="token punctuation">]</span></span><span class="token class">.line-numbers</span></span> <span class="token punctuation">{</span><span class="token property">position</span><span class="token punctuation">:</span> relative<span class="token punctuation">;</span><span class="token property">padding-left</span><span class="token punctuation">:</span> <span class="token number">3.8</span><span class="token unit">em</span><span class="token punctuation">;</span><span class="token property">counter-reset</span><span class="token punctuation">:</span> linenumber<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token selector">pre<span class="token attribute"><span class="token punctuation">[</span><span class="token attr-name">class</span><span class="token operator">*=</span><span class="token attr-value">"language-"</span><span class="token punctuation">]</span></span><span class="token class">.line-numbers</span> <span class="token combinator">&gt;</span> code</span> <span class="token punctuation">{</span><span class="token property">position</span><span class="token punctuation">:</span> relative<span class="token punctuation">;</span><span class="token property">white-space</span><span class="token punctuation">:</span> inherit<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token selector"><span class="token class">.line-numbers</span> <span class="token class">.line-numbers-rows</span></span> <span class="token punctuation">{</span><span class="token property">position</span><span class="token punctuation">:</span> absolute<span class="token punctuation">;</span><span class="token property">pointer-events</span><span class="token punctuation">:</span> none<span class="token punctuation">;</span><span class="token property">top</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token property">font-size</span><span class="token punctuation">:</span> <span class="token number">100</span><span class="token unit">%</span><span class="token punctuation">;</span><span class="token property">left</span><span class="token punctuation">:</span> <span class="token number">-3.8</span><span class="token unit">em</span><span class="token punctuation">;</span><span class="token property">width</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token unit">em</span><span class="token punctuation">;</span> <span class="token comment">/* works for line-numbers below 1000 lines */</span><span class="token property">letter-spacing</span><span class="token punctuation">:</span> <span class="token number">-1</span><span class="token unit">px</span><span class="token punctuation">;</span><span class="token property">border-right</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token unit">px</span> solid <span class="token hexcode color">#999</span><span class="token punctuation">;</span><span class="token property">-webkit-user-select</span><span class="token punctuation">:</span> none<span class="token punctuation">;</span><span class="token property">-moz-user-select</span><span class="token punctuation">:</span> none<span class="token punctuation">;</span><span class="token property">-ms-user-select</span><span class="token punctuation">:</span> none<span class="token punctuation">;</span><span class="token property">user-select</span><span class="token punctuation">:</span> none<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token selector"><span class="token class">.line-numbers-rows</span> <span class="token combinator">&gt;</span> span</span> <span class="token punctuation">{</span><span class="token property">display</span><span class="token punctuation">:</span> block<span class="token punctuation">;</span><span class="token property">counter-increment</span><span class="token punctuation">:</span> linenumber<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token selector"><span class="token class">.line-numbers-rows</span> <span class="token combinator">&gt;</span> span<span class="token pseudo-element">:before</span></span> <span class="token punctuation">{</span><span class="token property">content</span><span class="token punctuation">:</span> <span class="token function">counter</span><span class="token punctuation">(</span>linenumber<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token property">color</span><span class="token punctuation">:</span> <span class="token hexcode color">#999</span><span class="token punctuation">;</span><span class="token property">display</span><span class="token punctuation">:</span> block<span class="token punctuation">;</span><span class="token property">padding-right</span><span class="token punctuation">:</span> <span class="token number">0.8</span><span class="token unit">em</span><span class="token punctuation">;</span><span class="token property">text-align</span><span class="token punctuation">:</span> right<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将以下code样式中的font-size注释掉可以解决行号错位问题</p><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">code</span> <span class="token punctuation">{</span>    <span class="token property">padding</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token unit">px</span> <span class="token number">5</span><span class="token unit">px</span><span class="token punctuation">;</span>    <span class="token property">font-family</span><span class="token punctuation">:</span> Inconsolata<span class="token punctuation">,</span> Monaco<span class="token punctuation">,</span> Consolas<span class="token punctuation">,</span> <span class="token string">'Courier New'</span><span class="token punctuation">,</span> Courier<span class="token punctuation">,</span> monospace<span class="token punctuation">;</span>    <span class="token comment">/*font-size: 0.91rem;*/</span>    <span class="token property">color</span><span class="token punctuation">:</span> <span class="token hexcode color">#e96900</span><span class="token punctuation">;</span>    <span class="token property">background-color</span><span class="token punctuation">:</span> <span class="token hexcode color">#f8f8f8</span><span class="token punctuation">;</span>    <span class="token property">border-radius</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token unit">px</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>后面有什么问题继续在这里记录…….</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> hexo </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown-使用方式汇总</title>
      <link href="2021/04/22/markdown-shi-yong-fang-shi-hui-zong/"/>
      <url>2021/04/22/markdown-shi-yong-fang-shi-hui-zong/</url>
      
        <content type="html"><![CDATA[<h1 id="Markdown使用方法总结"><a href="#Markdown使用方法总结" class="headerlink" title="Markdown使用方法总结"></a>Markdown使用方法总结</h1><h2 id="一，为文本加入颜色元素"><a href="#一，为文本加入颜色元素" class="headerlink" title="一，为文本加入颜色元素"></a>一，为文本加入颜色元素</h2><ul><li><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a><strong>方法一</strong></h3></li></ul><p>​       使用格式</p><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">$\textcolor{下面的颜色名称}{文本} <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/2021/04/22/markdown-shi-yong-fang-shi-hui-zong/image-20210422001934902.png" alt="颜色参数"></p><ul><li><h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a><strong>方法二</strong></h3></li></ul><p>​        使用格式</p><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token special-attr"><span class="token attr-name">style</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">'</span><span class="token value css language-css"><span class="token property">color</span><span class="token punctuation">:</span>文字颜色<span class="token punctuation">;</span><span class="token property">background</span><span class="token punctuation">:</span>背景颜色<span class="token punctuation">;</span><span class="token property">font-size</span><span class="token punctuation">:</span>文字大小<span class="token punctuation">;</span><span class="token property">font-family</span><span class="token punctuation">:</span>字体<span class="token punctuation">;</span></span><span class="token punctuation">'</span></span></span><span class="token punctuation">&gt;</span></span>文字<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">&gt;</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer(上) 背景介绍</title>
      <link href="2021/04/19/transformer-bei-jing-jie-shao-0/"/>
      <url>2021/04/19/transformer-bei-jing-jie-shao-0/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer（上）-背景介绍"><a href="#Transformer（上）-背景介绍" class="headerlink" title="Transformer（上） 背景介绍"></a><strong>Transformer（上） 背景介绍</strong></h1><p>论文地址：<a href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p><h2 id="一，Transformer相关背景"><a href="#一，Transformer相关背景" class="headerlink" title="一，Transformer相关背景"></a>一，Transformer相关背景</h2><ul><li>2018年10月，Google发出一篇论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》, BERT模型横空出世, 并横扫NLP领域11项任务的最佳成绩!</li><li>而在BERT中发挥重要作用的结构就是Transformer, 之后又相继出现XLNET，roBERT等模型击败了BERT，但是他们的核心没有变，仍然是：Transformer.</li></ul><h2 id="二，Transformer的优势"><a href="#二，Transformer的优势" class="headerlink" title="二，Transformer的优势"></a>二，Transformer的优势</h2><ol><li><p>Transformer能够利用分布式GPU进行并训练，<span style="color:red;background:背景颜色;font-size:文字大小;font-family:字体;">提升模型训练效果</span>。</p></li><li><p><span style="color:red;background:背景颜色;font-size:文字大小;font-family:字体;">在分析预测更长的文本时, 捕捉间隔较长的语义关联效果更好</span>。</p><p>下面为评测比较图</p><p><img src="/2021/04/19/transformer-bei-jing-jie-shao-0/image-20210419190907778.png" alt="评测比较图"></p></li></ol><h2 id="三，Transformer的当前市场情况"><a href="#三，Transformer的当前市场情况" class="headerlink" title="三，Transformer的当前市场情况"></a>三，Transformer的当前市场情况</h2><p><img src="/2021/04/19/transformer-bei-jing-jie-shao-0/3.png" alt="市场情况"></p><p>​          其基本上可以看作是工业界的风向标, 市场空间自然不必多说！</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/04/14/hello-world/"/>
      <url>2021/04/14/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">$ hexo new "My New Post"<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
